{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f414380-8e96-4087-a16b-1a23fc23fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numba\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.models as models\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from torch.cuda.amp.autocast_mode import autocast\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch import nn\n",
    "from torch.cuda.amp.autocast_mode import autocast\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "\n",
    "import mmcv\n",
    "from mmcv.cnn import build_conv_layer\n",
    "\n",
    "from mmdet.models import build_backbone\n",
    "from mmdet.models.backbones.resnet import BasicBlock\n",
    "from mmdet.core import reduce_mean\n",
    "from mmdet.models import build_backbone\n",
    "\n",
    "from mmdet3d.core import draw_heatmap_gaussian, gaussian_radius\n",
    "from mmdet3d.models import build_neck\n",
    "from mmdet3d.models.dense_heads.centerpoint_head import CenterHead, circle_nms\n",
    "from mmdet3d.models.utils import clip_sigmoid\n",
    "from mmdet3d.models import build_neck\n",
    "\n",
    "from bevdepth.datasets.nusc_det_dataset import NuscDetDataset, collate_fn\n",
    "from bevdepth.evaluators.det_evaluators import DetNuscEvaluator\n",
    "from bevdepth.callbacks.ema import EMACallback\n",
    "from bevdepth.utils.torch_dist import all_gather_object, get_rank, synchronize\n",
    "\n",
    "try:\n",
    "    from bevdepth.ops.voxel_pooling import voxel_pooling\n",
    "except ImportError:\n",
    "    print('Import VoxelPooling fail.')\n",
    "    \n",
    "from mmdet3d.core.bbox.structures.lidar_box3d import LiDARInstance3DBoxes\n",
    "from nuscenes.utils.data_classes import Box, LidarPointCloud\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from PIL import Image\n",
    "from pyquaternion import Quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f23f99f-8106-433e-aaf7-88a72aec6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_name_from_general_to_detection = {\n",
    "    'human.pedestrian.adult': 'pedestrian',\n",
    "    'human.pedestrian.child': 'pedestrian',\n",
    "    'human.pedestrian.wheelchair': 'ignore',\n",
    "    'human.pedestrian.stroller': 'ignore',\n",
    "    'human.pedestrian.personal_mobility': 'ignore',\n",
    "    'human.pedestrian.police_officer': 'pedestrian',\n",
    "    'human.pedestrian.construction_worker': 'pedestrian',\n",
    "    'animal': 'ignore',\n",
    "    'vehicle.car': 'car',\n",
    "    'vehicle.motorcycle': 'motorcycle',\n",
    "    'vehicle.bicycle': 'bicycle',\n",
    "    'vehicle.bus.bendy': 'bus',\n",
    "    'vehicle.bus.rigid': 'bus',\n",
    "    'vehicle.truck': 'truck',\n",
    "    'vehicle.construction': 'construction_vehicle',\n",
    "    'vehicle.emergency.ambulance': 'ignore',\n",
    "    'vehicle.emergency.police': 'ignore',\n",
    "    'vehicle.trailer': 'trailer',\n",
    "    'movable_object.barrier': 'barrier',\n",
    "    'movable_object.trafficcone': 'traffic_cone',\n",
    "    'movable_object.pushable_pullable': 'ignore',\n",
    "    'movable_object.debris': 'ignore',\n",
    "    'static_object.bicycle_rack': 'ignore',\n",
    "}\n",
    "\n",
    "def get_rot(h):\n",
    "    return torch.Tensor([\n",
    "        [np.cos(h), np.sin(h)],\n",
    "        [-np.sin(h), np.cos(h)],\n",
    "    ])\n",
    "\n",
    "def img_transform(img, resize, resize_dims, crop, flip, rotate):\n",
    "    ida_rot = torch.eye(2)\n",
    "    ida_tran = torch.zeros(2)\n",
    "    # adjust image\n",
    "    img = img.resize(resize_dims)\n",
    "    img = img.crop(crop)\n",
    "    if flip:\n",
    "        img = img.transpose(method=Image.FLIP_LEFT_RIGHT)\n",
    "    img = img.rotate(rotate)\n",
    "\n",
    "    # post-homography transformation\n",
    "    ida_rot *= resize\n",
    "    ida_tran -= torch.Tensor(crop[:2])\n",
    "    if flip:\n",
    "        A = torch.Tensor([[-1, 0], [0, 1]])\n",
    "        b = torch.Tensor([crop[2] - crop[0], 0])\n",
    "        ida_rot = A.matmul(ida_rot)\n",
    "        ida_tran = A.matmul(ida_tran) + b\n",
    "    A = get_rot(rotate / 180 * np.pi)\n",
    "    b = torch.Tensor([crop[2] - crop[0], crop[3] - crop[1]]) / 2\n",
    "    b = A.matmul(-b) + b\n",
    "    ida_rot = A.matmul(ida_rot)\n",
    "    ida_tran = A.matmul(ida_tran) + b\n",
    "    ida_mat = ida_rot.new_zeros(4, 4)\n",
    "    ida_mat[3, 3] = 1\n",
    "    ida_mat[2, 2] = 1\n",
    "    ida_mat[:2, :2] = ida_rot\n",
    "    ida_mat[:2, 3] = ida_tran\n",
    "    return img, ida_mat\n",
    "\n",
    "def bev_transform(gt_boxes, rotate_angle, scale_ratio, flip_dx, flip_dy):\n",
    "    rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n",
    "    rot_sin = torch.sin(rotate_angle)\n",
    "    rot_cos = torch.cos(rotate_angle)\n",
    "    rot_mat = torch.Tensor([[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0],\n",
    "                            [0, 0, 1]])\n",
    "    scale_mat = torch.Tensor([[scale_ratio, 0, 0], [0, scale_ratio, 0],\n",
    "                              [0, 0, scale_ratio]])\n",
    "    flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    if flip_dx:\n",
    "        flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    if flip_dy:\n",
    "        flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n",
    "    rot_mat = flip_mat @ (scale_mat @ rot_mat)\n",
    "    if gt_boxes.shape[0] > 0:\n",
    "        gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n",
    "        gt_boxes[:, 3:6] *= scale_ratio\n",
    "        gt_boxes[:, 6] += rotate_angle\n",
    "        if flip_dx:\n",
    "            gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n",
    "        if flip_dy:\n",
    "            gt_boxes[:, 6] = -gt_boxes[:, 6]\n",
    "        gt_boxes[:, 7:] = (\n",
    "            rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(-1)\n",
    "    return gt_boxes, rot_mat\n",
    "\n",
    "\n",
    "def depth_transform(cam_depth, resize, resize_dims, crop, flip, rotate):\n",
    "    \"\"\"Transform depth based on ida augmentation configuration.\n",
    "\n",
    "    Args:\n",
    "        cam_depth (np array): Nx3, 3: x,y,d.\n",
    "        resize (float): Resize factor.\n",
    "        resize_dims (list): Final dimension.\n",
    "        crop (list): x1, y1, x2, y2\n",
    "        flip (bool): Whether to flip.\n",
    "        rotate (float): Rotation value.\n",
    "\n",
    "    Returns:\n",
    "        np array: [h/down_ratio, w/down_ratio, d]\n",
    "    \"\"\"\n",
    "\n",
    "    H, W = resize_dims\n",
    "    cam_depth[:, :2] = cam_depth[:, :2] * resize\n",
    "    cam_depth[:, 0] -= crop[0]\n",
    "    cam_depth[:, 1] -= crop[1]\n",
    "    if flip:\n",
    "        cam_depth[:, 0] = resize_dims[1] - cam_depth[:, 0]\n",
    "\n",
    "    cam_depth[:, 0] -= W / 2.0\n",
    "    cam_depth[:, 1] -= H / 2.0\n",
    "\n",
    "    h = rotate / 180 * np.pi\n",
    "    rot_matrix = [\n",
    "        [np.cos(h), np.sin(h)],\n",
    "        [-np.sin(h), np.cos(h)],\n",
    "    ]\n",
    "    cam_depth[:, :2] = np.matmul(rot_matrix, cam_depth[:, :2].T).T\n",
    "\n",
    "    cam_depth[:, 0] += W / 2.0\n",
    "    cam_depth[:, 1] += H / 2.0\n",
    "\n",
    "    depth_coords = cam_depth[:, :2].astype(np.int16)\n",
    "\n",
    "    depth_map = np.zeros(resize_dims)\n",
    "    valid_mask = ((depth_coords[:, 1] < resize_dims[0])\n",
    "                  & (depth_coords[:, 0] < resize_dims[1])\n",
    "                  & (depth_coords[:, 1] >= 0)\n",
    "                  & (depth_coords[:, 0] >= 0))\n",
    "    depth_map[depth_coords[valid_mask, 1],\n",
    "              depth_coords[valid_mask, 0]] = cam_depth[valid_mask, 2]\n",
    "\n",
    "    return torch.Tensor(depth_map)\n",
    "\n",
    "\n",
    "def map_pointcloud_to_image(\n",
    "    lidar_points,\n",
    "    img,\n",
    "    lidar_calibrated_sensor,\n",
    "    lidar_ego_pose,\n",
    "    cam_calibrated_sensor,\n",
    "    cam_ego_pose,\n",
    "    min_dist: float = 0.0,\n",
    "):\n",
    "\n",
    "    # Points live in the point sensor frame. So they need to be\n",
    "    # transformed via global to the image plane.\n",
    "    # First step: transform the pointcloud to the ego vehicle\n",
    "    # frame for the timestamp of the sweep.\n",
    "\n",
    "    lidar_points = LidarPointCloud(lidar_points.T)\n",
    "    lidar_points.rotate(\n",
    "        Quaternion(lidar_calibrated_sensor['rotation']).rotation_matrix)\n",
    "    lidar_points.translate(np.array(lidar_calibrated_sensor['translation']))\n",
    "\n",
    "    # Second step: transform from ego to the global frame.\n",
    "    lidar_points.rotate(Quaternion(lidar_ego_pose['rotation']).rotation_matrix)\n",
    "    lidar_points.translate(np.array(lidar_ego_pose['translation']))\n",
    "\n",
    "    # Third step: transform from global into the ego vehicle\n",
    "    # frame for the timestamp of the image.\n",
    "    lidar_points.translate(-np.array(cam_ego_pose['translation']))\n",
    "    lidar_points.rotate(Quaternion(cam_ego_pose['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fourth step: transform from ego into the camera.\n",
    "    lidar_points.translate(-np.array(cam_calibrated_sensor['translation']))\n",
    "    lidar_points.rotate(\n",
    "        Quaternion(cam_calibrated_sensor['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fifth step: actually take a \"picture\" of the point cloud.\n",
    "    # Grab the depths (camera frame z axis points away from the camera).\n",
    "    depths = lidar_points.points[2, :]\n",
    "    coloring = depths\n",
    "\n",
    "    # Take the actual picture (matrix multiplication with camera-matrix\n",
    "    # + renormalization).\n",
    "    points = view_points(lidar_points.points[:3, :],\n",
    "                         np.array(cam_calibrated_sensor['camera_intrinsic']),\n",
    "                         normalize=True)\n",
    "\n",
    "    # Remove points that are either outside or behind the camera.\n",
    "    # Leave a margin of 1 pixel for aesthetic reasons. Also make\n",
    "    # sure points are at least 1m in front of the camera to avoid\n",
    "    # seeing the lidar points on the camera casing for non-keyframes\n",
    "    # which are slightly out of sync.\n",
    "    mask = np.ones(depths.shape[0], dtype=bool)\n",
    "    mask = np.logical_and(mask, depths > min_dist)\n",
    "    mask = np.logical_and(mask, points[0, :] > 1)\n",
    "    mask = np.logical_and(mask, points[0, :] < img.size[0] - 1)\n",
    "    mask = np.logical_and(mask, points[1, :] > 1)\n",
    "    mask = np.logical_and(mask, points[1, :] < img.size[1] - 1)\n",
    "    points = points[:, mask]\n",
    "    coloring = coloring[mask]\n",
    "\n",
    "    return points, coloring\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(data, is_return_depth=False):\n",
    "    imgs_batch = list()\n",
    "    sensor2ego_mats_batch = list()\n",
    "    intrin_mats_batch = list()\n",
    "    ida_mats_batch = list()\n",
    "    sensor2sensor_mats_batch = list()\n",
    "    bda_mat_batch = list()\n",
    "    timestamps_batch = list()\n",
    "    gt_boxes_batch = list()\n",
    "    gt_labels_batch = list()\n",
    "    img_metas_batch = list()\n",
    "    depth_labels_batch = list()\n",
    "    for iter_data in data:\n",
    "        (\n",
    "            sweep_imgs,\n",
    "            sweep_sensor2ego_mats,\n",
    "            sweep_intrins,\n",
    "            sweep_ida_mats,\n",
    "            sweep_sensor2sensor_mats,\n",
    "            bda_mat,\n",
    "            sweep_timestamps,\n",
    "            img_metas,\n",
    "            gt_boxes,\n",
    "            gt_labels,\n",
    "        ) = iter_data[:10]\n",
    "        if is_return_depth:\n",
    "            gt_depth = iter_data[10]\n",
    "            depth_labels_batch.append(gt_depth)\n",
    "        imgs_batch.append(sweep_imgs)\n",
    "        sensor2ego_mats_batch.append(sweep_sensor2ego_mats)\n",
    "        intrin_mats_batch.append(sweep_intrins)\n",
    "        ida_mats_batch.append(sweep_ida_mats)\n",
    "        sensor2sensor_mats_batch.append(sweep_sensor2sensor_mats)\n",
    "        bda_mat_batch.append(bda_mat)\n",
    "        timestamps_batch.append(sweep_timestamps)\n",
    "        img_metas_batch.append(img_metas)\n",
    "        gt_boxes_batch.append(gt_boxes)\n",
    "        gt_labels_batch.append(gt_labels)\n",
    "    mats_dict = dict()\n",
    "    mats_dict['sensor2ego_mats'] = torch.stack(sensor2ego_mats_batch)\n",
    "    mats_dict['intrin_mats'] = torch.stack(intrin_mats_batch)\n",
    "    mats_dict['ida_mats'] = torch.stack(ida_mats_batch)\n",
    "    mats_dict['sensor2sensor_mats'] = torch.stack(sensor2sensor_mats_batch)\n",
    "    mats_dict['bda_mat'] = torch.stack(bda_mat_batch)\n",
    "    ret_list = [\n",
    "        torch.stack(imgs_batch),\n",
    "        mats_dict,\n",
    "        torch.stack(timestamps_batch),\n",
    "        img_metas_batch,\n",
    "        gt_boxes_batch,\n",
    "        gt_labels_batch,\n",
    "    ]\n",
    "    if is_return_depth:\n",
    "        ret_list.append(torch.stack(depth_labels_batch))\n",
    "    return ret_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d21911-8c50-4b97-8217-dc0361d07c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e762a26e-83ea-4f01-b3ac-451d85770573",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 900\n",
    "W = 1600\n",
    "final_dim = (256, 704)\n",
    "img_conf = dict(img_mean=[123.675, 116.28, 103.53],\n",
    "                img_std=[58.395, 57.12, 57.375],\n",
    "                to_rgb=True)\n",
    "\n",
    "backbone_conf = {\n",
    "    'x_bound': [-51.2, 51.2, 0.8],\n",
    "    'y_bound': [-51.2, 51.2, 0.8],\n",
    "    'z_bound': [-5, 3, 8],\n",
    "    'd_bound': [2.0, 58.0, 0.5],\n",
    "    'final_dim':\n",
    "    final_dim,\n",
    "    'output_channels':\n",
    "    80,\n",
    "    'downsample_factor':\n",
    "    16,\n",
    "    'img_backbone_conf':\n",
    "    dict(\n",
    "        type='ResNet',\n",
    "        depth=50,\n",
    "        frozen_stages=0,\n",
    "        out_indices=[0, 1, 2, 3],\n",
    "        norm_eval=False,\n",
    "        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),\n",
    "    ),\n",
    "    'img_neck_conf':\n",
    "    dict(\n",
    "        type='SECONDFPN',\n",
    "        in_channels=[256, 512, 1024, 2048],\n",
    "        upsample_strides=[0.25, 0.5, 1, 2],\n",
    "        out_channels=[128, 128, 128, 128],\n",
    "    ),\n",
    "    'depth_net_conf':\n",
    "    dict(in_channels=512, mid_channels=512)\n",
    "}\n",
    "\n",
    "ida_aug_conf = {\n",
    "    'resize_lim': (0.386, 0.55),\n",
    "    'final_dim': final_dim,\n",
    "    'rot_lim': (-5.4, 5.4),\n",
    "    'H': H,\n",
    "    'W': W,\n",
    "    'rand_flip': True,\n",
    "    'bot_pct_lim': (0.0, 0.0),\n",
    "    'cams': [\n",
    "        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT',\n",
    "        'CAM_BACK', 'CAM_BACK_RIGHT'\n",
    "    ],\n",
    "    'Ncams': 6,\n",
    "}\n",
    "\n",
    "bda_aug_conf = {\n",
    "    'rot_lim': (-22.5, 22.5),\n",
    "    'scale_lim': (0.95, 1.05),\n",
    "    'flip_dx_ratio': 0.5,\n",
    "    'flip_dy_ratio': 0.5\n",
    "}\n",
    "\n",
    "bev_backbone = dict(\n",
    "    type='ResNet',\n",
    "    in_channels=80,\n",
    "    depth=18,\n",
    "    num_stages=3,\n",
    "    strides=(1, 2, 2),\n",
    "    dilations=(1, 1, 1),\n",
    "    out_indices=[0, 1, 2],\n",
    "    norm_eval=False,\n",
    "    base_channels=160,\n",
    ")\n",
    "\n",
    "bev_neck = dict(type='SECONDFPN',\n",
    "                in_channels=[80, 160, 320, 640],\n",
    "                upsample_strides=[1, 2, 4, 8],\n",
    "                out_channels=[64, 64, 64, 64])\n",
    "\n",
    "TASKS = [\n",
    "    dict(num_class=1, class_names=['car']),\n",
    "    dict(num_class=2, class_names=['truck', 'construction_vehicle']),\n",
    "    dict(num_class=2, class_names=['bus', 'trailer']),\n",
    "    dict(num_class=1, class_names=['barrier']),\n",
    "    dict(num_class=2, class_names=['motorcycle', 'bicycle']),\n",
    "    dict(num_class=2, class_names=['pedestrian', 'traffic_cone']),\n",
    "]\n",
    "\n",
    "common_heads = dict(reg=(2, 2),\n",
    "                    height=(1, 2),\n",
    "                    dim=(3, 2),\n",
    "                    rot=(2, 2),\n",
    "                    vel=(2, 2))\n",
    "\n",
    "bbox_coder = dict(\n",
    "    type='CenterPointBBoxCoder',\n",
    "    post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],\n",
    "    max_num=500,\n",
    "    score_threshold=0.1,\n",
    "    out_size_factor=4,\n",
    "    voxel_size=[0.2, 0.2, 8],\n",
    "    pc_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n",
    "    code_size=9,\n",
    ")\n",
    "\n",
    "train_cfg = dict(\n",
    "    point_cloud_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n",
    "    grid_size=[512, 512, 1],\n",
    "    voxel_size=[0.2, 0.2, 8],\n",
    "    out_size_factor=4,\n",
    "    dense_reg=1,\n",
    "    gaussian_overlap=0.1,\n",
    "    max_objs=500,\n",
    "    min_radius=2,\n",
    "    code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5],\n",
    ")\n",
    "\n",
    "test_cfg = dict(\n",
    "    post_center_limit_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],\n",
    "    max_per_img=500,\n",
    "    max_pool_nms=False,\n",
    "    min_radius=[4, 12, 10, 1, 0.85, 0.175],\n",
    "    score_threshold=0.1,\n",
    "    out_size_factor=4,\n",
    "    voxel_size=[0.2, 0.2, 8],\n",
    "    nms_type='circle',\n",
    "    pre_max_size=1000,\n",
    "    post_max_size=83,\n",
    "    nms_thr=0.2,\n",
    ")\n",
    "\n",
    "head_conf = {\n",
    "    'bev_backbone_conf': bev_backbone,\n",
    "    'bev_neck_conf': bev_neck,\n",
    "    'tasks': TASKS,\n",
    "    'common_heads': common_heads,\n",
    "    'bbox_coder': bbox_coder,\n",
    "    'train_cfg': train_cfg,\n",
    "    'test_cfg': test_cfg,\n",
    "    'in_channels': 256,  # Equal to bev_neck output_channels.\n",
    "    'loss_cls': dict(type='GaussianFocalLoss', reduction='mean'),\n",
    "    'loss_bbox': dict(type='L1Loss', reduction='mean', loss_weight=0.25),\n",
    "    'gaussian_overlap': 0.1,\n",
    "    'min_radius': 2,\n",
    "}\n",
    "\n",
    "classes = [\n",
    "    'car',\n",
    "    'truck',\n",
    "    'construction_vehicle',\n",
    "    'bus',\n",
    "    'trailer',\n",
    "    'barrier',\n",
    "    'motorcycle',\n",
    "    'bicycle',\n",
    "    'pedestrian',\n",
    "    'traffic_cone',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be76bb44-f486-4f5d-b3d2-38641005b326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Insta360DetDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 ida_aug_conf,\n",
    "                 bda_aug_conf,\n",
    "                 classes,\n",
    "                 data_root,\n",
    "                 info_paths,\n",
    "                 is_train, # False\n",
    "                 use_cbgs=False,\n",
    "                 num_sweeps=1,\n",
    "                 img_conf=dict(img_mean=[123.675, 116.28, 103.53],\n",
    "                               img_std=[58.395, 57.12, 57.375],\n",
    "                               to_rgb=True),\n",
    "                 return_depth=False,\n",
    "                 sweep_idxes=list(),\n",
    "                 key_idxes=list(),\n",
    "                 use_fusion=False):\n",
    "        \"\"\"Dataset used for bevdetection task.\n",
    "        Args:\n",
    "            ida_aug_conf (dict): Config for ida augmentation.\n",
    "            bda_aug_conf (dict): Config for bda augmentation.\n",
    "            classes (list): Class names.\n",
    "            use_cbgs (bool): Whether to use cbgs strategy,\n",
    "                Default: False.\n",
    "            num_sweeps (int): Number of sweeps to be used for each sample.\n",
    "                default: 1.\n",
    "            img_conf (dict): Config for image.\n",
    "            return_depth (bool): Whether to use depth gt.\n",
    "                default: False.\n",
    "            sweep_idxes (list): List of sweep idxes to be used.\n",
    "                default: list().\n",
    "            key_idxes (list): List of key idxes to be used.\n",
    "                default: list().\n",
    "            use_fusion (bool): Whether to use lidar data.\n",
    "                default: False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if isinstance(info_paths, list):\n",
    "            self.infos = list()\n",
    "            for info_path in info_paths:\n",
    "                self.infos.extend(mmcv.load(info_path))\n",
    "        else:\n",
    "            self.infos = mmcv.load(info_paths)\n",
    "        \n",
    "        self.is_train = is_train\n",
    "        self.ida_aug_conf = ida_aug_conf\n",
    "        self.bda_aug_conf = bda_aug_conf\n",
    "        self.data_root = data_root\n",
    "        self.classes = classes\n",
    "        self.use_cbgs = use_cbgs\n",
    "        if self.use_cbgs:\n",
    "            self.cat2id = {name: i for i, name in enumerate(self.classes)}\n",
    "            self.sample_indices = self._get_sample_indices()\n",
    "        self.num_sweeps = num_sweeps\n",
    "        self.img_mean = np.array(img_conf['img_mean'], np.float32)\n",
    "        self.img_std = np.array(img_conf['img_std'], np.float32)\n",
    "        self.to_rgb = img_conf['to_rgb']\n",
    "        self.return_depth = return_depth\n",
    "        assert sum([sweep_idx >= 0 for sweep_idx in sweep_idxes]) \\\n",
    "            == len(sweep_idxes), 'All `sweep_idxes` must greater \\\n",
    "                than or equal to 0.'\n",
    "\n",
    "        self.sweeps_idx = sweep_idxes\n",
    "        assert sum([key_idx < 0 for key_idx in key_idxes]) == len(key_idxes),\\\n",
    "            'All `key_idxes` must less than 0.'\n",
    "        self.key_idxes = [0] + key_idxes\n",
    "        self.use_fusion = use_fusion\n",
    "\n",
    "    def _get_sample_indices(self):\n",
    "        \"\"\"Load annotations from ann_file.\n",
    "        Args: ann_file (str): Path of the annotation file.\n",
    "        Returns: list[dict]: List of annotations after class sampling.\n",
    "        \"\"\"\n",
    "        class_sample_idxs = {cat_id: [] for cat_id in self.cat2id.values()}\n",
    "        for idx, info in enumerate(self.infos):\n",
    "            gt_names = set(\n",
    "                [ann_info['category_name'] for ann_info in info['ann_infos']])\n",
    "            for gt_name in gt_names:\n",
    "                gt_name = map_name_from_general_to_detection[gt_name]\n",
    "                if gt_name not in self.classes:\n",
    "                    continue\n",
    "                class_sample_idxs[self.cat2id[gt_name]].append(idx)\n",
    "        duplicated_samples = sum(\n",
    "            [len(v) for _, v in class_sample_idxs.items()])\n",
    "        class_distribution = {\n",
    "            k: len(v) / duplicated_samples\n",
    "            for k, v in class_sample_idxs.items()\n",
    "        }\n",
    "\n",
    "        sample_indices = []\n",
    "\n",
    "        frac = 1.0 / len(self.classes)\n",
    "        ratios = [frac / v for v in class_distribution.values()]\n",
    "        for cls_inds, ratio in zip(list(class_sample_idxs.values()), ratios):\n",
    "            sample_indices += np.random.choice(cls_inds, int(len(cls_inds) * ratio)).tolist()\n",
    "        return sample_indices\n",
    "\n",
    "    def sample_ida_augmentation(self):\n",
    "        \"\"\"Generate ida augmentation values based on ida_config.\"\"\"\n",
    "        H, W = self.ida_aug_conf['H'], self.ida_aug_conf['W']\n",
    "        fH, fW = self.ida_aug_conf['final_dim']\n",
    "        if self.is_train:\n",
    "            resize = np.random.uniform(*self.ida_aug_conf['resize_lim'])\n",
    "            resize_dims = (int(W * resize), int(H * resize))\n",
    "            newW, newH = resize_dims\n",
    "            crop_h = int(\n",
    "                (1 - np.random.uniform(*self.ida_aug_conf['bot_pct_lim'])) *\n",
    "                newH) - fH\n",
    "            crop_w = int(np.random.uniform(0, max(0, newW - fW)))\n",
    "            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n",
    "            flip = False\n",
    "            if self.ida_aug_conf['rand_flip'] and np.random.choice([0, 1]):\n",
    "                flip = True\n",
    "            rotate_ida = np.random.uniform(*self.ida_aug_conf['rot_lim'])\n",
    "        else:\n",
    "            resize = max(fH / H, fW / W)\n",
    "            resize_dims = (int(W * resize), int(H * resize))\n",
    "            newW, newH = resize_dims\n",
    "            crop_h = int(\n",
    "                (1 - np.mean(self.ida_aug_conf['bot_pct_lim'])) * newH) - fH\n",
    "            crop_w = int(max(0, newW - fW) / 2)\n",
    "            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n",
    "            flip = False\n",
    "            rotate_ida = 0\n",
    "        return resize, resize_dims, crop, flip, rotate_ida\n",
    "\n",
    "    def sample_bda_augmentation(self):\n",
    "        \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n",
    "        if self.is_train:\n",
    "            rotate_bda = np.random.uniform(*self.bda_aug_conf['rot_lim'])\n",
    "            scale_bda = np.random.uniform(*self.bda_aug_conf['scale_lim'])\n",
    "            flip_dx = np.random.uniform() < self.bda_aug_conf['flip_dx_ratio']\n",
    "            flip_dy = np.random.uniform() < self.bda_aug_conf['flip_dy_ratio']\n",
    "        else:\n",
    "            rotate_bda = 0\n",
    "            scale_bda = 1.0\n",
    "            flip_dx = False\n",
    "            flip_dy = False\n",
    "        return rotate_bda, scale_bda, flip_dx, flip_dy\n",
    "\n",
    "    def get_lidar_depth(self, lidar_points, img, lidar_info, cam_info):\n",
    "        lidar_calibrated_sensor = lidar_info['LIDAR_TOP']['calibrated_sensor']\n",
    "        lidar_ego_pose = lidar_info['LIDAR_TOP']['ego_pose']\n",
    "        cam_calibrated_sensor = cam_info['calibrated_sensor']\n",
    "        cam_ego_pose = cam_info['ego_pose']\n",
    "        pts_img, depth = map_pointcloud_to_image(\n",
    "            lidar_points.copy(), img, lidar_calibrated_sensor.copy(),\n",
    "            lidar_ego_pose.copy(), cam_calibrated_sensor, cam_ego_pose)\n",
    "        return np.concatenate([pts_img[:2, :].T, depth[:, None]],\n",
    "                              axis=1).astype(np.float32)\n",
    "\n",
    "    def get_image(self, cam_infos, cams, lidar_infos=None):\n",
    "        \"\"\"Given data and cam_names, return image data needed.\n",
    "\n",
    "        Args:\n",
    "            sweeps_data (list): Raw data used to generate the data we needed.\n",
    "            cams (list): Camera names.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Image data after processing.\n",
    "            Tensor: Transformation matrix from camera to ego.\n",
    "            Tensor: Intrinsic matrix.\n",
    "            Tensor: Transformation matrix for ida.\n",
    "            Tensor: Transformation matrix from key\n",
    "                frame camera to sweep frame camera.\n",
    "            Tensor: timestamps.\n",
    "            dict: meta infos needed for evaluation.\n",
    "        \"\"\"\n",
    "        assert len(cam_infos) > 0\n",
    "        sweep_imgs = list()\n",
    "        sweep_sensor2ego_mats = list()\n",
    "        sweep_intrin_mats = list()\n",
    "        sweep_ida_mats = list()\n",
    "        sweep_sensor2sensor_mats = list()\n",
    "        sweep_timestamps = list()\n",
    "        sweep_lidar_depth = list()\n",
    "        \n",
    "        if self.return_depth or self.use_fusion:\n",
    "            sweep_lidar_points = list()\n",
    "            for lidar_info in lidar_infos:\n",
    "                lidar_path = lidar_info['LIDAR_TOP']['filename']\n",
    "                lidar_points = np.fromfile(os.path.join(\n",
    "                    self.data_root, lidar_path),\n",
    "                                           dtype=np.float32,\n",
    "                                           count=-1).reshape(-1, 5)[..., :4]\n",
    "                sweep_lidar_points.append(lidar_points)\n",
    "\n",
    "        for cam in cams:\n",
    "            imgs = list()\n",
    "            sensor2ego_mats = list()\n",
    "            intrin_mats = list()\n",
    "            ida_mats = list()\n",
    "            sensor2sensor_mats = list()\n",
    "            timestamps = list()\n",
    "            lidar_depth = list()\n",
    "            key_info = cam_infos[0]\n",
    "            resize, resize_dims, crop, flip, rotate_ida = self.sample_ida_augmentation()\n",
    "            \n",
    "            for sweep_idx, cam_info in enumerate(cam_infos):\n",
    "                img = Image.open(os.path.join(self.data_root, cam_info[cam]['filename']))\n",
    "                # img = Image.fromarray(img)\n",
    "                \n",
    "                # sweep sensor to sweep ego\n",
    "                w, x, y, z = cam_info[cam]['calibrated_sensor']['rotation']\n",
    "                sweepsensor2sweepego_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n",
    "                sweepsensor2sweepego_tran = torch.Tensor(cam_info[cam]['calibrated_sensor']['translation'])\n",
    "                sweepsensor2sweepego = sweepsensor2sweepego_rot.new_zeros((4, 4))\n",
    "                sweepsensor2sweepego[3, 3] = 1\n",
    "                sweepsensor2sweepego[:3, :3] = sweepsensor2sweepego_rot\n",
    "                sweepsensor2sweepego[:3, -1] = sweepsensor2sweepego_tran\n",
    "                \n",
    "                # sweep ego to global\n",
    "                w, x, y, z = cam_info[cam]['ego_pose']['rotation']\n",
    "                sweepego2global_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n",
    "                sweepego2global_tran = torch.Tensor(cam_info[cam]['ego_pose']['translation'])\n",
    "                sweepego2global = sweepego2global_rot.new_zeros((4, 4))\n",
    "                sweepego2global[3, 3] = 1\n",
    "                sweepego2global[:3, :3] = sweepego2global_rot\n",
    "                sweepego2global[:3, -1] = sweepego2global_tran\n",
    "\n",
    "                # global sensor to cur ego\n",
    "                w, x, y, z = key_info[cam]['ego_pose']['rotation']\n",
    "                keyego2global_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n",
    "                keyego2global_tran = torch.Tensor(key_info[cam]['ego_pose']['translation'])\n",
    "                keyego2global = keyego2global_rot.new_zeros((4, 4))\n",
    "                keyego2global[3, 3] = 1\n",
    "                keyego2global[:3, :3] = keyego2global_rot\n",
    "                keyego2global[:3, -1] = keyego2global_tran\n",
    "                global2keyego = keyego2global.inverse()\n",
    "\n",
    "                # cur ego to sensor\n",
    "                w, x, y, z = key_info[cam]['calibrated_sensor']['rotation']\n",
    "                keysensor2keyego_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n",
    "                keysensor2keyego_tran = torch.Tensor(key_info[cam]['calibrated_sensor']['translation'])\n",
    "                keysensor2keyego = keysensor2keyego_rot.new_zeros((4, 4))\n",
    "                keysensor2keyego[3, 3] = 1\n",
    "                keysensor2keyego[:3, :3] = keysensor2keyego_rot\n",
    "                keysensor2keyego[:3, -1] = keysensor2keyego_tran\n",
    "                \n",
    "                # sensor2sensor matrix\n",
    "                keyego2keysensor = keysensor2keyego.inverse()\n",
    "                keysensor2sweepsensor = (keyego2keysensor @ global2keyego @ sweepego2global @ sweepsensor2sweepego).inverse()\n",
    "                sensor2sensor_mats.append(keysensor2sweepsensor)\n",
    "                \n",
    "                # sensor2ego matrix\n",
    "                sweepsensor2keyego = global2keyego @ sweepego2global @ sweepsensor2sweepego\n",
    "                sensor2ego_mats.append(sweepsensor2keyego)\n",
    "                \n",
    "                # intrinsic matrix\n",
    "                intrin_mat = torch.zeros((4, 4))\n",
    "                intrin_mat[3, 3] = 1\n",
    "                intrin_mat[:3, :3] = torch.Tensor(cam_info[cam]['calibrated_sensor']['camera_intrinsic'])\n",
    "                \n",
    "                if self.return_depth and (self.use_fusion or sweep_idx == 0):\n",
    "                    point_depth = self.get_lidar_depth(\n",
    "                        sweep_lidar_points[sweep_idx], img,\n",
    "                        lidar_infos[sweep_idx], cam_info[cam])\n",
    "                    point_depth_augmented = depth_transform(\n",
    "                        point_depth, resize, self.ida_aug_conf['final_dim'],\n",
    "                        crop, flip, rotate_ida)\n",
    "                    lidar_depth.append(point_depth_augmented)\n",
    "                \n",
    "                img, ida_mat = img_transform(\n",
    "                    img,\n",
    "                    resize=resize,\n",
    "                    resize_dims=resize_dims,\n",
    "                    crop=crop,\n",
    "                    flip=flip,\n",
    "                    rotate=rotate_ida,\n",
    "                )\n",
    "                ida_mats.append(ida_mat)\n",
    "                img = mmcv.imnormalize(np.array(img), self.img_mean,self.img_std, self.to_rgb)\n",
    "                img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "                imgs.append(img)\n",
    "                intrin_mats.append(intrin_mat)\n",
    "                timestamps.append(cam_info[cam]['timestamp'])\n",
    "                \n",
    "            sweep_imgs.append(torch.stack(imgs))\n",
    "            sweep_sensor2ego_mats.append(torch.stack(sensor2ego_mats))\n",
    "            sweep_intrin_mats.append(torch.stack(intrin_mats))\n",
    "            sweep_ida_mats.append(torch.stack(ida_mats))\n",
    "            sweep_sensor2sensor_mats.append(torch.stack(sensor2sensor_mats))\n",
    "            sweep_timestamps.append(torch.tensor(timestamps))\n",
    "            if self.return_depth:\n",
    "                sweep_lidar_depth.append(torch.stack(lidar_depth))\n",
    "                \n",
    "        # Get mean pose of all cams.\n",
    "        ego2global_rotation = np.mean(\n",
    "            [key_info[cam]['ego_pose']['rotation'] for cam in cams], 0)\n",
    "        ego2global_translation = np.mean(\n",
    "            [key_info[cam]['ego_pose']['translation'] for cam in cams], 0)\n",
    "        img_metas = dict(\n",
    "            box_type_3d=LiDARInstance3DBoxes,\n",
    "            ego2global_translation=ego2global_translation,\n",
    "            ego2global_rotation=ego2global_rotation,\n",
    "        )\n",
    "        \n",
    "        ret_list = [\n",
    "            torch.stack(sweep_imgs).permute(1, 0, 2, 3, 4),\n",
    "            torch.stack(sweep_sensor2ego_mats).permute(1, 0, 2, 3),\n",
    "            torch.stack(sweep_intrin_mats).permute(1, 0, 2, 3),\n",
    "            torch.stack(sweep_ida_mats).permute(1, 0, 2, 3),\n",
    "            torch.stack(sweep_sensor2sensor_mats).permute(1, 0, 2, 3),\n",
    "            torch.stack(sweep_timestamps).permute(1, 0),\n",
    "            img_metas,\n",
    "        ]\n",
    "        if self.return_depth:\n",
    "            ret_list.append(torch.stack(sweep_lidar_depth).permute(1, 0, 2, 3))\n",
    "        return ret_list\n",
    "\n",
    "    def get_gt(self, info, cams):\n",
    "        \"\"\"Generate gt labels from info.\n",
    "\n",
    "        Args:\n",
    "            info(dict): Infos needed to generate gt labels.\n",
    "            cams(list): Camera names.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: GT bboxes.\n",
    "            Tensor: GT labels.\n",
    "        \"\"\"\n",
    "        ego2global_rotation = np.mean(\n",
    "            [info['cam_infos'][cam]['ego_pose']['rotation'] for cam in cams],\n",
    "            0)\n",
    "        ego2global_translation = np.mean([\n",
    "            info['cam_infos'][cam]['ego_pose']['translation'] for cam in cams\n",
    "        ], 0)\n",
    "        trans = -np.array(ego2global_translation)\n",
    "        rot = Quaternion(ego2global_rotation).inverse\n",
    "        gt_boxes = list()\n",
    "        gt_labels = list()\n",
    "        for ann_info in info['ann_infos']:\n",
    "            # Use ego coordinate.\n",
    "            if (map_name_from_general_to_detection[ann_info['category_name']]\n",
    "                    not in self.classes\n",
    "                    or ann_info['num_lidar_pts'] + ann_info['num_radar_pts'] <=\n",
    "                    0):\n",
    "                continue\n",
    "            box = Box(\n",
    "                ann_info['translation'],\n",
    "                ann_info['size'],\n",
    "                Quaternion(ann_info['rotation']),\n",
    "                velocity=ann_info['velocity'],\n",
    "            )\n",
    "            box.translate(trans)\n",
    "            box.rotate(rot)\n",
    "            box_xyz = np.array(box.center)\n",
    "            box_dxdydz = np.array(box.wlh)[[1, 0, 2]]\n",
    "            box_yaw = np.array([box.orientation.yaw_pitch_roll[0]])\n",
    "            box_velo = np.array(box.velocity[:2])\n",
    "            gt_box = np.concatenate([box_xyz, box_dxdydz, box_yaw, box_velo])\n",
    "            gt_boxes.append(gt_box)\n",
    "            gt_labels.append(\n",
    "                self.classes.index(map_name_from_general_to_detection[\n",
    "                    ann_info['category_name']]))\n",
    "        return torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n",
    "\n",
    "    def choose_cams(self):\n",
    "        \"\"\"Choose cameras randomly.\n",
    "\n",
    "        Returns:\n",
    "            list: Cameras to be used.\n",
    "        \"\"\"\n",
    "        if self.is_train and self.ida_aug_conf['Ncams'] < len(\n",
    "                self.ida_aug_conf['cams']):\n",
    "            cams = np.random.choice(self.ida_aug_conf['cams'],\n",
    "                                    self.ida_aug_conf['Ncams'],\n",
    "                                    replace=False)\n",
    "        else:\n",
    "            cams = self.ida_aug_conf['cams']\n",
    "        return cams\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.use_cbgs:\n",
    "            idx = self.sample_indices[idx]\n",
    "        cam_infos = list()\n",
    "        lidar_infos = list()\n",
    "        # TODO: Check if it still works when number of cameras is reduced.\n",
    "        cams = self.choose_cams() # 'CAM_FRONT' to 'CAM_BACK' (total 6)\n",
    "        \n",
    "        for key_idx in self.key_idxes: # [0, -1] default (current frame and previous frame for stereo depth)\n",
    "            cur_idx = key_idx + idx\n",
    "            # Handle scenarios when current idx doesn't have previous key\n",
    "            # frame or previous key frame is from another scene.\n",
    "            if cur_idx < 0:\n",
    "                cur_idx = idx\n",
    "            elif self.infos[cur_idx]['scene_token'] != self.infos[idx]['scene_token']:\n",
    "                cur_idx = idx\n",
    "            \n",
    "            info = self.infos[cur_idx]\n",
    "            cam_infos.append(info['cam_infos'])\n",
    "            lidar_infos.append(info['lidar_infos'])\n",
    "            lidar_sweep_timestamps = [\n",
    "                lidar_sweep['LIDAR_TOP']['timestamp']\n",
    "                for lidar_sweep in info['lidar_sweeps']\n",
    "            ]\n",
    "            \n",
    "            \"\"\"\n",
    "            sweep_idx가 non-empty list라면 (즉 sweep frame fusion을 활용하는 경우) 아래 for문 수행\n",
    "            Sweep frames 얻기 위한 정보 list에 추가 (cam_infos, lidar_infos)\n",
    "            \"\"\"\n",
    "            for sweep_idx in self.sweeps_idx:\n",
    "                # 현재 key frame에 매칭되는 sweep frames가 없는 경우\n",
    "                if len(info['cam_sweeps']) == 0:\n",
    "                    cam_infos.append(info['cam_infos'])\n",
    "                    lidar_infos.append(info['lidar_infos'])\n",
    "                else:\n",
    "                    # Handle scenarios when current sweep doesn't have all cam keys.\n",
    "                    for i in range(min(len(info['cam_sweeps']) - 1, sweep_idx), -1, -1):\n",
    "                        if sum([cam in info['cam_sweeps'][i] for cam in cams]) == len(cams):\n",
    "                            cam_infos.append(info['cam_sweeps'][i])\n",
    "                            cam_timestamp = np.mean([\n",
    "                                val['timestamp']\n",
    "                                for val in info['cam_sweeps'][i].values()\n",
    "                            ])\n",
    "                            \n",
    "                            # Find the closest lidar frame to the cam frame.\n",
    "                            lidar_idx = np.abs(lidar_sweep_timestamps - cam_timestamp).argmin()\n",
    "                            lidar_infos.append(info['lidar_sweeps'][lidar_idx])\n",
    "                            break\n",
    "\n",
    "        \"\"\"key frame 및 sweep frames의 image, meta-data 획득\"\"\"\n",
    "        if self.return_depth or self.use_fusion:\n",
    "            image_data_list = self.get_image(cam_infos, cams, lidar_infos)\n",
    "        else:\n",
    "            image_data_list = self.get_image(cam_infos, cams)\n",
    "            \n",
    "        ret_list = list()\n",
    "        (\n",
    "            sweep_imgs,\n",
    "            sweep_sensor2ego_mats,\n",
    "            sweep_intrins,\n",
    "            sweep_ida_mats,\n",
    "            sweep_sensor2sensor_mats,\n",
    "            sweep_timestamps,\n",
    "            img_metas,\n",
    "        ) = image_data_list[:7]\n",
    "        \n",
    "        img_metas['token'] = self.infos[idx]['sample_token']\n",
    "        if self.is_train:\n",
    "            gt_boxes, gt_labels = self.get_gt(self.infos[idx], cams)\n",
    "        # Temporary solution for test.\n",
    "        else:\n",
    "            gt_boxes = sweep_imgs.new_zeros(0, 7)\n",
    "            gt_labels = sweep_imgs.new_zeros(0, )\n",
    "\n",
    "        rotate_bda, scale_bda, flip_dx, flip_dy = self.sample_bda_augmentation()\n",
    "        bda_mat = sweep_imgs.new_zeros(4, 4)\n",
    "        bda_mat[3, 3] = 1\n",
    "        gt_boxes, bda_rot = bev_transform(gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy)\n",
    "        bda_mat[:3, :3] = bda_rot\n",
    "        ret_list = [\n",
    "            sweep_imgs,\n",
    "            sweep_sensor2ego_mats,\n",
    "            sweep_intrins,\n",
    "            sweep_ida_mats,\n",
    "            sweep_sensor2sensor_mats,\n",
    "            bda_mat,\n",
    "            sweep_timestamps,\n",
    "            img_metas,\n",
    "            gt_boxes,\n",
    "            gt_labels,\n",
    "        ]\n",
    "        if self.return_depth:\n",
    "            ret_list.append(image_data_list[7])\n",
    "        return ret_list\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"NuscData: {len(self)} samples. Split: \\\n",
    "            {\"train\" if self.is_train else \"val\"}.\n",
    "                    Augmentation Conf: {self.ida_aug_conf}\"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.use_cbgs:\n",
    "            return len(self.sample_indices)\n",
    "        else:\n",
    "            return len(self.infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62760927-ec22-4acf-a86a-3f01a09df4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data, is_return_depth=False):\n",
    "    imgs_batch = list()\n",
    "    sensor2ego_mats_batch = list()\n",
    "    intrin_mats_batch = list()\n",
    "    ida_mats_batch = list()\n",
    "    sensor2sensor_mats_batch = list()\n",
    "    bda_mat_batch = list()\n",
    "    timestamps_batch = list()\n",
    "    gt_boxes_batch = list()\n",
    "    gt_labels_batch = list()\n",
    "    img_metas_batch = list()\n",
    "    depth_labels_batch = list()\n",
    "    print(data)\n",
    "    for iter_data in data:\n",
    "        (\n",
    "            sweep_imgs,\n",
    "            sweep_sensor2ego_mats,\n",
    "            sweep_intrins,\n",
    "            sweep_ida_mats,\n",
    "            sweep_sensor2sensor_mats,\n",
    "            bda_mat,\n",
    "            sweep_timestamps,\n",
    "            img_metas,\n",
    "            gt_boxes,\n",
    "            gt_labels,\n",
    "        ) = iter_data[:10]\n",
    "        if is_return_depth:\n",
    "            gt_depth = iter_data[10]\n",
    "            depth_labels_batch.append(gt_depth)\n",
    "        imgs_batch.append(sweep_imgs)\n",
    "        sensor2ego_mats_batch.append(sweep_sensor2ego_mats)\n",
    "        intrin_mats_batch.append(sweep_intrins)\n",
    "        ida_mats_batch.append(sweep_ida_mats)\n",
    "        sensor2sensor_mats_batch.append(sweep_sensor2sensor_mats)\n",
    "        bda_mat_batch.append(bda_mat)\n",
    "        timestamps_batch.append(sweep_timestamps)\n",
    "        img_metas_batch.append(img_metas)\n",
    "        gt_boxes_batch.append(gt_boxes)\n",
    "        gt_labels_batch.append(gt_labels)\n",
    "    mats_dict = dict()\n",
    "    mats_dict['sensor2ego_mats'] = torch.stack(sensor2ego_mats_batch)\n",
    "    mats_dict['intrin_mats'] = torch.stack(intrin_mats_batch)\n",
    "    mats_dict['ida_mats'] = torch.stack(ida_mats_batch)\n",
    "    mats_dict['sensor2sensor_mats'] = torch.stack(sensor2sensor_mats_batch)\n",
    "    mats_dict['bda_mat'] = torch.stack(bda_mat_batch)\n",
    "    ret_list = [\n",
    "        torch.stack(imgs_batch),\n",
    "        mats_dict,\n",
    "        torch.stack(timestamps_batch),\n",
    "        img_metas_batch,\n",
    "        gt_boxes_batch,\n",
    "        gt_labels_batch,\n",
    "    ]\n",
    "    if is_return_depth:\n",
    "        ret_list.append(torch.stack(depth_labels_batch))\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7fca037-631f-493d-985e-97a701197d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화\n",
    "infos = None\n",
    "data_root = '../data/nuscenes/'\n",
    "# data_root = '../data/simple_road_outdoor_oct_23/'\n",
    "info_paths = os.path.join('../data/nuscenes/', 'nuscenes_infos_val.pkl')\n",
    "\n",
    "is_train = False\n",
    "use_cbgs = False\n",
    "num_sweeps = 1\n",
    "img_conf=dict(img_mean=[123.675, 116.28, 103.53],\n",
    "           img_std=[58.395, 57.12, 57.375],\n",
    "           to_rgb=True)\n",
    "return_depth=False\n",
    "sweep_idxes=list()\n",
    "key_idxes = [-1]\n",
    "use_fusion=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "581ed913-1b28-49b0-9a2c-d6a4146ea2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infos = mmcv.load(info_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e2915d9-f78b-4913-a79d-de130d456fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Insta360DetDataset(ida_aug_conf=ida_aug_conf, \n",
    "                             bda_aug_conf=bda_aug_conf, \n",
    "                             classes=classes, \n",
    "                             data_root=data_root, \n",
    "                             info_paths=info_paths, \n",
    "                             is_train=is_train, \n",
    "                             use_cbgs=use_cbgs,\n",
    "                             num_sweeps=num_sweeps,\n",
    "                             img_conf=img_conf,\n",
    "                             return_depth=return_depth,\n",
    "                             sweep_idxes=sweep_idxes,\n",
    "                             key_idxes=key_idxes,\n",
    "                             use_fusion=use_fusion\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69570768-5f29-4615-92e9-e7b31394d279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000e+00, -9.9019e-10, -2.9257e-08, -2.0266e-06],\n",
       "          [ 1.1053e-09,  1.0000e+00, -1.4283e-09, -1.0729e-06],\n",
       "          [-3.3506e-08,  6.6716e-10,  1.0000e+00,  5.4479e-05],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00,  1.6161e-09, -3.1413e-09, -6.6641e-05],\n",
       "          [-2.4650e-10,  1.0000e+00,  1.9030e-10,  2.5034e-06],\n",
       "          [ 1.4799e-08, -7.4101e-10,  1.0000e+00, -1.2159e-05],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00, -4.1103e-10, -3.0601e-08,  4.7684e-07],\n",
       "          [-5.5932e-10,  1.0000e+00, -5.3595e-10,  8.3446e-07],\n",
       "          [-8.0239e-10, -7.7139e-10,  1.0000e+00,  9.2983e-06],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00,  3.4563e-10, -3.3821e-08, -4.1842e-05],\n",
       "          [-1.5349e-09,  1.0000e+00, -2.8968e-09, -2.3842e-06],\n",
       "          [-4.0153e-09, -9.8796e-10,  1.0000e+00, -6.5044e-06],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00,  4.2134e-10, -1.0893e-09, -7.1805e-05],\n",
       "          [ 1.3639e-09,  1.0000e+00, -2.0067e-09,  3.5763e-07],\n",
       "          [ 1.3965e-08, -1.0645e-11,  1.0000e+00,  1.1560e-05],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00,  1.6678e-10,  1.6211e-08, -2.7895e-05],\n",
       "          [-4.0799e-10,  1.0000e+00, -1.5728e-09,  3.5763e-07],\n",
       "          [ 5.6871e-09,  2.1275e-10,  1.0000e+00,  1.7338e-05],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000e+00, -9.9019e-10, -2.9257e-08, -2.0266e-06],\n",
       "          [ 1.1053e-09,  1.0000e+00, -1.4283e-09, -1.0729e-06],\n",
       "          [-3.3506e-08,  6.6716e-10,  1.0000e+00,  5.4479e-05],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00,  1.6161e-09, -3.1413e-09, -6.6641e-05],\n",
       "          [-2.4650e-10,  1.0000e+00,  1.9030e-10,  2.5034e-06],\n",
       "          [ 1.4799e-08, -7.4101e-10,  1.0000e+00, -1.2159e-05],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00, -4.1103e-10, -3.0601e-08,  4.7684e-07],\n",
       "          [-5.5932e-10,  1.0000e+00, -5.3595e-10,  8.3446e-07],\n",
       "          [-8.0239e-10, -7.7139e-10,  1.0000e+00,  9.2983e-06],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00,  3.4563e-10, -3.3821e-08, -4.1842e-05],\n",
       "          [-1.5349e-09,  1.0000e+00, -2.8968e-09, -2.3842e-06],\n",
       "          [-4.0153e-09, -9.8796e-10,  1.0000e+00, -6.5044e-06],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00,  4.2134e-10, -1.0893e-09, -7.1805e-05],\n",
       "          [ 1.3639e-09,  1.0000e+00, -2.0067e-09,  3.5763e-07],\n",
       "          [ 1.3965e-08, -1.0645e-11,  1.0000e+00,  1.1560e-05],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[ 1.0000e+00,  1.6678e-10,  1.6211e-08, -2.7895e-05],\n",
       "          [-4.0799e-10,  1.0000e+00, -1.5728e-09,  3.5763e-07],\n",
       "          [ 5.6871e-09,  2.1275e-10,  1.0000e+00,  1.7338e-05],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset.__getitem__(0)\n",
    "sensor2sensor_mat = data[4]\n",
    "sensor2sensor_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04056585-a1d9-4980-9a3e-db1d2f8275cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e312bc-20dd-4c38-9a8a-938c0bb23257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37461048-005e-4d90-95e8-ee09405f83d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c4c37-5e05-4869-a874-1613a5bf0c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11cb35d-d182-4eef-aef1-5e95eaa46e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54c02f-13fd-41bb-ae49-87f662526798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238737e-cfda-496e-9012-38a12cb12f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29ff61-ea9c-4e2d-be4a-dd5858a5f313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicv",
   "language": "python",
   "name": "omnicv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
