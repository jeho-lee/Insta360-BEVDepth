{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2170d01d-b687-4da0-bd0e-173b80ee5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numba\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from torch.cuda.amp.autocast_mode import autocast\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch import nn\n",
    "from torch.cuda.amp.autocast_mode import autocast\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "import mmcv\n",
    "from mmcv.cnn import build_conv_layer\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "from mmdet.models import build_backbone\n",
    "from mmdet.models.backbones.resnet import BasicBlock\n",
    "from mmdet.core import reduce_mean\n",
    "\n",
    "from mmdet3d.core import draw_heatmap_gaussian, gaussian_radius\n",
    "from mmdet3d.models import build_neck\n",
    "from mmdet3d.models.dense_heads.centerpoint_head import CenterHead, circle_nms\n",
    "from mmdet3d.models.utils import clip_sigmoid\n",
    "from mmdet3d.models import build_neck\n",
    "\n",
    "from bevdepth.evaluators.det_evaluators import DetNuscEvaluator\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from bevdepth.callbacks.ema import EMACallback\n",
    "from bevdepth.utils.torch_dist import all_gather_object, get_rank, synchronize\n",
    "\n",
    "try:\n",
    "    from bevdepth.ops.voxel_pooling import voxel_pooling\n",
    "except ImportError:\n",
    "    print('Import VoxelPooling fail.')\n",
    "\n",
    "from mmdet3d.core.bbox.structures.lidar_box3d import LiDARInstance3DBoxes\n",
    "from nuscenes.utils.data_classes import Box, LidarPointCloud\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from PIL import Image\n",
    "from pyquaternion import Quaternion\n",
    "import pyquaternion\n",
    "\n",
    "import cv2\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from nuscenes.utils.data_classes import Box, LidarPointCloud\n",
    "from collections import OrderedDict\n",
    "\n",
    "# from bevdepth.datasets.insta360_det_dataset import map_name_from_general_to_detection\n",
    "# from bevdepth.datasets.insta360_det_dataset import NuscDetDataset, collate_fn\n",
    "\n",
    "import glob\n",
    "from math import pi\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7fde8d-0805-4c52-8f7d-9e8617187ac9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b5a882-5cde-4fb0-9f0f-44c648b99156",
   "metadata": {},
   "outputs": [],
   "source": [
    "infos_path = '../data/nuscenes/nuscenes_infos_val.pkl'\n",
    "infos = mmcv.load(infos_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e19e5a-e686-44cc-af34-6f082a8413d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Base Backbone (BEVDepth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ffedbd-6ef2-4da4-9fb9-a9ff379d88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from layers/backbones/base_lss_fpn\"\"\"\n",
    "\n",
    "class _ASPPModule(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size, padding, dilation,\n",
    "                 BatchNorm):\n",
    "        super(_ASPPModule, self).__init__()\n",
    "        self.atrous_conv = nn.Conv2d(inplanes,\n",
    "                                     planes,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     stride=1,\n",
    "                                     padding=padding,\n",
    "                                     dilation=dilation,\n",
    "                                     bias=False)\n",
    "        self.bn = BatchNorm(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.atrous_conv(x)\n",
    "        x = self.bn(x)\n",
    "\n",
    "        return self.relu(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "class ASPP(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, mid_channels=256, BatchNorm=nn.BatchNorm2d):\n",
    "        super(ASPP, self).__init__()\n",
    "\n",
    "        dilations = [1, 6, 12, 18]\n",
    "\n",
    "        self.aspp1 = _ASPPModule(inplanes,\n",
    "                                 mid_channels,\n",
    "                                 1,\n",
    "                                 padding=0,\n",
    "                                 dilation=dilations[0],\n",
    "                                 BatchNorm=BatchNorm)\n",
    "        self.aspp2 = _ASPPModule(inplanes,\n",
    "                                 mid_channels,\n",
    "                                 3,\n",
    "                                 padding=dilations[1],\n",
    "                                 dilation=dilations[1],\n",
    "                                 BatchNorm=BatchNorm)\n",
    "        self.aspp3 = _ASPPModule(inplanes,\n",
    "                                 mid_channels,\n",
    "                                 3,\n",
    "                                 padding=dilations[2],\n",
    "                                 dilation=dilations[2],\n",
    "                                 BatchNorm=BatchNorm)\n",
    "        self.aspp4 = _ASPPModule(inplanes,\n",
    "                                 mid_channels,\n",
    "                                 3,\n",
    "                                 padding=dilations[3],\n",
    "                                 dilation=dilations[3],\n",
    "                                 BatchNorm=BatchNorm)\n",
    "\n",
    "        self.global_avg_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(inplanes, mid_channels, 1, stride=1, bias=False),\n",
    "            BatchNorm(mid_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(int(mid_channels * 5),\n",
    "                               mid_channels,\n",
    "                               1,\n",
    "                               bias=False)\n",
    "        self.bn1 = BatchNorm(mid_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = self.global_avg_pool(x)\n",
    "        x5 = F.interpolate(x5,\n",
    "                           size=x4.size()[2:],\n",
    "                           mode='bilinear',\n",
    "                           align_corners=True)\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return self.dropout(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features=None,\n",
    "                 out_features=None,\n",
    "                 act_layer=nn.ReLU,\n",
    "                 drop=0.0):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop2 = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, act_layer=nn.ReLU, gate_layer=nn.Sigmoid):\n",
    "        super().__init__()\n",
    "        self.conv_reduce = nn.Conv2d(channels, channels, 1, bias=True)\n",
    "        self.act1 = act_layer()\n",
    "        self.conv_expand = nn.Conv2d(channels, channels, 1, bias=True)\n",
    "        self.gate = gate_layer()\n",
    "\n",
    "    def forward(self, x, x_se):\n",
    "        x_se = self.conv_reduce(x_se)\n",
    "        x_se = self.act1(x_se)\n",
    "        x_se = self.conv_expand(x_se)\n",
    "        return x * self.gate(x_se)\n",
    "\n",
    "class DepthAggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    pixel cloud feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(DepthAggregation, self).__init__()\n",
    "\n",
    "        self.reduce_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      mid_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels,\n",
    "                      mid_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels,\n",
    "                      mid_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.out_conv = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=True),\n",
    "            # nn.BatchNorm3d(out_channels),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    @autocast(False)\n",
    "    def forward(self, x):\n",
    "        x = self.reduce_conv(x)\n",
    "        x = self.conv(x) + x\n",
    "        x = self.out_conv(x)\n",
    "        return x\n",
    "\n",
    "class BaseLSSFPN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 x_bound,\n",
    "                 y_bound,\n",
    "                 z_bound,\n",
    "                 d_bound,\n",
    "                 final_dim,\n",
    "                 downsample_factor,\n",
    "                 output_channels,\n",
    "                 img_backbone_conf,\n",
    "                 img_neck_conf,\n",
    "                 depth_net_conf,\n",
    "                 device='cuda:0',\n",
    "                 use_da=True): # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 임의로 바꿈\n",
    "        \"\"\"Modified from `https://github.com/nv-tlabs/lift-splat-shoot`.\n",
    "\n",
    "        Args:\n",
    "            x_bound (list): Boundaries for x.\n",
    "            y_bound (list): Boundaries for y.\n",
    "            z_bound (list): Boundaries for z.\n",
    "            d_bound (list): Boundaries for d.\n",
    "            final_dim (list): Dimension for input images.\n",
    "            downsample_factor (int): Downsample factor between feature map\n",
    "                and input image.\n",
    "            output_channels (int): Number of channels for the output\n",
    "                feature map.\n",
    "            img_backbone_conf (dict): Config for image backbone.\n",
    "            img_neck_conf (dict): Config for image neck.\n",
    "            depth_net_conf (dict): Config for depth net.\n",
    "        \"\"\"\n",
    "\n",
    "        super(BaseLSSFPN, self).__init__()\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.d_bound = d_bound\n",
    "        self.final_dim = final_dim\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        self.register_buffer(\n",
    "            'voxel_size',\n",
    "            torch.Tensor([row[2] for row in [x_bound, y_bound, z_bound]]))\n",
    "        self.register_buffer(\n",
    "            'voxel_coord',\n",
    "            torch.Tensor([\n",
    "                row[0] + row[2] / 2.0 for row in [x_bound, y_bound, z_bound]\n",
    "            ]))\n",
    "        self.register_buffer(\n",
    "            'voxel_num',\n",
    "            torch.LongTensor([(row[1] - row[0]) / row[2]\n",
    "                              for row in [x_bound, y_bound, z_bound]]))\n",
    "        self.register_buffer('frustum', self.create_frustum())\n",
    "        self.depth_channels, _, _, _ = self.frustum.shape\n",
    "\n",
    "        self.img_backbone = build_backbone(img_backbone_conf)\n",
    "        self.img_neck = build_neck(img_neck_conf)\n",
    "        self.depth_net = self._configure_depth_net(depth_net_conf)\n",
    "\n",
    "        self.img_neck.init_weights()\n",
    "        self.img_backbone.init_weights()\n",
    "        self.use_da = use_da\n",
    "        if self.use_da:\n",
    "            self.depth_aggregation_net = self._configure_depth_aggregation_net()\n",
    "            \n",
    "        self.device = device\n",
    "\n",
    "    def _configure_depth_net(self, depth_net_conf):\n",
    "        return DepthNet(\n",
    "            depth_net_conf['in_channels'],\n",
    "            depth_net_conf['mid_channels'],\n",
    "            self.output_channels,\n",
    "            self.depth_channels,\n",
    "        )\n",
    "\n",
    "    def _configure_depth_aggregation_net(self):\n",
    "        \"\"\"build pixel cloud feature extractor\"\"\"\n",
    "        return DepthAggregation(self.output_channels, self.output_channels, self.output_channels)\n",
    "\n",
    "    def _forward_voxel_net(self, img_feat_with_depth):\n",
    "        if self.use_da:\n",
    "            # BEVConv2D [n, c, d, h, w] -> [n, h, c, w, d]\n",
    "            img_feat_with_depth = img_feat_with_depth.permute(\n",
    "                0, 3, 1, 4,\n",
    "                2).contiguous()  # [n, c, d, h, w] -> [n, h, c, w, d]\n",
    "            n, h, c, w, d = img_feat_with_depth.shape\n",
    "            img_feat_with_depth = img_feat_with_depth.view(-1, c, w, d)\n",
    "            img_feat_with_depth = (self.depth_aggregation_net(img_feat_with_depth).view(\n",
    "                    n, h, c, w, d).permute(0, 2, 4, 1, 3).contiguous().float())\n",
    "        return img_feat_with_depth\n",
    "\n",
    "    def create_frustum(self):\n",
    "        \"\"\"Generate frustum\"\"\"\n",
    "        # make grid in image plane\n",
    "        ogfH, ogfW = self.final_dim\n",
    "        fH, fW = ogfH // self.downsample_factor, ogfW // self.downsample_factor\n",
    "        d_coords = torch.arange(*self.d_bound, dtype=torch.float).view(-1, 1, 1).expand(-1, fH, fW)\n",
    "        D, _, _ = d_coords.shape\n",
    "        x_coords = torch.linspace(0, ogfW - 1, fW, dtype=torch.float).view(1, 1, fW).expand(D, fH, fW)\n",
    "        y_coords = torch.linspace(0, ogfH - 1, fH, dtype=torch.float).view(1, fH, 1).expand(D, fH, fW)\n",
    "        paddings = torch.ones_like(d_coords)\n",
    "\n",
    "        # D x H x W x 3\n",
    "        frustum = torch.stack((x_coords, y_coords, d_coords, paddings), -1)\n",
    "        return frustum\n",
    "\n",
    "    def get_geometry(self, sensor2ego_mat, intrin_mat, ida_mat, bda_mat):\n",
    "        \"\"\"Transfer points from camera coord to ego coord.\n",
    "\n",
    "        Args:\n",
    "            rots(Tensor): Rotation matrix from camera to ego.\n",
    "            trans(Tensor): Translation matrix from camera to ego.\n",
    "            intrins(Tensor): Intrinsic matrix.\n",
    "            post_rots_ida(Tensor): Rotation matrix for ida.\n",
    "            post_trans_ida(Tensor): Translation matrix for ida\n",
    "            post_rot_bda(Tensor): Rotation matrix for bda.\n",
    "\n",
    "        Returns:\n",
    "            Tensors: points ego coord.\n",
    "        \"\"\"\n",
    "        batch_size, num_cams, _, _ = sensor2ego_mat.shape\n",
    "\n",
    "        # undo post-transformation\n",
    "        # B x N x D x H x W x 3\n",
    "        points = self.frustum\n",
    "        ida_mat = ida_mat.view(batch_size, num_cams, 1, 1, 1, 4, 4)\n",
    "        points = ida_mat.inverse().matmul(points.unsqueeze(-1))\n",
    "        # cam_to_ego\n",
    "        points = torch.cat(\n",
    "            (points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3],\n",
    "             points[:, :, :, :, :, 2:]), 5)\n",
    "\n",
    "        combine = sensor2ego_mat.matmul(torch.inverse(intrin_mat))\n",
    "        points = combine.view(batch_size, num_cams, 1, 1, 1, 4, 4).matmul(points)\n",
    "        if bda_mat is not None:\n",
    "            bda_mat = bda_mat.unsqueeze(1).repeat(1, num_cams, 1, 1).view(\n",
    "                batch_size, num_cams, 1, 1, 1, 4, 4)\n",
    "            points = (bda_mat @ points).squeeze(-1)\n",
    "        else:\n",
    "            points = points.squeeze(-1)\n",
    "        return points[..., :3]\n",
    "\n",
    "    def get_cam_feats(self, imgs):\n",
    "        \"\"\"Get feature maps from images.\"\"\"\n",
    "        batch_size, num_sweeps, num_cams, num_channels, imH, imW = imgs.shape\n",
    "\n",
    "        imgs = imgs.flatten().view(batch_size * num_sweeps * num_cams,\n",
    "                                   num_channels, imH, imW)\n",
    "        img_feats = self.img_neck(self.img_backbone(imgs))[0]\n",
    "        img_feats = img_feats.reshape(batch_size, num_sweeps, num_cams,\n",
    "                                      img_feats.shape[1], img_feats.shape[2],\n",
    "                                      img_feats.shape[3])\n",
    "        return img_feats\n",
    "\n",
    "    def _forward_depth_net(self, feat, mats_dict):\n",
    "        return self.depth_net(feat, mats_dict)\n",
    "\n",
    "    def _forward_single_sweep(self,\n",
    "                              sweep_index,\n",
    "                              sweep_imgs,\n",
    "                              mats_dict,\n",
    "                              is_return_depth=False):\n",
    "        \"\"\"Forward function for single sweep.\n",
    "\n",
    "        Args:\n",
    "            sweep_index (int): Index of sweeps.\n",
    "            sweep_imgs (Tensor): Input images.\n",
    "            mats_dict (dict):\n",
    "                sensor2ego_mats(Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats(Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats(Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats(Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat(Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            is_return_depth (bool, optional): Whether to return depth.\n",
    "                Default: False.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: BEV feature map.\n",
    "        \"\"\"\n",
    "        batch_size, num_sweeps, num_cams, num_channels, img_height, \\\n",
    "            img_width = sweep_imgs.shape\n",
    "        img_feats = self.get_cam_feats(sweep_imgs)\n",
    "        source_features = img_feats[:, 0, ...]\n",
    "        depth_feature = self._forward_depth_net(\n",
    "            source_features.reshape(batch_size * num_cams,\n",
    "                                    source_features.shape[2],\n",
    "                                    source_features.shape[3],\n",
    "                                    source_features.shape[4]),\n",
    "            mats_dict,\n",
    "        )\n",
    "        depth = depth_feature[:, :self.depth_channels].softmax(1)\n",
    "        img_feat_with_depth = depth.unsqueeze(\n",
    "            1) * depth_feature[:, self.depth_channels:(\n",
    "                self.depth_channels + self.output_channels)].unsqueeze(2)\n",
    "\n",
    "        img_feat_with_depth = self._forward_voxel_net(img_feat_with_depth)\n",
    "\n",
    "        img_feat_with_depth = img_feat_with_depth.reshape(\n",
    "            batch_size,\n",
    "            num_cams,\n",
    "            img_feat_with_depth.shape[1],\n",
    "            img_feat_with_depth.shape[2],\n",
    "            img_feat_with_depth.shape[3],\n",
    "            img_feat_with_depth.shape[4],\n",
    "        )\n",
    "        geom_xyz = self.get_geometry(\n",
    "            mats_dict['sensor2ego_mats'][:, sweep_index, ...],\n",
    "            mats_dict['intrin_mats'][:, sweep_index, ...],\n",
    "            mats_dict['ida_mats'][:, sweep_index, ...],\n",
    "            mats_dict.get('bda_mat', None),\n",
    "        )\n",
    "        img_feat_with_depth = img_feat_with_depth.permute(0, 1, 3, 4, 5, 2)\n",
    "        geom_xyz = ((geom_xyz - (self.voxel_coord - self.voxel_size / 2.0)) /\n",
    "                    self.voxel_size).int()\n",
    "        \n",
    "        # feature_map = voxel_pooling(geom_xyz, img_feat_with_depth.contiguous(), self.voxel_num.cuda())\n",
    "        feature_map = voxel_pooling(geom_xyz, img_feat_with_depth.contiguous(), self.voxel_num.to(self.device))\n",
    "        \n",
    "        if is_return_depth:\n",
    "            return feature_map.contiguous(), depth\n",
    "        return feature_map.contiguous()\n",
    "\n",
    "    def forward(self,\n",
    "                sweep_imgs,\n",
    "                mats_dict,\n",
    "                timestamps=None,\n",
    "                is_return_depth=False):\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            sweep_imgs(Tensor): Input images with shape of (B, num_sweeps,\n",
    "                num_cameras, 3, H, W).\n",
    "            mats_dict(dict):\n",
    "                sensor2ego_mats(Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats(Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats(Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats(Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat(Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            timestamps(Tensor): Timestamp for all images with the shape of(B,\n",
    "                num_sweeps, num_cameras).\n",
    "\n",
    "        Return:\n",
    "            Tensor: bev feature map.\n",
    "        \"\"\"\n",
    "        batch_size, num_sweeps, num_cams, num_channels, img_height, img_width = sweep_imgs.shape\n",
    "\n",
    "        key_frame_res = self._forward_single_sweep(\n",
    "            0,\n",
    "            sweep_imgs[:, 0:1, ...],\n",
    "            mats_dict,\n",
    "            is_return_depth=is_return_depth)\n",
    "        if num_sweeps == 1:\n",
    "            return key_frame_res\n",
    "\n",
    "        key_frame_feature = key_frame_res[\n",
    "            0] if is_return_depth else key_frame_res\n",
    "\n",
    "        ret_feature_list = [key_frame_feature]\n",
    "        for sweep_index in range(1, num_sweeps):\n",
    "            with torch.no_grad():\n",
    "                feature_map = self._forward_single_sweep(\n",
    "                    sweep_index,\n",
    "                    sweep_imgs[:, sweep_index:sweep_index + 1, ...],\n",
    "                    mats_dict,\n",
    "                    is_return_depth=False)\n",
    "                ret_feature_list.append(feature_map)\n",
    "\n",
    "        if is_return_depth:\n",
    "            return torch.cat(ret_feature_list, 1), key_frame_res[1]\n",
    "        else:\n",
    "            return torch.cat(ret_feature_list, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aee07e-f23b-485a-91ab-ee611ed79bc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BEVStereo Backbone (LSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42db3824-356f-4f73-910d-3b7ce00ccce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from layers/backbones/bevstereo_lss_fpn.py\"\"\"\n",
    "\n",
    "class ConvBnReLU3D(nn.Module):\n",
    "    \"\"\"Implements of 3d convolution + batch normalization + ReLU.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        pad: int = 1,\n",
    "        dilation: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"initialization method for convolution3D +\n",
    "            batch normalization + relu module\n",
    "        Args:\n",
    "            in_channels: input channel number of convolution layer\n",
    "            out_channels: output channel number of convolution layer\n",
    "            kernel_size: kernel size of convolution layer\n",
    "            stride: stride of convolution layer\n",
    "            pad: pad of convolution layer\n",
    "            dilation: dilation of convolution layer\n",
    "        \"\"\"\n",
    "        super(ConvBnReLU3D, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels,\n",
    "                              out_channels,\n",
    "                              kernel_size,\n",
    "                              stride=stride,\n",
    "                              padding=pad,\n",
    "                              dilation=dilation,\n",
    "                              bias=False)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"forward method\"\"\"\n",
    "        return F.relu(self.bn(self.conv(x)), inplace=True)\n",
    "\n",
    "\n",
    "class DepthNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 mid_channels,\n",
    "                 context_channels,\n",
    "                 depth_channels,\n",
    "                 d_bound,\n",
    "                 num_ranges=4):\n",
    "        super(DepthNet, self).__init__()\n",
    "        self.reduce_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      mid_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.context_conv = nn.Conv2d(mid_channels,\n",
    "                                      context_channels,\n",
    "                                      kernel_size=1,\n",
    "                                      stride=1,\n",
    "                                      padding=0)\n",
    "        self.bn = nn.BatchNorm1d(27)\n",
    "        self.depth_mlp = Mlp(27, mid_channels, mid_channels)\n",
    "        self.depth_se = SELayer(mid_channels)  # NOTE: add camera-aware\n",
    "        self.context_mlp = Mlp(27, mid_channels, mid_channels)\n",
    "        self.context_se = SELayer(mid_channels)  # NOTE: add camera-aware\n",
    "        self.depth_feat_conv = nn.Sequential(\n",
    "            BasicBlock(mid_channels, mid_channels),\n",
    "            BasicBlock(mid_channels, mid_channels),\n",
    "            ASPP(mid_channels, mid_channels),\n",
    "            build_conv_layer(cfg=dict(\n",
    "                type='DCN',\n",
    "                in_channels=mid_channels,\n",
    "                out_channels=mid_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                groups=4,\n",
    "                im2col_step=128,\n",
    "            )),\n",
    "        )\n",
    "        self.mu_sigma_range_net = nn.Sequential(\n",
    "            BasicBlock(mid_channels, mid_channels),\n",
    "            nn.ConvTranspose2d(mid_channels,\n",
    "                               mid_channels,\n",
    "                               3,\n",
    "                               stride=2,\n",
    "                               padding=1,\n",
    "                               output_padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(mid_channels,\n",
    "                               mid_channels,\n",
    "                               3,\n",
    "                               stride=2,\n",
    "                               padding=1,\n",
    "                               output_padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels,\n",
    "                      num_ranges * 3,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "        )\n",
    "        self.mono_depth_net = nn.Sequential(\n",
    "            BasicBlock(mid_channels, mid_channels),\n",
    "            nn.Conv2d(mid_channels,\n",
    "                      depth_channels,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "        )\n",
    "        self.d_bound = d_bound\n",
    "        self.num_ranges = num_ranges\n",
    "\n",
    "    # @autocast(False)\n",
    "    def forward(self, x, mats_dict, scale_depth_factor=1000.0):\n",
    "        B, _, H, W = x.shape\n",
    "        intrins = mats_dict['intrin_mats'][:, 0:1, ..., :3, :3]\n",
    "        batch_size = intrins.shape[0]\n",
    "        num_cams = intrins.shape[2]\n",
    "        ida = mats_dict['ida_mats'][:, 0:1, ...]\n",
    "        sensor2ego = mats_dict['sensor2ego_mats'][:, 0:1, ..., :3, :]\n",
    "        bda = mats_dict['bda_mat'].view(batch_size, 1, 1, 4,\n",
    "                                        4).repeat(1, 1, num_cams, 1, 1)\n",
    "        mlp_input = torch.cat(\n",
    "            [\n",
    "                torch.stack(\n",
    "                    [\n",
    "                        intrins[:, 0:1, ..., 0, 0],\n",
    "                        intrins[:, 0:1, ..., 1, 1],\n",
    "                        intrins[:, 0:1, ..., 0, 2],\n",
    "                        intrins[:, 0:1, ..., 1, 2],\n",
    "                        ida[:, 0:1, ..., 0, 0],\n",
    "                        ida[:, 0:1, ..., 0, 1],\n",
    "                        ida[:, 0:1, ..., 0, 3],\n",
    "                        ida[:, 0:1, ..., 1, 0],\n",
    "                        ida[:, 0:1, ..., 1, 1],\n",
    "                        ida[:, 0:1, ..., 1, 3],\n",
    "                        bda[:, 0:1, ..., 0, 0],\n",
    "                        bda[:, 0:1, ..., 0, 1],\n",
    "                        bda[:, 0:1, ..., 1, 0],\n",
    "                        bda[:, 0:1, ..., 1, 1],\n",
    "                        bda[:, 0:1, ..., 2, 2],\n",
    "                    ],\n",
    "                    dim=-1,\n",
    "                ),\n",
    "                sensor2ego.view(batch_size, 1, num_cams, -1),\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        mlp_input = self.bn(mlp_input.reshape(-1, mlp_input.shape[-1]))\n",
    "        x = self.reduce_conv(x)\n",
    "        context_se = self.context_mlp(mlp_input)[..., None, None]\n",
    "        context = self.context_se(x, context_se)\n",
    "        context = self.context_conv(context)\n",
    "        depth_se = self.depth_mlp(mlp_input)[..., None, None]\n",
    "        depth_feat = self.depth_se(x, depth_se)\n",
    "        depth_feat = self.depth_feat_conv(depth_feat)\n",
    "        mono_depth = self.mono_depth_net(depth_feat)\n",
    "        mu_sigma_score = self.mu_sigma_range_net(depth_feat)\n",
    "        d_coords = torch.arange(*self.d_bound,\n",
    "                                dtype=torch.float).reshape(1, -1, 1, 1).cuda()\n",
    "        d_coords = d_coords.repeat(B, 1, H, W)\n",
    "        mu = mu_sigma_score[:, 0:self.num_ranges, ...]\n",
    "        sigma = mu_sigma_score[:, self.num_ranges:2 * self.num_ranges, ...]\n",
    "        range_score = mu_sigma_score[:,\n",
    "                                     2 * self.num_ranges:3 * self.num_ranges,\n",
    "                                     ...]\n",
    "        sigma = F.elu(sigma) + 1.0 + 1e-10\n",
    "        return x, context, mu, sigma, range_score, mono_depth\n",
    "\n",
    "\n",
    "class BEVStereoLSSFPN(BaseLSSFPN):\n",
    "\n",
    "    def __init__(self,\n",
    "                 x_bound,\n",
    "                 y_bound,\n",
    "                 z_bound,\n",
    "                 d_bound,\n",
    "                 final_dim,\n",
    "                 downsample_factor,\n",
    "                 output_channels,\n",
    "                 img_backbone_conf,\n",
    "                 img_neck_conf,\n",
    "                 depth_net_conf,\n",
    "                 use_da=False,\n",
    "                 sampling_range=3,\n",
    "                 num_samples=3,\n",
    "                 stereo_downsample_factor=4,\n",
    "                 em_iteration=3,\n",
    "                 min_sigma=1,\n",
    "                 num_groups=8,\n",
    "                 num_ranges=4,\n",
    "                 range_list=[[2, 8], [8, 16], [16, 28], [28, 58]],\n",
    "                 k_list=None,\n",
    "                 use_mask=True):\n",
    "        \"\"\"Modified from `https://github.com/nv-tlabs/lift-splat-shoot`.\n",
    "        Args:\n",
    "            x_bound (list): Boundaries for x.\n",
    "            y_bound (list): Boundaries for y.\n",
    "            z_bound (list): Boundaries for z.\n",
    "            d_bound (list): Boundaries for d.\n",
    "            final_dim (list): Dimension for input images.\n",
    "            downsample_factor (int): Downsample factor between feature map\n",
    "                and input image.\n",
    "            output_channels (int): Number of channels for the output\n",
    "                feature map.\n",
    "            img_backbone_conf (dict): Config for image backbone.\n",
    "            img_neck_conf (dict): Config for image neck.\n",
    "            depth_net_conf (dict): Config for depth net.\n",
    "            sampling_range (int): The base range of sampling candidates.\n",
    "                Defaults to 3.\n",
    "            num_samples (int): Number of samples. Defaults to 3.\n",
    "            stereo_downsample_factor (int): Downsample factor from input image\n",
    "                and stereo depth. Defaults to 4.\n",
    "            em_iteration (int): Number of iterations for em. Defaults to 3.\n",
    "            min_sigma (float): Minimal value for sigma. Defaults to 1.\n",
    "            num_groups (int): Number of groups to keep after inner product.\n",
    "                Defaults to 8.\n",
    "            num_ranges (int): Number of split ranges. Defaults to 1.\n",
    "            range_list (list): Start and end of every range, Defaults to None.\n",
    "            k_list (list): Depth of all candidates inside the range.\n",
    "                Defaults to None.\n",
    "            use_mask (bool): Whether to use mask_net. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.num_ranges = num_ranges\n",
    "        self.sampling_range = sampling_range\n",
    "        self.num_samples = num_samples\n",
    "        super(BEVStereoLSSFPN,\n",
    "              self).__init__(x_bound, y_bound, z_bound, d_bound, final_dim,\n",
    "                             downsample_factor, output_channels,\n",
    "                             img_backbone_conf, img_neck_conf, depth_net_conf,\n",
    "                             use_da)\n",
    "\n",
    "        self.depth_channels, _, _, _ = self.frustum.shape\n",
    "        self.use_mask = use_mask\n",
    "        if k_list is None:\n",
    "            self.register_buffer('k_list', torch.Tensor(self.depth_sampling()))\n",
    "        else:\n",
    "            self.register_buffer('k_list', torch.Tensor(k_list))\n",
    "        self.stereo_downsample_factor = stereo_downsample_factor\n",
    "        self.em_iteration = em_iteration\n",
    "        self.register_buffer(\n",
    "            'depth_values',\n",
    "            torch.arange((self.d_bound[1] - self.d_bound[0]) / self.d_bound[2],\n",
    "                         dtype=torch.float))\n",
    "        self.num_groups = num_groups\n",
    "        self.similarity_net = nn.Sequential(\n",
    "            ConvBnReLU3D(in_channels=num_groups,\n",
    "                         out_channels=16,\n",
    "                         kernel_size=1,\n",
    "                         stride=1,\n",
    "                         pad=0),\n",
    "            ConvBnReLU3D(in_channels=16,\n",
    "                         out_channels=8,\n",
    "                         kernel_size=1,\n",
    "                         stride=1,\n",
    "                         pad=0),\n",
    "            nn.Conv3d(in_channels=8,\n",
    "                      out_channels=1,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "        )\n",
    "        if range_list is None:\n",
    "            range_length = (d_bound[1] - d_bound[0]) / num_ranges\n",
    "            self.range_list = [[\n",
    "                d_bound[0] + range_length * i,\n",
    "                d_bound[0] + range_length * (i + 1)\n",
    "            ] for i in range(num_ranges)]\n",
    "        else:\n",
    "            assert len(range_list) == num_ranges\n",
    "            self.range_list = range_list\n",
    "\n",
    "        self.min_sigma = min_sigma\n",
    "        self.depth_downsample_net = nn.Sequential(\n",
    "            nn.Conv2d(self.depth_channels, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, self.depth_channels, 1, 1, 0),\n",
    "        )\n",
    "        self.context_downsample_net = nn.Identity()\n",
    "        if self.use_mask:\n",
    "            self.mask_net = nn.Sequential(\n",
    "                nn.Conv2d(224, 64, 3, 1, 1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                BasicBlock(64, 64),\n",
    "                BasicBlock(64, 64),\n",
    "                nn.Conv2d(64, 1, 1, 1, 0),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def depth_sampling(self):\n",
    "        \"\"\"Generate sampling range of candidates.\n",
    "        Returns:\n",
    "            list[float]: List of all candidates.\n",
    "        \"\"\"\n",
    "        P_total = erf(self.sampling_range / np.sqrt(2))  # Probability covered by the sampling range\n",
    "        idx_list = np.arange(0, self.num_samples + 1)\n",
    "        p_list = (1 - P_total) / 2 + ((idx_list / self.num_samples) * P_total)\n",
    "        k_list = norm.ppf(p_list)\n",
    "        k_list = (k_list[1:] + k_list[:-1]) / 2\n",
    "        return list(k_list)\n",
    "\n",
    "    def _generate_cost_volume(\n",
    "        self,\n",
    "        sweep_index,\n",
    "        stereo_feats_all_sweeps,\n",
    "        mats_dict,\n",
    "        depth_sample,\n",
    "        depth_sample_frustum,\n",
    "        sensor2sensor_mats,\n",
    "    ):\n",
    "        \"\"\"Generate cost volume based on depth sample.\n",
    "        Args:\n",
    "            sweep_index (int): Index of sweep.\n",
    "            stereo_feats_all_sweeps (list[Tensor]): Stereo feature\n",
    "                of all sweeps.\n",
    "            mats_dict (dict):\n",
    "                sensor2ego_mats (Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats (Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats (Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats (Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat (Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            depth_sample (Tensor): Depth map of all candidates.\n",
    "            depth_sample_frustum (Tensor): Pre-generated frustum.\n",
    "            sensor2sensor_mats (Tensor): Transformation matrix from reference\n",
    "                sensor to source sensor.\n",
    "        Returns:\n",
    "            Tensor: Depth score for all sweeps.\n",
    "        \"\"\"\n",
    "        batch_size, num_channels, height, width = stereo_feats_all_sweeps[0].shape\n",
    "        num_sweeps = len(stereo_feats_all_sweeps)\n",
    "        depth_score_all_sweeps = list()\n",
    "        for idx in range(num_sweeps):\n",
    "            if idx == sweep_index: # 같은 시점의 frames (똑같은 두 frames)로는 cost volume 구축하지 않도록 (src <=> ref 사이에서만 구축)\n",
    "                continue\n",
    "            \n",
    "            warped_stereo_fea = self.homo_warping(\n",
    "                stereo_feats_all_sweeps[idx],\n",
    "                mats_dict['intrin_mats'][:, sweep_index, ...],\n",
    "                mats_dict['intrin_mats'][:, idx, ...],\n",
    "                sensor2sensor_mats[idx],\n",
    "                mats_dict['ida_mats'][:, sweep_index, ...],\n",
    "                mats_dict['ida_mats'][:, idx, ...],\n",
    "                depth_sample,\n",
    "                depth_sample_frustum.type_as(stereo_feats_all_sweeps[idx]),\n",
    "            )\n",
    "            warped_stereo_fea = warped_stereo_fea.reshape(\n",
    "                batch_size, self.num_groups, num_channels // self.num_groups,\n",
    "                self.num_samples, height, width)\n",
    "            ref_stereo_feat = stereo_feats_all_sweeps[sweep_index].reshape(\n",
    "                batch_size, self.num_groups, num_channels // self.num_groups,\n",
    "                height, width)\n",
    "            feat_cost = torch.mean(\n",
    "                (ref_stereo_feat.unsqueeze(3) * warped_stereo_fea), axis=2)\n",
    "            depth_score = self.similarity_net(feat_cost).squeeze(1)\n",
    "            depth_score_all_sweeps.append(depth_score)\n",
    "        return torch.stack(depth_score_all_sweeps).mean(0)\n",
    "\n",
    "    def homo_warping(\n",
    "        self,\n",
    "        stereo_feat,\n",
    "        key_intrin_mats,\n",
    "        sweep_intrin_mats,\n",
    "        sensor2sensor_mats,\n",
    "        key_ida_mats,\n",
    "        sweep_ida_mats,\n",
    "        depth_sample,\n",
    "        frustum,\n",
    "    ):\n",
    "        \"\"\"Used for mvs method to transfer sweep image feature to\n",
    "            key image feature.\n",
    "        Args:\n",
    "            src_fea(Tensor): image features.\n",
    "            key_intrin_mats(Tensor): Intrin matrix for key sensor.\n",
    "            sweep_intrin_mats(Tensor): Intrin matrix for sweep sensor.\n",
    "            sensor2sensor_mats(Tensor): Transformation matrix from key\n",
    "                sensor to sweep sensor.\n",
    "            key_ida_mats(Tensor): Ida matrix for key frame.\n",
    "            sweep_ida_mats(Tensor): Ida matrix for sweep frame.\n",
    "            depth_sample (Tensor): Depth map of all candidates.\n",
    "            depth_sample_frustum (Tensor): Pre-generated frustum.\n",
    "        \"\"\"\n",
    "        batch_size_with_num_cams, channels = stereo_feat.shape[\n",
    "            0], stereo_feat.shape[1]\n",
    "        height, width = stereo_feat.shape[2], stereo_feat.shape[3]\n",
    "        with torch.no_grad():\n",
    "            points = frustum\n",
    "            points = points.reshape(points.shape[0], -1, points.shape[-1])\n",
    "            points[..., 2] = 1\n",
    "            # Undo ida for key frame.\n",
    "            points = key_ida_mats.reshape(batch_size_with_num_cams, *\n",
    "                                          key_ida_mats.shape[2:]).inverse(\n",
    "                                          ).unsqueeze(1) @ points.unsqueeze(-1)\n",
    "            # Convert points from pixel coord to key camera coord.\n",
    "            points[..., :3, :] *= depth_sample.reshape(\n",
    "                batch_size_with_num_cams, -1, 1, 1)\n",
    "            num_depth = frustum.shape[1]\n",
    "            points = (key_intrin_mats.reshape(\n",
    "                batch_size_with_num_cams, *\n",
    "                key_intrin_mats.shape[2:]).inverse().unsqueeze(1) @ points)\n",
    "            points = (sensor2sensor_mats.reshape(\n",
    "                batch_size_with_num_cams, *\n",
    "                sensor2sensor_mats.shape[2:]).unsqueeze(1) @ points)\n",
    "            # points in sweep sensor coord.\n",
    "            points = (sweep_intrin_mats.reshape(\n",
    "                batch_size_with_num_cams, *\n",
    "                sweep_intrin_mats.shape[2:]).unsqueeze(1) @ points)\n",
    "            # points in sweep pixel coord.\n",
    "            points[..., :2, :] = points[..., :2, :] / points[\n",
    "                ..., 2:3, :]  # [B, 2, Ndepth, H*W]\n",
    "\n",
    "            points = (sweep_ida_mats.reshape(\n",
    "                batch_size_with_num_cams, *\n",
    "                sweep_ida_mats.shape[2:]).unsqueeze(1) @ points).squeeze(-1)\n",
    "            neg_mask = points[..., 2] < 1e-3\n",
    "            points[..., 0][neg_mask] = width * self.stereo_downsample_factor\n",
    "            points[..., 1][neg_mask] = height * self.stereo_downsample_factor\n",
    "            points[..., 2][neg_mask] = 1\n",
    "            proj_x_normalized = points[..., 0] / (\n",
    "                (width * self.stereo_downsample_factor - 1) / 2) - 1\n",
    "            proj_y_normalized = points[..., 1] / (\n",
    "                (height * self.stereo_downsample_factor - 1) / 2) - 1\n",
    "            grid = torch.stack([proj_x_normalized, proj_y_normalized],\n",
    "                               dim=2)  # [B, Ndepth, H*W, 2]\n",
    "\n",
    "        warped_stereo_fea = F.grid_sample(\n",
    "            stereo_feat,\n",
    "            grid.view(batch_size_with_num_cams, num_depth * height, width, 2),\n",
    "            mode='bilinear',\n",
    "            padding_mode='zeros',\n",
    "        )\n",
    "        warped_stereo_fea = warped_stereo_fea.view(batch_size_with_num_cams,\n",
    "                                                   channels, num_depth, height,\n",
    "                                                   width)\n",
    "\n",
    "        return warped_stereo_fea\n",
    "\n",
    "    def _forward_stereo(\n",
    "        self,\n",
    "        sweep_index,\n",
    "        stereo_feats_all_sweeps,\n",
    "        mono_depth_all_sweeps,\n",
    "        mats_dict,\n",
    "        sensor2sensor_mats,\n",
    "        mu_all_sweeps,\n",
    "        sigma_all_sweeps,\n",
    "        range_score_all_sweeps,\n",
    "        depth_feat_all_sweeps,\n",
    "    ):\n",
    "        \"\"\"Forward function to generate stereo depth.\n",
    "        Args:\n",
    "            sweep_index (int): Index of sweep.\n",
    "            stereo_feats_all_sweeps (list[Tensor]): Stereo feature of all sweeps.\n",
    "            mono_depth_all_sweeps (list[Tensor]):\n",
    "            mats_dict (dict):\n",
    "                sensor2ego_mats (Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats (Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats (Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats (Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat (Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            sensor2sensor_mats(Tensor): Transformation matrix from key\n",
    "                sensor to sweep sensor.\n",
    "            mu_all_sweeps (list[Tensor]): List of mu for all sweeps.\n",
    "            sigma_all_sweeps (list[Tensor]): List of sigma for all sweeps.\n",
    "            range_score_all_sweeps (list[Tensor]): List of all range score\n",
    "                for all sweeps.\n",
    "            depth_feat_all_sweeps (list[Tensor]): List of all depth feat for\n",
    "                all sweeps.\n",
    "        Returns:\n",
    "            Tensor: stereo_depth\n",
    "        \"\"\"\n",
    "        batch_size_with_cams, _, feat_height, feat_width = stereo_feats_all_sweeps[0].shape\n",
    "        device = stereo_feats_all_sweeps[0].device\n",
    "        d_coords = torch.arange(*self.d_bound,\n",
    "                                dtype=torch.float,\n",
    "                                device=device).reshape(1, -1, 1, 1)\n",
    "        d_coords = d_coords.repeat(batch_size_with_cams, 1, feat_height,\n",
    "                                   feat_width)\n",
    "        stereo_depth = stereo_feats_all_sweeps[0].new_zeros(\n",
    "            batch_size_with_cams, self.depth_channels, feat_height, feat_width)\n",
    "        mask_score = stereo_feats_all_sweeps[0].new_zeros(\n",
    "            batch_size_with_cams,\n",
    "            self.depth_channels,\n",
    "            feat_height * self.stereo_downsample_factor //\n",
    "            self.downsample_factor,\n",
    "            feat_width * self.stereo_downsample_factor //\n",
    "            self.downsample_factor,\n",
    "        )\n",
    "        score_all_ranges = list()\n",
    "        range_score = range_score_all_sweeps[sweep_index].softmax(1)\n",
    "        \n",
    "        for range_idx in range(self.num_ranges):\n",
    "            # Map mu to the corresponding interval.\n",
    "            range_start = self.range_list[range_idx][0]\n",
    "            mu_all_sweeps_single_range = [\n",
    "                mu[:, range_idx:range_idx + 1, ...].sigmoid() *\n",
    "                (self.range_list[range_idx][1] - self.range_list[range_idx][0])\n",
    "                + range_start for mu in mu_all_sweeps\n",
    "            ]\n",
    "            sigma_all_sweeps_single_range = [\n",
    "                sigma[:, range_idx:range_idx + 1, ...]\n",
    "                for sigma in sigma_all_sweeps\n",
    "            ]\n",
    "            batch_size_with_cams, _, feat_height, feat_width = stereo_feats_all_sweeps[0].shape\n",
    "            mu = mu_all_sweeps_single_range[sweep_index]\n",
    "            sigma = sigma_all_sweeps_single_range[sweep_index]\n",
    "            for _ in range(self.em_iteration):\n",
    "                depth_sample = torch.cat([mu + sigma * k for k in self.k_list], 1)\n",
    "                depth_sample_frustum = self.create_depth_sample_frustum(depth_sample, self.stereo_downsample_factor)\n",
    "                \n",
    "                \"\"\"\n",
    "                cost volumn 함수에 stereo features와 depth sample (mu와 sigma에 대한 함수의 아웃풋)가 input으로 들어감\n",
    "                mu와 sigma는 그럼 source image features에서 예측된 값? YES!\n",
    "                \n",
    "                BEVStereo의 key idea는 cost volumn을 구축할 때 모든 depth samples (depth map의 모든 pixels)를 고려하는 것이 아닌,\n",
    "                일부 depth samples (gen from mu and sigma)로부터 sparse cost volumn을 구축함!\n",
    "                \"\"\"\n",
    "                \n",
    "                mu_score = self._generate_cost_volume(\n",
    "                    sweep_index,\n",
    "                    stereo_feats_all_sweeps,\n",
    "                    mats_dict,\n",
    "                    depth_sample,\n",
    "                    depth_sample_frustum,\n",
    "                    sensor2sensor_mats,\n",
    "                )\n",
    "                mu_score = mu_score.softmax(1)\n",
    "                scale_factor = torch.clamp(\n",
    "                    0.5 / (1e-4 + mu_score[:, self.num_samples // 2:self.num_samples // 2 + 1, ...]),\n",
    "                    min=0.1,\n",
    "                    max=10)\n",
    "\n",
    "                sigma = torch.clamp(sigma * scale_factor, min=0.1, max=10)\n",
    "                mu = (depth_sample * mu_score).sum(1, keepdim=True)\n",
    "                del depth_sample\n",
    "                del depth_sample_frustum\n",
    "            range_length = int((self.range_list[range_idx][1] - self.range_list[range_idx][0]) // self.d_bound[2])\n",
    "            if self.use_mask:\n",
    "                depth_sample = F.avg_pool2d(\n",
    "                    mu,\n",
    "                    self.downsample_factor // self.stereo_downsample_factor,\n",
    "                    self.downsample_factor // self.stereo_downsample_factor,\n",
    "                )\n",
    "                depth_sample_frustum = self.create_depth_sample_frustum(depth_sample, self.downsample_factor)\n",
    "                mask = self._forward_mask(\n",
    "                    sweep_index,\n",
    "                    mono_depth_all_sweeps,\n",
    "                    mats_dict,\n",
    "                    depth_sample,\n",
    "                    depth_sample_frustum,\n",
    "                    sensor2sensor_mats,\n",
    "                )\n",
    "                mask_score[:,\n",
    "                           int((range_start - self.d_bound[0]) //\n",
    "                               self.d_bound[2]):range_length +\n",
    "                           int((range_start - self.d_bound[0]) //\n",
    "                               self.d_bound[2]), ..., ] += mask\n",
    "                del depth_sample\n",
    "                del depth_sample_frustum\n",
    "            sigma = torch.clamp(sigma, self.min_sigma)\n",
    "            mu_repeated = mu.repeat(1, range_length, 1, 1)\n",
    "            eps = 1e-6\n",
    "            depth_score_single_range = (-1 / 2 * (\n",
    "                (d_coords[:,\n",
    "                          int((range_start - self.d_bound[0]) //\n",
    "                              self.d_bound[2]):range_length + int(\n",
    "                                  (range_start - self.d_bound[0]) //\n",
    "                                  self.d_bound[2]), ..., ] - mu_repeated) /\n",
    "                torch.sqrt(sigma))**2)\n",
    "            depth_score_single_range = depth_score_single_range.exp()\n",
    "            score_all_ranges.append(mu_score.sum(1).unsqueeze(1))\n",
    "            depth_score_single_range = depth_score_single_range / (sigma * math.sqrt(2 * math.pi) + eps)\n",
    "            stereo_depth[:,\n",
    "                         int((range_start - self.d_bound[0]) //\n",
    "                             self.d_bound[2]):range_length +\n",
    "                         int((range_start - self.d_bound[0]) //\n",
    "                             self.d_bound[2]), ..., ] = (\n",
    "                                 depth_score_single_range *\n",
    "                                 range_score[:, range_idx:range_idx + 1, ...])\n",
    "            del depth_score_single_range\n",
    "            del mu_repeated\n",
    "        if self.use_mask:\n",
    "            return stereo_depth, mask_score\n",
    "        else:\n",
    "            return stereo_depth\n",
    "\n",
    "    def create_depth_sample_frustum(self, depth_sample, downsample_factor=16):\n",
    "        \"\"\"Generate frustum\"\"\"\n",
    "        # make grid in image plane\n",
    "        ogfH, ogfW = self.final_dim\n",
    "        fH, fW = ogfH // downsample_factor, ogfW // downsample_factor\n",
    "        batch_size, num_depth, _, _ = depth_sample.shape\n",
    "        x_coords = (torch.linspace(0,\n",
    "                                   ogfW - 1,\n",
    "                                   fW,\n",
    "                                   dtype=torch.float,\n",
    "                                   device=depth_sample.device).view(1, 1, 1, fW).expand(batch_size, num_depth, fH, fW))\n",
    "        y_coords = (torch.linspace(0,\n",
    "                                   ogfH - 1,\n",
    "                                   fH,\n",
    "                                   dtype=torch.float,\n",
    "                                   device=depth_sample.device).view(1, 1, fH, 1).expand(batch_size, num_depth, fH, fW))\n",
    "        paddings = torch.ones_like(depth_sample)\n",
    "\n",
    "        # D x H x W x 3\n",
    "        frustum = torch.stack((x_coords, y_coords, depth_sample, paddings), -1)\n",
    "        return frustum\n",
    "\n",
    "    def _configure_depth_net(self, depth_net_conf):\n",
    "        return DepthNet(\n",
    "            depth_net_conf['in_channels'],\n",
    "            depth_net_conf['mid_channels'],\n",
    "            self.output_channels,\n",
    "            self.depth_channels,\n",
    "            self.d_bound,\n",
    "            self.num_ranges,\n",
    "        )\n",
    "\n",
    "    def get_cam_feats(self, imgs):\n",
    "        \"\"\"Get feature maps from images.\"\"\"\n",
    "        batch_size, num_sweeps, num_cams, num_channels, imH, imW = imgs.shape\n",
    "\n",
    "        imgs = imgs.flatten().view(batch_size * num_sweeps * num_cams,\n",
    "                                   num_channels, imH, imW)\n",
    "        backbone_feats = self.img_backbone(imgs)\n",
    "        img_feats = self.img_neck(backbone_feats)[0]\n",
    "        img_feats_reshape = img_feats.reshape(batch_size, num_sweeps, num_cams,\n",
    "                                              img_feats.shape[1],\n",
    "                                              img_feats.shape[2],\n",
    "                                              img_feats.shape[3])\n",
    "        return img_feats_reshape, backbone_feats[0].detach()\n",
    "\n",
    "    def _forward_mask(\n",
    "        self,\n",
    "        sweep_index,\n",
    "        mono_depth_all_sweeps,\n",
    "        mats_dict,\n",
    "        depth_sample,\n",
    "        depth_sample_frustum,\n",
    "        sensor2sensor_mats,\n",
    "    ):\n",
    "        \"\"\"Forward function to generate mask.\n",
    "        Args:\n",
    "            sweep_index (int): Index of sweep.\n",
    "            mono_depth_all_sweeps (list[Tensor]): List of mono_depth for\n",
    "                all sweeps.\n",
    "            mats_dict (dict):\n",
    "                sensor2ego_mats (Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats (Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats (Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats (Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat (Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            depth_sample (Tensor): Depth map of all candidates.\n",
    "            depth_sample_frustum (Tensor): Pre-generated frustum.\n",
    "            sensor2sensor_mats (Tensor): Transformation matrix from reference\n",
    "                sensor to source sensor.\n",
    "        Returns:\n",
    "            Tensor: Generated mask.\n",
    "        \"\"\"\n",
    "        num_sweeps = len(mono_depth_all_sweeps)\n",
    "        mask_all_sweeps = list()\n",
    "        for idx in range(num_sweeps):\n",
    "            if idx == sweep_index:\n",
    "                continue\n",
    "            warped_mono_depth = self.homo_warping(\n",
    "                mono_depth_all_sweeps[idx],\n",
    "                mats_dict['intrin_mats'][:, sweep_index, ...],\n",
    "                mats_dict['intrin_mats'][:, idx, ...],\n",
    "                sensor2sensor_mats[idx],\n",
    "                mats_dict['ida_mats'][:, sweep_index, ...],\n",
    "                mats_dict['ida_mats'][:, idx, ...],\n",
    "                depth_sample,\n",
    "                depth_sample_frustum.type_as(mono_depth_all_sweeps[idx]),\n",
    "            )\n",
    "            mask = self.mask_net(\n",
    "                torch.cat([\n",
    "                    mono_depth_all_sweeps[sweep_index].detach(),\n",
    "                    warped_mono_depth.mean(2).detach()\n",
    "                ], 1))\n",
    "            mask_all_sweeps.append(mask)\n",
    "        return torch.stack(mask_all_sweeps).mean(0)\n",
    "\n",
    "    def _forward_single_sweep(self,\n",
    "                              sweep_index,\n",
    "                              context,\n",
    "                              mats_dict,\n",
    "                              depth_score,\n",
    "                              is_return_depth=False):\n",
    "        \"\"\"Forward function for single sweep.\n",
    "        Args:\n",
    "            sweep_index (int): Index of sweeps.\n",
    "            sweep_imgs (Tensor): Input images.\n",
    "            mats_dict (dict):\n",
    "                sensor2ego_mats(Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats(Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats(Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats(Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat(Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            is_return_depth (bool, optional): Whether to return depth.\n",
    "                Default: False.\n",
    "        Returns:\n",
    "            Tensor: BEV feature map.\n",
    "        \"\"\"\n",
    "        batch_size, num_cams = context.shape[0], context.shape[1]\n",
    "        context = context.reshape(batch_size * num_cams, *context.shape[2:])\n",
    "        depth = depth_score\n",
    "        img_feat_with_depth = depth.unsqueeze(1) * context.unsqueeze(2)\n",
    "\n",
    "        img_feat_with_depth = self._forward_voxel_net(img_feat_with_depth)\n",
    "\n",
    "        img_feat_with_depth = img_feat_with_depth.reshape(\n",
    "            batch_size,\n",
    "            num_cams,\n",
    "            img_feat_with_depth.shape[1],\n",
    "            img_feat_with_depth.shape[2],\n",
    "            img_feat_with_depth.shape[3],\n",
    "            img_feat_with_depth.shape[4],\n",
    "        )\n",
    "        geom_xyz = self.get_geometry(\n",
    "            mats_dict['sensor2ego_mats'][:, sweep_index, ...],\n",
    "            mats_dict['intrin_mats'][:, sweep_index, ...],\n",
    "            mats_dict['ida_mats'][:, sweep_index, ...],\n",
    "            mats_dict.get('bda_mat', None),\n",
    "        )\n",
    "        img_feat_with_depth = img_feat_with_depth.permute(0, 1, 3, 4, 5, 2)\n",
    "        geom_xyz = ((geom_xyz - (self.voxel_coord - self.voxel_size / 2.0)) / self.voxel_size).int()\n",
    "        \n",
    "        feature_map = voxel_pooling(geom_xyz, img_feat_with_depth.contiguous().float(), self.voxel_num.cuda())\n",
    "        \n",
    "        if is_return_depth:\n",
    "            return feature_map.contiguous(), depth\n",
    "        return feature_map.contiguous()\n",
    "\n",
    "    def forward(self,\n",
    "                sweep_imgs,\n",
    "                mats_dict,\n",
    "                timestamps=None,\n",
    "                \n",
    "                # for pose network\n",
    "                posenet_outputs=None,\n",
    "                \n",
    "                is_return_depth=False):\n",
    "        \"\"\"Forward function.\n",
    "        Args:\n",
    "            sweep_imgs(Tensor): Input images with shape of (B, num_sweeps,\n",
    "                num_cameras, 3, H, W).\n",
    "            mats_dict(dict):\n",
    "                sensor2ego_mats(Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats(Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats(Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats(Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat(Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            timestamps(Tensor): Timestamp for all images with the shape of(B,\n",
    "                num_sweeps, num_cameras).\n",
    "        Return:\n",
    "            Tensor: bev feature map.\n",
    "        \"\"\"\n",
    "        batch_size, num_sweeps, num_cams, num_channels, img_height, img_width = sweep_imgs.shape\n",
    "        context_all_sweeps = list()\n",
    "        depth_feat_all_sweeps = list()\n",
    "        img_feats_all_sweeps = list()\n",
    "        stereo_feats_all_sweeps = list()\n",
    "        mu_all_sweeps = list()\n",
    "        sigma_all_sweeps = list()\n",
    "        mono_depth_all_sweeps = list()\n",
    "        range_score_all_sweeps = list()\n",
    "        \n",
    "        \"\"\" num_sweep: 2 (연속된 두 개의 keyframes) \"\"\" \n",
    "        \n",
    "        \"\"\" 1. Mono depth map (mu, sigma 포함)과 image features 추출 \"\"\"\n",
    "        for sweep_index in range(0, num_sweeps):\n",
    "            if sweep_index > 0:\n",
    "                with torch.no_grad():\n",
    "                    img_feats, stereo_feats = self.get_cam_feats(sweep_imgs[:, sweep_index:sweep_index + 1, ...])\n",
    "                    \n",
    "                    img_feats_all_sweeps.append(img_feats.view(batch_size * num_cams, *img_feats.shape[3:]))\n",
    "                    stereo_feats_all_sweeps.append(stereo_feats)\n",
    "                    \n",
    "                    depth_feat, context, mu, sigma, range_score, mono_depth =\\\n",
    "                        self.depth_net(img_feats.view(batch_size * num_cams, *img_feats.shape[3:]), mats_dict)\n",
    "                    \n",
    "                    context_all_sweeps.append(\n",
    "                        self.context_downsample_net(\n",
    "                            context.reshape(batch_size * num_cams, *context.shape[1:])))\n",
    "                    depth_feat_all_sweeps.append(depth_feat)\n",
    "            else:\n",
    "                img_feats, stereo_feats = self.get_cam_feats(sweep_imgs[:, sweep_index:sweep_index + 1, ...])\n",
    "                \n",
    "                img_feats_all_sweeps.append(img_feats.view(batch_size * num_cams, *img_feats.shape[3:]))\n",
    "                stereo_feats_all_sweeps.append(stereo_feats)\n",
    "                \n",
    "                depth_feat, context, mu, sigma, range_score, mono_depth =\\\n",
    "                    self.depth_net(img_feats.view(batch_size * num_cams, *img_feats.shape[3:]), mats_dict)\n",
    "                \n",
    "                context_all_sweeps.append(\n",
    "                    self.context_downsample_net(\n",
    "                        context.reshape(batch_size * num_cams, *context.shape[1:])))\n",
    "                depth_feat_all_sweeps.append(depth_feat)\n",
    "                \n",
    "            mu_all_sweeps.append(mu)\n",
    "            sigma_all_sweeps.append(sigma)\n",
    "            mono_depth_all_sweeps.append(mono_depth)\n",
    "            range_score_all_sweeps.append(range_score)\n",
    "        \n",
    "        \"\"\" 2. Stereo depth 추출 (_forward_stereo() function) \"\"\"\n",
    "        depth_score_all_sweeps = list()\n",
    "        for ref_idx in range(num_sweeps):\n",
    "            sensor2sensor_mats = list()\n",
    "            for src_idx in range(num_sweeps):\n",
    "                \"\"\" Original Code \"\"\"\n",
    "                # ref2keysensor_mats = mats_dict['sensor2sensor_mats'][:, ref_idx, ...].inverse()\n",
    "                # key2srcsensor_mats = mats_dict['sensor2sensor_mats'][:, src_idx, ...]\n",
    "                # ref2srcsensor_mats = key2srcsensor_mats @ ref2keysensor_mats\n",
    "                # sensor2sensor_mats.append(ref2srcsensor_mats)\n",
    "                \"\"\" Modification for PoseNet \"\"\"\n",
    "                if posenet_outputs is not None:\n",
    "                    if ref_idx < src_idx:\n",
    "                        ref2srcsensor_mats = posenet_outputs[0]\n",
    "                    else:\n",
    "                        ref2srcsensor_mats = posenet_outputs[1]\n",
    "                else:\n",
    "                    ref2keysensor_mats = mats_dict['sensor2sensor_mats'][:, ref_idx, ...].inverse()\n",
    "                    key2srcsensor_mats = mats_dict['sensor2sensor_mats'][:, src_idx, ...]\n",
    "                    ref2srcsensor_mats = key2srcsensor_mats @ ref2keysensor_mats\n",
    "                sensor2sensor_mats.append(ref2srcsensor_mats)\n",
    "                # if ref_idx != src_idx:\n",
    "                #     print(\"ref_idx: \", ref_idx, \" src_idx: \", src_idx)\n",
    "                #     print(\"ref2srcsensor_mats: \\n\", ref2srcsensor_mats)\n",
    "                #     print(\"shape: \", ref2srcsensor_mats.shape)\n",
    "            if ref_idx == 0:\n",
    "                # last iteration on stage 1 does not have propagation (photometric consistency filtering)\n",
    "                if self.use_mask:\n",
    "                    stereo_depth, mask = self._forward_stereo(\n",
    "                        ref_idx,\n",
    "                        stereo_feats_all_sweeps,\n",
    "                        mono_depth_all_sweeps,\n",
    "                        mats_dict,\n",
    "                        sensor2sensor_mats,\n",
    "                        mu_all_sweeps,\n",
    "                        sigma_all_sweeps,\n",
    "                        range_score_all_sweeps,\n",
    "                        depth_feat_all_sweeps,\n",
    "                    )\n",
    "                else:\n",
    "                    stereo_depth = self._forward_stereo(\n",
    "                        ref_idx,\n",
    "                        stereo_feats_all_sweeps,\n",
    "                        mono_depth_all_sweeps,\n",
    "                        mats_dict,\n",
    "                        sensor2sensor_mats,\n",
    "                        mu_all_sweeps,\n",
    "                        sigma_all_sweeps,\n",
    "                        range_score_all_sweeps,\n",
    "                        depth_feat_all_sweeps,\n",
    "                    )\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    # last iteration on stage 1 does not have propagation (photometric consistency filtering)\n",
    "                    if self.use_mask:\n",
    "                        stereo_depth, mask = self._forward_stereo(\n",
    "                            ref_idx,\n",
    "                            stereo_feats_all_sweeps,\n",
    "                            mono_depth_all_sweeps,\n",
    "                            mats_dict,\n",
    "                            sensor2sensor_mats,\n",
    "                            mu_all_sweeps,\n",
    "                            sigma_all_sweeps,\n",
    "                            range_score_all_sweeps,\n",
    "                            depth_feat_all_sweeps,\n",
    "                        )\n",
    "                    else:\n",
    "                        stereo_depth = self._forward_stereo(\n",
    "                            ref_idx,\n",
    "                            stereo_feats_all_sweeps,\n",
    "                            mono_depth_all_sweeps,\n",
    "                            mats_dict,\n",
    "                            sensor2sensor_mats,\n",
    "                            mu_all_sweeps,\n",
    "                            sigma_all_sweeps,\n",
    "                            range_score_all_sweeps,\n",
    "                            depth_feat_all_sweeps,\n",
    "                        )\n",
    "            if self.use_mask:\n",
    "                depth_score = (\n",
    "                    mono_depth_all_sweeps[ref_idx] +\n",
    "                    self.depth_downsample_net(stereo_depth) * mask).softmax(1)\n",
    "            else:\n",
    "                depth_score = (\n",
    "                    mono_depth_all_sweeps[ref_idx] +\n",
    "                    self.depth_downsample_net(stereo_depth)).softmax(1)\n",
    "                \n",
    "            depth_score_all_sweeps.append(depth_score)\n",
    "        \n",
    "        \"\"\" 3. Stereo depth map과 context image feature로 voxel pooling하여 BEV features를 얻음 \"\"\" \n",
    "        \"\"\"    3-1. 현재 시점의 keyframe에 대해서 voxel pooling \"\"\" \n",
    "        key_frame_res = self._forward_single_sweep(\n",
    "            0,\n",
    "            context_all_sweeps[0].reshape(batch_size, num_cams, *context_all_sweeps[0].shape[1:]),\n",
    "            mats_dict,\n",
    "            depth_score_all_sweeps[0],\n",
    "            is_return_depth=is_return_depth,\n",
    "        )\n",
    "        if num_sweeps == 1:\n",
    "            return key_frame_res\n",
    "        \n",
    "        key_frame_feature = key_frame_res[0] if is_return_depth else key_frame_res\n",
    "        ret_feature_list = [key_frame_feature]\n",
    "        \n",
    "        \"\"\"    3-2. 다음 시점의 keyframe에 대해서 voxel pooling \"\"\"\n",
    "        for sweep_index in range(1, num_sweeps): # num_sweep: 2 -> one iteration (i==1)\n",
    "            with torch.no_grad():\n",
    "                feature_map = self._forward_single_sweep(\n",
    "                    sweep_index, # 1\n",
    "                    context_all_sweeps[sweep_index].reshape(batch_size, num_cams, *context_all_sweeps[sweep_index].shape[1:]),\n",
    "                    mats_dict,\n",
    "                    depth_score_all_sweeps[sweep_index],\n",
    "                    is_return_depth=False,\n",
    "                )\n",
    "                ret_feature_list.append(feature_map)\n",
    "\n",
    "        if is_return_depth:\n",
    "            return torch.cat(ret_feature_list, 1), depth_score_all_sweeps[0]\n",
    "        else:\n",
    "            return torch.cat(ret_feature_list, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824fd719-6505-40fd-aa6a-b2ebd0680a08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# BEVStereo Head (inherit from BEVDepth)\n",
    "from layers/heads/bev_depth_head.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edc55b94-0d3d-43ea-b896-2a45d1f90c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bev_backbone_conf = dict(\n",
    "    type='ResNet',\n",
    "    in_channels=80,\n",
    "    depth=18,\n",
    "    num_stages=3,\n",
    "    strides=(1, 2, 2),\n",
    "    dilations=(1, 1, 1),\n",
    "    out_indices=[0, 1, 2],\n",
    "    norm_eval=False,\n",
    "    base_channels=160,\n",
    ")\n",
    "\n",
    "bev_neck_conf = dict(type='SECONDFPN',\n",
    "                     in_channels=[160, 320, 640],\n",
    "                     upsample_strides=[2, 4, 8],\n",
    "                     out_channels=[64, 64, 128])\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def size_aware_circle_nms(dets, thresh_scale, post_max_size=83):\n",
    "    \"\"\"Circular NMS.\n",
    "    An object is only counted as positive if no other center\n",
    "    with a higher confidence exists within a radius r using a\n",
    "    bird-eye view distance metric.\n",
    "    Args:\n",
    "        dets (torch.Tensor): Detection results with the shape of [N, 3].\n",
    "        thresh (float): Value of threshold.\n",
    "        post_max_size (int): Max number of prediction to be kept. Defaults\n",
    "            to 83\n",
    "    Returns:\n",
    "        torch.Tensor: Indexes of the detections to be kept.\n",
    "    \"\"\"\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    dx1 = dets[:, 2]\n",
    "    dy1 = dets[:, 3]\n",
    "    yaws = dets[:, 4]\n",
    "    scores = dets[:, -1]\n",
    "    order = scores.argsort()[::-1].astype(np.int32)  # highest->lowest\n",
    "    ndets = dets.shape[0]\n",
    "    suppressed = np.zeros((ndets), dtype=np.int32)\n",
    "    keep = []\n",
    "    for _i in range(ndets):\n",
    "        i = order[_i]  # start with highest score box\n",
    "        if suppressed[\n",
    "                i] == 1:  # if any box have enough iou with this, remove it\n",
    "            continue\n",
    "        keep.append(i)\n",
    "        for _j in range(_i + 1, ndets):\n",
    "            j = order[_j]\n",
    "            if suppressed[j] == 1:\n",
    "                continue\n",
    "            # calculate center distance between i and j box\n",
    "            dist_x = abs(x1[i] - x1[j])\n",
    "            dist_y = abs(y1[i] - y1[j])\n",
    "            dist_x_th = (abs(dx1[i] * np.cos(yaws[i])) +\n",
    "                         abs(dx1[j] * np.cos(yaws[j])) +\n",
    "                         abs(dy1[i] * np.sin(yaws[i])) +\n",
    "                         abs(dy1[j] * np.sin(yaws[j])))\n",
    "            dist_y_th = (abs(dx1[i] * np.sin(yaws[i])) +\n",
    "                         abs(dx1[j] * np.sin(yaws[j])) +\n",
    "                         abs(dy1[i] * np.cos(yaws[i])) +\n",
    "                         abs(dy1[j] * np.cos(yaws[j])))\n",
    "            # ovr = inter / areas[j]\n",
    "            if dist_x <= dist_x_th * thresh_scale / 2 and \\\n",
    "               dist_y <= dist_y_th * thresh_scale / 2:\n",
    "                suppressed[j] = 1\n",
    "    return keep[:post_max_size]\n",
    "\n",
    "class BEVDepthHead(CenterHead):\n",
    "    \"\"\"Head for BevDepth.\n",
    "\n",
    "    Args:\n",
    "        in_channels(int): Number of channels after bev_neck.\n",
    "        tasks(dict): Tasks for head.\n",
    "        bbox_coder(dict): Config of bbox coder.\n",
    "        common_heads(dict): Config of head for each task.\n",
    "        loss_cls(dict): Config of classification loss.\n",
    "        loss_bbox(dict): Config of regression loss.\n",
    "        gaussian_overlap(float): Gaussian overlap used for `get_targets`.\n",
    "        min_radius(int): Min radius used for `get_targets`.\n",
    "        train_cfg(dict): Config used in the training process.\n",
    "        test_cfg(dict): Config used in the test process.\n",
    "        bev_backbone_conf(dict): Cnfig of bev_backbone.\n",
    "        bev_neck_conf(dict): Cnfig of bev_neck.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=256,\n",
    "        tasks=None,\n",
    "        bbox_coder=None,\n",
    "        common_heads=dict(),\n",
    "        loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),\n",
    "        loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),\n",
    "        gaussian_overlap=0.1,\n",
    "        min_radius=2,\n",
    "        train_cfg=None,\n",
    "        test_cfg=None,\n",
    "        bev_backbone_conf=bev_backbone_conf,\n",
    "        bev_neck_conf=bev_neck_conf,\n",
    "        separate_head=dict(type='SeparateHead',\n",
    "                           init_bias=-2.19,\n",
    "                           final_kernel=3),\n",
    "    ):\n",
    "        super(BEVDepthHead, self).__init__(\n",
    "            in_channels=in_channels,\n",
    "            tasks=tasks,\n",
    "            bbox_coder=bbox_coder,\n",
    "            common_heads=common_heads,\n",
    "            loss_cls=loss_cls,\n",
    "            loss_bbox=loss_bbox,\n",
    "            separate_head=separate_head,\n",
    "        )\n",
    "        self.trunk = build_backbone(bev_backbone_conf)\n",
    "        self.trunk.init_weights()\n",
    "        self.neck = build_neck(bev_neck_conf)\n",
    "        self.neck.init_weights()\n",
    "        del self.trunk.maxpool\n",
    "        self.gaussian_overlap = gaussian_overlap\n",
    "        self.min_radius = min_radius\n",
    "        self.train_cfg = train_cfg\n",
    "        self.test_cfg = test_cfg\n",
    "\n",
    "    @autocast(False)\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            feats (list[torch.Tensor]): Multi-level features, e.g.,\n",
    "                features produced by FPN.\n",
    "\n",
    "        Returns:\n",
    "            tuple(list[dict]): Output results for tasks.\n",
    "        \"\"\"\n",
    "        # FPN\n",
    "        trunk_outs = [x]\n",
    "        if self.trunk.deep_stem:\n",
    "            x = self.trunk.stem(x)\n",
    "        else:\n",
    "            x = self.trunk.conv1(x)\n",
    "            x = self.trunk.norm1(x)\n",
    "            x = self.trunk.relu(x)\n",
    "        for i, layer_name in enumerate(self.trunk.res_layers):\n",
    "            res_layer = getattr(self.trunk, layer_name)\n",
    "            x = res_layer(x)\n",
    "            if i in self.trunk.out_indices:\n",
    "                trunk_outs.append(x)\n",
    "        fpn_output = self.neck(trunk_outs)\n",
    "        ret_values = super().forward(fpn_output)\n",
    "        return ret_values\n",
    "\n",
    "    def get_targets_single(self, gt_bboxes_3d, gt_labels_3d):\n",
    "        \"\"\"Generate training targets for a single sample.\n",
    "\n",
    "        Args:\n",
    "            gt_bboxes_3d (:obj:`LiDARInstance3DBoxes`): Ground truth gt boxes.\n",
    "            gt_labels_3d (torch.Tensor): Labels of boxes.\n",
    "\n",
    "        Returns:\n",
    "            tuple[list[torch.Tensor]]: Tuple of target including \\\n",
    "                the following results in order.\n",
    "\n",
    "                - list[torch.Tensor]: Heatmap scores.\n",
    "                - list[torch.Tensor]: Ground truth boxes.\n",
    "                - list[torch.Tensor]: Indexes indicating the position \\\n",
    "                    of the valid boxes.\n",
    "                - list[torch.Tensor]: Masks indicating which boxes \\\n",
    "                    are valid.\n",
    "        \"\"\"\n",
    "        max_objs = self.train_cfg['max_objs'] * self.train_cfg['dense_reg']\n",
    "        grid_size = torch.tensor(self.train_cfg['grid_size'])\n",
    "        pc_range = torch.tensor(self.train_cfg['point_cloud_range'])\n",
    "        voxel_size = torch.tensor(self.train_cfg['voxel_size'])\n",
    "\n",
    "        feature_map_size = grid_size[:2] // self.train_cfg['out_size_factor']\n",
    "\n",
    "        # reorganize the gt_dict by tasks\n",
    "        task_masks = []\n",
    "        flag = 0\n",
    "        for class_name in self.class_names:\n",
    "            task_masks.append([\n",
    "                torch.where(gt_labels_3d == class_name.index(i) + flag)\n",
    "                for i in class_name\n",
    "            ])\n",
    "            flag += len(class_name)\n",
    "\n",
    "        task_boxes = []\n",
    "        task_classes = []\n",
    "        flag2 = 0\n",
    "        for idx, mask in enumerate(task_masks):\n",
    "            task_box = []\n",
    "            task_class = []\n",
    "            for m in mask:\n",
    "                task_box.append(gt_bboxes_3d[m])\n",
    "                # 0 is background for each task, so we need to add 1 here.\n",
    "                task_class.append(gt_labels_3d[m] + 1 - flag2)\n",
    "            task_boxes.append(\n",
    "                torch.cat(task_box, axis=0).to(gt_bboxes_3d.device))\n",
    "            task_classes.append(\n",
    "                torch.cat(task_class).long().to(gt_bboxes_3d.device))\n",
    "            flag2 += len(mask)\n",
    "        draw_gaussian = draw_heatmap_gaussian\n",
    "        heatmaps, anno_boxes, inds, masks = [], [], [], []\n",
    "\n",
    "        for idx, task_head in enumerate(self.task_heads):\n",
    "            heatmap = gt_bboxes_3d.new_zeros(\n",
    "                (len(self.class_names[idx]), feature_map_size[1],\n",
    "                 feature_map_size[0]),\n",
    "                device='cuda')\n",
    "\n",
    "            anno_box = gt_bboxes_3d.new_zeros(\n",
    "                (max_objs, len(self.train_cfg['code_weights'])),\n",
    "                dtype=torch.float32,\n",
    "                device='cuda')\n",
    "\n",
    "            ind = gt_labels_3d.new_zeros((max_objs),\n",
    "                                         dtype=torch.int64,\n",
    "                                         device='cuda')\n",
    "            mask = gt_bboxes_3d.new_zeros((max_objs),\n",
    "                                          dtype=torch.uint8,\n",
    "                                          device='cuda')\n",
    "\n",
    "            num_objs = min(task_boxes[idx].shape[0], max_objs)\n",
    "\n",
    "            for k in range(num_objs):\n",
    "                cls_id = task_classes[idx][k] - 1\n",
    "\n",
    "                width = task_boxes[idx][k][3]\n",
    "                length = task_boxes[idx][k][4]\n",
    "                width = width / voxel_size[0] / self.train_cfg[\n",
    "                    'out_size_factor']\n",
    "                length = length / voxel_size[1] / self.train_cfg[\n",
    "                    'out_size_factor']\n",
    "\n",
    "                if width > 0 and length > 0:\n",
    "                    radius = gaussian_radius(\n",
    "                        (length, width),\n",
    "                        min_overlap=self.train_cfg['gaussian_overlap'])\n",
    "                    radius = max(self.train_cfg['min_radius'], int(radius))\n",
    "\n",
    "                    # be really careful for the coordinate system of\n",
    "                    # your box annotation.\n",
    "                    x, y, z = task_boxes[idx][k][0], task_boxes[idx][k][\n",
    "                        1], task_boxes[idx][k][2]\n",
    "\n",
    "                    coor_x = (\n",
    "                        x - pc_range[0]\n",
    "                    ) / voxel_size[0] / self.train_cfg['out_size_factor']\n",
    "                    coor_y = (\n",
    "                        y - pc_range[1]\n",
    "                    ) / voxel_size[1] / self.train_cfg['out_size_factor']\n",
    "\n",
    "                    center = torch.tensor([coor_x, coor_y],\n",
    "                                          dtype=torch.float32,\n",
    "                                          device='cuda')\n",
    "                    center_int = center.to(torch.int32)\n",
    "\n",
    "                    # throw out not in range objects to avoid out of array\n",
    "                    # area when creating the heatmap\n",
    "                    if not (0 <= center_int[0] < feature_map_size[0]\n",
    "                            and 0 <= center_int[1] < feature_map_size[1]):\n",
    "                        continue\n",
    "\n",
    "                    draw_gaussian(heatmap[cls_id], center_int, radius)\n",
    "\n",
    "                    new_idx = k\n",
    "                    x, y = center_int[0], center_int[1]\n",
    "\n",
    "                    assert y * feature_map_size[0] + x < feature_map_size[\n",
    "                        0] * feature_map_size[1]\n",
    "\n",
    "                    ind[new_idx] = y * feature_map_size[0] + x\n",
    "                    mask[new_idx] = 1\n",
    "                    # TODO: support other outdoor dataset\n",
    "                    if len(task_boxes[idx][k]) > 7:\n",
    "                        vx, vy = task_boxes[idx][k][7:]\n",
    "                    rot = task_boxes[idx][k][6]\n",
    "                    box_dim = task_boxes[idx][k][3:6]\n",
    "                    if self.norm_bbox:\n",
    "                        box_dim = box_dim.log()\n",
    "                    if len(task_boxes[idx][k]) > 7:\n",
    "                        anno_box[new_idx] = torch.cat([\n",
    "                            center - torch.tensor([x, y], device='cuda'),\n",
    "                            z.unsqueeze(0),\n",
    "                            box_dim,\n",
    "                            torch.sin(rot).unsqueeze(0),\n",
    "                            torch.cos(rot).unsqueeze(0),\n",
    "                            vx.unsqueeze(0),\n",
    "                            vy.unsqueeze(0),\n",
    "                        ])\n",
    "                    else:\n",
    "                        anno_box[new_idx] = torch.cat([\n",
    "                            center - torch.tensor([x, y], device='cuda'),\n",
    "                            z.unsqueeze(0), box_dim,\n",
    "                            torch.sin(rot).unsqueeze(0),\n",
    "                            torch.cos(rot).unsqueeze(0)\n",
    "                        ])\n",
    "\n",
    "            heatmaps.append(heatmap)\n",
    "            anno_boxes.append(anno_box)\n",
    "            masks.append(mask)\n",
    "            inds.append(ind)\n",
    "        return heatmaps, anno_boxes, inds, masks\n",
    "\n",
    "    def loss(self, targets, preds_dicts, **kwargs):\n",
    "        \"\"\"Loss function for BEVDepthHead.\n",
    "\n",
    "        Args:\n",
    "            gt_bboxes_3d (list[:obj:`LiDARInstance3DBoxes`]): Ground\n",
    "                truth gt boxes.\n",
    "            gt_labels_3d (list[torch.Tensor]): Labels of boxes.\n",
    "            preds_dicts (dict): Output of forward function.\n",
    "\n",
    "        Returns:\n",
    "            dict[str:torch.Tensor]: Loss of heatmap and bbox of each task.\n",
    "        \"\"\"\n",
    "        heatmaps, anno_boxes, inds, masks = targets\n",
    "        return_loss = 0\n",
    "        for task_id, preds_dict in enumerate(preds_dicts):\n",
    "            # heatmap focal loss\n",
    "            preds_dict[0]['heatmap'] = clip_sigmoid(preds_dict[0]['heatmap'])\n",
    "            num_pos = heatmaps[task_id].eq(1).float().sum().item()\n",
    "            cls_avg_factor = torch.clamp(reduce_mean(\n",
    "                heatmaps[task_id].new_tensor(num_pos)),\n",
    "                                         min=1).item()\n",
    "            loss_heatmap = self.loss_cls(preds_dict[0]['heatmap'],\n",
    "                                         heatmaps[task_id],\n",
    "                                         avg_factor=cls_avg_factor)\n",
    "            target_box = anno_boxes[task_id]\n",
    "            # reconstruct the anno_box from multiple reg heads\n",
    "            if 'vel' in preds_dict[0].keys():\n",
    "                preds_dict[0]['anno_box'] = torch.cat(\n",
    "                    (preds_dict[0]['reg'], preds_dict[0]['height'],\n",
    "                     preds_dict[0]['dim'], preds_dict[0]['rot'],\n",
    "                     preds_dict[0]['vel']),\n",
    "                    dim=1,\n",
    "                )\n",
    "            else:\n",
    "                preds_dict[0]['anno_box'] = torch.cat(\n",
    "                    (preds_dict[0]['reg'], preds_dict[0]['height'],\n",
    "                     preds_dict[0]['dim'], preds_dict[0]['rot']),\n",
    "                    dim=1,\n",
    "                )\n",
    "            # Regression loss for dimension, offset, height, rotation\n",
    "            num = masks[task_id].float().sum()\n",
    "            ind = inds[task_id]\n",
    "            pred = preds_dict[0]['anno_box'].permute(0, 2, 3, 1).contiguous()\n",
    "            pred = pred.view(pred.size(0), -1, pred.size(3))\n",
    "            pred = self._gather_feat(pred, ind)\n",
    "            mask = masks[task_id].unsqueeze(2).expand_as(target_box).float()\n",
    "            num = torch.clamp(reduce_mean(target_box.new_tensor(num)),\n",
    "                              min=1e-4).item()\n",
    "            isnotnan = (~torch.isnan(target_box)).float()\n",
    "            mask *= isnotnan\n",
    "            code_weights = self.train_cfg['code_weights']\n",
    "            bbox_weights = mask * mask.new_tensor(code_weights)\n",
    "            loss_bbox = self.loss_bbox(pred,\n",
    "                                       target_box,\n",
    "                                       bbox_weights,\n",
    "                                       avg_factor=num)\n",
    "            return_loss += loss_bbox\n",
    "            return_loss += loss_heatmap\n",
    "        return return_loss\n",
    "\n",
    "    def get_bboxes(self, preds_dicts, img_metas, img=None, rescale=False):\n",
    "        \"\"\"Generate bboxes from bbox head predictions.\n",
    "\n",
    "        Args:\n",
    "            preds_dicts (tuple[list[dict]]): Prediction results.\n",
    "            img_metas (list[dict]): Point cloud and image's meta info.\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: Decoded bbox, scores and labels after nms.\n",
    "        \"\"\"\n",
    "        rets = []\n",
    "        for task_id, preds_dict in enumerate(preds_dicts):\n",
    "            num_class_with_bg = self.num_classes[task_id]\n",
    "            batch_size = preds_dict[0]['heatmap'].shape[0]\n",
    "            batch_heatmap = preds_dict[0]['heatmap'].sigmoid()\n",
    "\n",
    "            batch_reg = preds_dict[0]['reg']\n",
    "            batch_hei = preds_dict[0]['height']\n",
    "\n",
    "            if self.norm_bbox:\n",
    "                batch_dim = torch.exp(preds_dict[0]['dim'])\n",
    "            else:\n",
    "                batch_dim = preds_dict[0]['dim']\n",
    "\n",
    "            batch_rots = preds_dict[0]['rot'][:, 0].unsqueeze(1)\n",
    "            batch_rotc = preds_dict[0]['rot'][:, 1].unsqueeze(1)\n",
    "\n",
    "            if 'vel' in preds_dict[0]:\n",
    "                batch_vel = preds_dict[0]['vel']\n",
    "            else:\n",
    "                batch_vel = None\n",
    "            temp = self.bbox_coder.decode(batch_heatmap,\n",
    "                                          batch_rots,\n",
    "                                          batch_rotc,\n",
    "                                          batch_hei,\n",
    "                                          batch_dim,\n",
    "                                          batch_vel,\n",
    "                                          reg=batch_reg,\n",
    "                                          task_id=task_id)\n",
    "            assert self.test_cfg['nms_type'] in [\n",
    "                'size_aware_circle', 'circle', 'rotate'\n",
    "            ]\n",
    "            batch_reg_preds = [box['bboxes'] for box in temp]\n",
    "            batch_cls_preds = [box['scores'] for box in temp]\n",
    "            batch_cls_labels = [box['labels'] for box in temp]\n",
    "            if self.test_cfg['nms_type'] == 'circle':\n",
    "                ret_task = []\n",
    "                for i in range(batch_size):\n",
    "                    boxes3d = temp[i]['bboxes']\n",
    "                    scores = temp[i]['scores']\n",
    "                    labels = temp[i]['labels']\n",
    "                    centers = boxes3d[:, [0, 1]]\n",
    "                    boxes = torch.cat([centers, scores.view(-1, 1)], dim=1)\n",
    "                    keep = torch.tensor(circle_nms(\n",
    "                        boxes.detach().cpu().numpy(),\n",
    "                        self.test_cfg['min_radius'][task_id],\n",
    "                        post_max_size=self.test_cfg['post_max_size']),\n",
    "                                        dtype=torch.long,\n",
    "                                        device=boxes.device)\n",
    "\n",
    "                    boxes3d = boxes3d[keep]\n",
    "                    scores = scores[keep]\n",
    "                    labels = labels[keep]\n",
    "                    ret = dict(bboxes=boxes3d, scores=scores, labels=labels)\n",
    "                    ret_task.append(ret)\n",
    "                rets.append(ret_task)\n",
    "            elif self.test_cfg['nms_type'] == 'size_aware_circle':\n",
    "                ret_task = []\n",
    "                for i in range(batch_size):\n",
    "                    boxes3d = temp[i]['bboxes']\n",
    "                    scores = temp[i]['scores']\n",
    "                    labels = temp[i]['labels']\n",
    "                    boxes_2d = boxes3d[:, [0, 1, 3, 4, 6]]\n",
    "                    boxes = torch.cat([boxes_2d, scores.view(-1, 1)], dim=1)\n",
    "                    keep = torch.tensor(\n",
    "                        size_aware_circle_nms(\n",
    "                            boxes.detach().cpu().numpy(),\n",
    "                            self.test_cfg['thresh_scale'][task_id],\n",
    "                            post_max_size=self.test_cfg['post_max_size'],\n",
    "                        ),\n",
    "                        dtype=torch.long,\n",
    "                        device=boxes.device,\n",
    "                    )\n",
    "\n",
    "                    boxes3d = boxes3d[keep]\n",
    "                    scores = scores[keep]\n",
    "                    labels = labels[keep]\n",
    "                    ret = dict(bboxes=boxes3d, scores=scores, labels=labels)\n",
    "                    ret_task.append(ret)\n",
    "                rets.append(ret_task)\n",
    "            else:\n",
    "                rets.append(\n",
    "                    self.get_task_detections(num_class_with_bg,\n",
    "                                             batch_cls_preds, batch_reg_preds,\n",
    "                                             batch_cls_labels, img_metas))\n",
    "\n",
    "        # Merge branches results\n",
    "        num_samples = len(rets[0])\n",
    "\n",
    "        ret_list = []\n",
    "        for i in range(num_samples):\n",
    "            for k in rets[0][i].keys():\n",
    "                if k == 'bboxes':\n",
    "                    bboxes = torch.cat([ret[i][k] for ret in rets])\n",
    "                elif k == 'scores':\n",
    "                    scores = torch.cat([ret[i][k] for ret in rets])\n",
    "                elif k == 'labels':\n",
    "                    flag = 0\n",
    "                    for j, num_class in enumerate(self.num_classes):\n",
    "                        rets[j][i][k] += flag\n",
    "                        flag += num_class\n",
    "                    labels = torch.cat([ret[i][k].int() for ret in rets])\n",
    "            ret_list.append([bboxes, scores, labels])\n",
    "        return ret_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fc5bb-dc2d-4656-b55c-5da06497f711",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# BEVStereo Model\n",
    "- from models/bev_stereo.py\n",
    "- Motification by Jeho\n",
    "- BEVDepth에서 상속 받지 말고 BEVStereo class에서 model functions 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d78655e4-a7a8-42aa-9484-4fa129f80a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BEVStereo(nn.Module):\n",
    "    \"\"\"Source code of `BEVStereo`, `https://arxiv.org/abs/2209.10248`.\n",
    "\n",
    "    Args:\n",
    "        backbone_conf (dict): Config of backbone.\n",
    "        head_conf (dict): Config of head.\n",
    "        is_train_depth (bool): Whether to return depth.\n",
    "            Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Reduce grid_conf and data_aug_conf\n",
    "    def __init__(self, backbone_conf, head_conf, is_train_depth=False):\n",
    "        super(BEVStereo, self).__init__()\n",
    "        self.backbone = BEVStereoLSSFPN(**backbone_conf)\n",
    "        self.head = BEVDepthHead(**head_conf)\n",
    "        self.is_train_depth = is_train_depth\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        mats_dict,\n",
    "        timestamps=None,\n",
    "        \n",
    "        # for pose network\n",
    "        posenet_outputs=None\n",
    "    ):\n",
    "        \"\"\"Forward function for BEVDepth\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input ferature map.\n",
    "            mats_dict(dict):\n",
    "                sensor2ego_mats(Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats(Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats(Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats(Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat(Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            timestamps (long): Timestamp.\n",
    "                Default: None.\n",
    "\n",
    "        Returns:\n",
    "            tuple(list[dict]): Output results for tasks.\n",
    "        \"\"\"\n",
    "        if self.is_train_depth and self.training:\n",
    "            x, depth_pred = self.backbone(x,\n",
    "                                          mats_dict,\n",
    "                                          timestamps,\n",
    "                                          is_return_depth=True)\n",
    "            preds = self.head(x)\n",
    "            return preds, depth_pred\n",
    "        else:\n",
    "            x = self.backbone(x, mats_dict, timestamps, \n",
    "                              # for pose network\n",
    "                              posenet_outputs=posenet_outputs)\n",
    "            preds = self.head(x)\n",
    "            return preds\n",
    "\n",
    "    def get_targets(self, gt_boxes, gt_labels):\n",
    "        \"\"\"Generate training targets for a single sample.\n",
    "\n",
    "        Args:\n",
    "            gt_bboxes_3d (:obj:`LiDARInstance3DBoxes`): Ground truth gt boxes.\n",
    "            gt_labels_3d (torch.Tensor): Labels of boxes.\n",
    "\n",
    "        Returns:\n",
    "            tuple[list[torch.Tensor]]: Tuple of target including \\\n",
    "                the following results in order.\n",
    "\n",
    "                - list[torch.Tensor]: Heatmap scores.\n",
    "                - list[torch.Tensor]: Ground truth boxes.\n",
    "                - list[torch.Tensor]: Indexes indicating the position \\\n",
    "                    of the valid boxes.\n",
    "                - list[torch.Tensor]: Masks indicating which boxes \\\n",
    "                    are valid.\n",
    "        \"\"\"\n",
    "        return self.head.get_targets(gt_boxes, gt_labels)\n",
    "\n",
    "    def loss(self, targets, preds_dicts):\n",
    "        \"\"\"Loss function for BEVDepth.\n",
    "\n",
    "        Args:\n",
    "            gt_bboxes_3d (list[:obj:`LiDARInstance3DBoxes`]): Ground\n",
    "                truth gt boxes.\n",
    "            gt_labels_3d (list[torch.Tensor]): Labels of boxes.\n",
    "            preds_dicts (dict): Output of forward function.\n",
    "\n",
    "        Returns:\n",
    "            dict[str:torch.Tensor]: Loss of heatmap and bbox of each task.\n",
    "        \"\"\"\n",
    "        return self.head.loss(targets, preds_dicts)\n",
    "\n",
    "    def get_bboxes(self, preds_dicts, img_metas=None, img=None, rescale=False):\n",
    "        \"\"\"Generate bboxes from bbox head predictions.\n",
    "\n",
    "        Args:\n",
    "            preds_dicts (tuple[list[dict]]): Prediction results.\n",
    "            img_metas (list[dict]): Point cloud and image's meta info.\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: Decoded bbox, scores and labels after nms.\n",
    "        \"\"\"\n",
    "        return self.head.get_bboxes(preds_dicts, img_metas, img, rescale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524aaaf-c8a2-41d4-a98a-2290a24a6aae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# BEVStereo Detector (based on BEVDepth) (PyTorch Lightning Module)\n",
    "- from exps/nuscenes/base_exp.py, exps/nuscenes/mv/bevstereo.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7318ffd5-5ab0-46dd-9fc8-26fbfab81360",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a9939e-b3f6-44cb-9dfa-8cb550f3e38b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "H = 900\n",
    "W = 1600\n",
    "final_dim = (256, 704)\n",
    "img_conf = dict(img_mean=[123.675, 116.28, 103.53],\n",
    "                img_std=[58.395, 57.12, 57.375],\n",
    "                to_rgb=True)\n",
    "\n",
    "backbone_conf = {\n",
    "    'x_bound': [-51.2, 51.2, 0.8],\n",
    "    'y_bound': [-51.2, 51.2, 0.8],\n",
    "    'z_bound': [-5, 3, 8],\n",
    "    'd_bound': [2.0, 58.0, 0.5],\n",
    "    'final_dim':\n",
    "    final_dim,\n",
    "    'output_channels': 80,\n",
    "    'downsample_factor': 16,\n",
    "    'img_backbone_conf':\n",
    "    dict(\n",
    "        type='ResNet',\n",
    "        depth=50,\n",
    "        frozen_stages=0,\n",
    "        out_indices=[0, 1, 2, 3],\n",
    "        norm_eval=False,\n",
    "        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),\n",
    "    ),\n",
    "    'img_neck_conf':\n",
    "    dict(\n",
    "        type='SECONDFPN',\n",
    "        in_channels=[256, 512, 1024, 2048],\n",
    "        upsample_strides=[0.25, 0.5, 1, 2],\n",
    "        out_channels=[128, 128, 128, 128],\n",
    "    ),\n",
    "    'depth_net_conf':\n",
    "    dict(in_channels=512, mid_channels=512)\n",
    "}\n",
    "\n",
    "ida_aug_conf = {\n",
    "    'resize_lim': (0.386, 0.55),\n",
    "    'final_dim': final_dim,\n",
    "    'rot_lim': (-5.4, 5.4),\n",
    "    'H': H,\n",
    "    'W': W,\n",
    "    'rand_flip': True,\n",
    "    'bot_pct_lim': (0.0, 0.0),\n",
    "    'cams': [\n",
    "        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT',\n",
    "        'CAM_BACK', 'CAM_BACK_RIGHT'\n",
    "    ],\n",
    "    'Ncams': 6,\n",
    "}\n",
    "\n",
    "bda_aug_conf = {\n",
    "    'rot_lim': (-22.5, 22.5),\n",
    "    'scale_lim': (0.95, 1.05),\n",
    "    'flip_dx_ratio': 0.5,\n",
    "    'flip_dy_ratio': 0.5\n",
    "}\n",
    "\n",
    "bev_backbone = dict(\n",
    "    type='ResNet',\n",
    "    in_channels=80,\n",
    "    depth=18,\n",
    "    num_stages=3,\n",
    "    strides=(1, 2, 2),\n",
    "    dilations=(1, 1, 1),\n",
    "    out_indices=[0, 1, 2],\n",
    "    norm_eval=False,\n",
    "    base_channels=160,\n",
    ")\n",
    "\n",
    "bev_neck = dict(type='SECONDFPN',\n",
    "                in_channels=[80, 160, 320, 640],\n",
    "                upsample_strides=[1, 2, 4, 8],\n",
    "                out_channels=[64, 64, 64, 64])\n",
    "\n",
    "CLASSES = [\n",
    "    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',\n",
    "    'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone',\n",
    "]\n",
    "\n",
    "TASKS = [\n",
    "    dict(num_class=1, class_names=['car']),\n",
    "    dict(num_class=2, class_names=['truck', 'construction_vehicle']),\n",
    "    dict(num_class=2, class_names=['bus', 'trailer']),\n",
    "    dict(num_class=1, class_names=['barrier']),\n",
    "    dict(num_class=2, class_names=['motorcycle', 'bicycle']),\n",
    "    dict(num_class=2, class_names=['pedestrian', 'traffic_cone']),\n",
    "]\n",
    "\n",
    "common_heads = dict(reg=(2, 2),\n",
    "                    height=(1, 2),\n",
    "                    dim=(3, 2),\n",
    "                    rot=(2, 2),\n",
    "                    vel=(2, 2))\n",
    "\n",
    "bbox_coder = dict(\n",
    "    type='CenterPointBBoxCoder',\n",
    "    post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],\n",
    "    max_num=500,\n",
    "    score_threshold=0.1,\n",
    "    out_size_factor=4,\n",
    "    voxel_size=[0.2, 0.2, 8],\n",
    "    pc_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n",
    "    code_size=9,\n",
    ")\n",
    "\n",
    "train_cfg = dict(\n",
    "    point_cloud_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n",
    "    grid_size=[512, 512, 1],\n",
    "    voxel_size=[0.2, 0.2, 8],\n",
    "    out_size_factor=4,\n",
    "    dense_reg=1,\n",
    "    gaussian_overlap=0.1,\n",
    "    max_objs=500,\n",
    "    min_radius=2,\n",
    "    code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5],\n",
    ")\n",
    "\n",
    "test_cfg = dict(\n",
    "    post_center_limit_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],\n",
    "    max_per_img=500,\n",
    "    max_pool_nms=False,\n",
    "    min_radius=[4, 12, 10, 1, 0.85, 0.175],\n",
    "    score_threshold=0.1,\n",
    "    out_size_factor=4,\n",
    "    voxel_size=[0.2, 0.2, 8],\n",
    "    nms_type='circle',\n",
    "    pre_max_size=1000,\n",
    "    post_max_size=83,\n",
    "    nms_thr=0.2,\n",
    ")\n",
    "\n",
    "head_conf = {\n",
    "    'bev_backbone_conf': bev_backbone,\n",
    "    'bev_neck_conf': bev_neck,\n",
    "    'tasks': TASKS,\n",
    "    'common_heads': common_heads,\n",
    "    'bbox_coder': bbox_coder,\n",
    "    'train_cfg': train_cfg,\n",
    "    'test_cfg': test_cfg,\n",
    "    'in_channels': 256,  # Equal to bev_neck output_channels.\n",
    "    'loss_cls': dict(type='GaussianFocalLoss', reduction='mean'),\n",
    "    'loss_bbox': dict(type='L1Loss', reduction='mean', loss_weight=0.25),\n",
    "    'gaussian_overlap': 0.1,\n",
    "    'min_radius': 2,\n",
    "}\n",
    "\n",
    "DefaultAttribute = {\n",
    "        'car': 'vehicle.parked',\n",
    "        'pedestrian': 'pedestrian.moving',\n",
    "        'trailer': 'vehicle.parked',\n",
    "        'truck': 'vehicle.parked',\n",
    "        'bus': 'vehicle.moving',\n",
    "        'motorcycle': 'cycle.without_rider',\n",
    "        'construction_vehicle': 'vehicle.parked',\n",
    "        'bicycle': 'cycle.without_rider',\n",
    "        'barrier': '',\n",
    "        'traffic_cone': '',\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb3d30-27f2-4eb5-85f9-924ef190b55a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Detector (Lightning module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a1acb8-7d85-4179-947c-3f8b5628fbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class BEVDepthLightningModel(LightningModule):\n",
    "class BEVStereoLightningDetector(LightningModule):\n",
    "    MODEL_NAMES = sorted(name for name in models.__dict__\n",
    "                         if name.islower() and not name.startswith('__')\n",
    "                         and callable(models.__dict__[name]))\n",
    "\n",
    "    def __init__(self,\n",
    "                 gpus: int = 1,\n",
    "                 data_root='data/nuscenes/',\n",
    "                 eval_interval=1,\n",
    "                 batch_size_per_device=8,\n",
    "                 class_names=CLASSES,\n",
    "                 backbone_conf=backbone_conf,\n",
    "                 head_conf=head_conf,\n",
    "                 ida_aug_conf=ida_aug_conf,\n",
    "                 bda_aug_conf=bda_aug_conf,\n",
    "                 default_root_dir='./outputs/',\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.gpus = gpus\n",
    "        self.eval_interval = eval_interval\n",
    "        self.batch_size_per_device = batch_size_per_device\n",
    "        self.data_root = data_root\n",
    "        self.basic_lr_per_img = 2e-4 / 64\n",
    "        self.class_names = class_names\n",
    "        self.backbone_conf = backbone_conf\n",
    "        self.head_conf = head_conf\n",
    "        self.ida_aug_conf = ida_aug_conf\n",
    "        self.bda_aug_conf = bda_aug_conf\n",
    "        mmcv.mkdir_or_exist(default_root_dir)\n",
    "        self.default_root_dir = default_root_dir\n",
    "        self.evaluator = DetNuscEvaluator(class_names=self.class_names,\n",
    "                                          output_dir=self.default_root_dir)\n",
    "        # self.model = BaseBEVDepth(self.backbone_conf,\n",
    "        #                           self.head_conf,\n",
    "        #                           is_train_depth=True)\n",
    "        self.mode = 'valid'\n",
    "        self.img_conf = img_conf\n",
    "        self.data_use_cbgs = False\n",
    "        self.num_sweeps = 1\n",
    "        self.sweep_idxes = list()\n",
    "        self.key_idxes = list()\n",
    "        self.data_return_depth = True\n",
    "        self.downsample_factor = self.backbone_conf['downsample_factor']\n",
    "        self.dbound = self.backbone_conf['d_bound']\n",
    "        self.depth_channels = int((self.dbound[1] - self.dbound[0]) / self.dbound[2])\n",
    "        self.use_fusion = False\n",
    "        \n",
    "        self.train_info_paths = os.path.join(self.data_root, 'nuscenes_infos_train.pkl')\n",
    "        self.val_info_paths = os.path.join(self.data_root, 'nuscenes_infos_val.pkl')\n",
    "        # self.predict_info_paths = os.path.join(self.data_root, 'nuscenes_infos_test.pkl')\n",
    "        self.predict_info_paths = os.path.join(self.data_root, 'nuscenes_infos_val.pkl')\n",
    "        \n",
    "        \"\"\"BEVStereo configurations\"\"\"\n",
    "        # base ckpt of bevstereo (2 key)\n",
    "        self.key_idxes = [-1]\n",
    "        self.head_conf['bev_backbone_conf']['in_channels'] = 80 * (len(self.key_idxes) + 1)\n",
    "        self.head_conf['bev_neck_conf']['in_channels'] = [80 * (len(self.key_idxes) + 1), 160, 320, 640]\n",
    "        self.head_conf['train_cfg']['code_weight'] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "        self.head_conf['test_cfg']['thresh_scale'] = [0.6, 0.4, 0.4, 0.7, 0.8, 0.9]\n",
    "        self.head_conf['test_cfg']['nms_type'] = 'size_aware_circle'\n",
    "        \n",
    "        # ckpt with depth aggregtaion\n",
    "        self.backbone_conf['use_da'] = True\n",
    "        self.data_use_cbgs = True\n",
    "        self.basic_lr_per_img = 2e-4 / 32\n",
    "        \n",
    "        self.model = BEVStereo(self.backbone_conf,\n",
    "                               self.head_conf,\n",
    "                               is_train_depth=True)\n",
    "        \n",
    "        self.starter, self.ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        # device = 'cuda:0'\n",
    "        # torch.cuda.set_device(device)\n",
    "        # self.model.to(device)\n",
    "        \n",
    "    def forward(self, sweep_imgs, mats):\n",
    "        return self.model(sweep_imgs, mats)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        (sweep_imgs, mats, _, _, gt_boxes, gt_labels, depth_labels) = batch\n",
    "        if torch.cuda.is_available():\n",
    "            for key, value in mats.items():\n",
    "                mats[key] = value.cuda()\n",
    "            sweep_imgs = sweep_imgs.cuda()\n",
    "            gt_boxes = [gt_box.cuda() for gt_box in gt_boxes]\n",
    "            gt_labels = [gt_label.cuda() for gt_label in gt_labels]\n",
    "        preds, depth_preds = self(sweep_imgs, mats)\n",
    "        if isinstance(self.model, torch.nn.parallel.DistributedDataParallel):\n",
    "            targets = self.model.module.get_targets(gt_boxes, gt_labels)\n",
    "            detection_loss = self.model.module.loss(targets, preds)\n",
    "        else:\n",
    "            targets = self.model.get_targets(gt_boxes, gt_labels)\n",
    "            detection_loss = self.model.loss(targets, preds)\n",
    "\n",
    "        if len(depth_labels.shape) == 5:\n",
    "            # only key-frame will calculate depth loss\n",
    "            depth_labels = depth_labels[:, 0, ...]\n",
    "        depth_loss = self.get_depth_loss(depth_labels.cuda(), depth_preds)\n",
    "        self.log('detection_loss', detection_loss)\n",
    "        self.log('depth_loss', depth_loss)\n",
    "        return detection_loss + depth_loss\n",
    "\n",
    "    def get_depth_loss(self, depth_labels, depth_preds):\n",
    "        depth_labels = self.get_downsampled_gt_depth(depth_labels)\n",
    "        depth_preds = depth_preds.permute(0, 2, 3, 1).contiguous().view(\n",
    "            -1, self.depth_channels)\n",
    "        fg_mask = torch.max(depth_labels, dim=1).values > 0.0\n",
    "\n",
    "        with autocast(enabled=False):\n",
    "            depth_loss = (F.binary_cross_entropy(\n",
    "                depth_preds[fg_mask],\n",
    "                depth_labels[fg_mask],\n",
    "                reduction='none',\n",
    "            ).sum() / max(1.0, fg_mask.sum()))\n",
    "\n",
    "        return 3.0 * depth_loss\n",
    "\n",
    "    def get_downsampled_gt_depth(self, gt_depths):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            gt_depths: [B, N, H, W]\n",
    "        Output:\n",
    "            gt_depths: [B*N*h*w, d]\n",
    "        \"\"\"\n",
    "        B, N, H, W = gt_depths.shape\n",
    "        gt_depths = gt_depths.view(\n",
    "            B * N,\n",
    "            H // self.downsample_factor,\n",
    "            self.downsample_factor,\n",
    "            W // self.downsample_factor,\n",
    "            self.downsample_factor,\n",
    "            1,\n",
    "        )\n",
    "        gt_depths = gt_depths.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
    "        gt_depths = gt_depths.view(\n",
    "            -1, self.downsample_factor * self.downsample_factor)\n",
    "        gt_depths_tmp = torch.where(gt_depths == 0.0,\n",
    "                                    1e5 * torch.ones_like(gt_depths),\n",
    "                                    gt_depths)\n",
    "        gt_depths = torch.min(gt_depths_tmp, dim=-1).values\n",
    "        gt_depths = gt_depths.view(B * N, H // self.downsample_factor,\n",
    "                                   W // self.downsample_factor)\n",
    "\n",
    "        gt_depths = (gt_depths - (self.dbound[0] - self.dbound[2])) / self.dbound[2]\n",
    "        gt_depths = torch.where(\n",
    "            (gt_depths < self.depth_channels + 1) & (gt_depths >= 0.0),\n",
    "            gt_depths, torch.zeros_like(gt_depths))\n",
    "        gt_depths = F.one_hot(gt_depths.long(),\n",
    "                              num_classes=self.depth_channels + 1).view(\n",
    "                                  -1, self.depth_channels + 1)[:, 1:]\n",
    "\n",
    "        return gt_depths.float()\n",
    "\n",
    "    def eval_step(self, batch, batch_idx, prefix: str):\n",
    "        (sweep_imgs, mats, _, img_metas, _, _) = batch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            for key, value in mats.items():\n",
    "                mats[key] = value.cuda()\n",
    "            sweep_imgs = sweep_imgs.cuda()\n",
    "        \n",
    "        self.starter.record()\n",
    "        \n",
    "        preds = self.model(sweep_imgs, mats)\n",
    "        \n",
    "        print(\"preds: \", preds)\n",
    "        \n",
    "        self.ender.record()\n",
    "        # wait for gpu sync\n",
    "        torch.cuda.synchronize()\n",
    "        inference_time = self.starter.elapsed_time(self.ender)\n",
    "        print(\"inference time (ms): \", inference_time)\n",
    "        \n",
    "        if isinstance(self.model, torch.nn.parallel.DistributedDataParallel):\n",
    "            results = self.model.module.get_bboxes(preds, img_metas)\n",
    "        else:\n",
    "            results = self.model.get_bboxes(preds, img_metas)\n",
    "        for i in range(len(results)):\n",
    "            results[i][0] = results[i][0].detach().cpu().numpy()\n",
    "            results[i][1] = results[i][1].detach().cpu().numpy()\n",
    "            results[i][2] = results[i][2].detach().cpu().numpy()\n",
    "            results[i].append(img_metas[i])\n",
    "        return results\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.eval_step(batch, batch_idx, 'val')\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        all_pred_results = list()\n",
    "        all_img_metas = list()\n",
    "        for validation_step_output in validation_step_outputs:\n",
    "            for i in range(len(validation_step_output)):\n",
    "                all_pred_results.append(validation_step_output[i][:3])\n",
    "                all_img_metas.append(validation_step_output[i][3])\n",
    "        synchronize()\n",
    "        len_dataset = len(self.val_dataloader().dataset)\n",
    "        all_pred_results = sum(\n",
    "            map(list, zip(*all_gather_object(all_pred_results))),\n",
    "            [])[:len_dataset]\n",
    "        all_img_metas = sum(map(list, zip(*all_gather_object(all_img_metas))),\n",
    "                            [])[:len_dataset]\n",
    "        if get_rank() == 0:\n",
    "            self.evaluator.evaluate(all_pred_results, all_img_metas)\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        all_pred_results = list()\n",
    "        all_img_metas = list()\n",
    "        for test_step_output in test_step_outputs:\n",
    "            for i in range(len(test_step_output)):\n",
    "                all_pred_results.append(test_step_output[i][:3])\n",
    "                all_img_metas.append(test_step_output[i][3])\n",
    "        synchronize()\n",
    "        # TODO: Change another way.\n",
    "        dataset_length = len(self.val_dataloader().dataset)\n",
    "        all_pred_results = sum(\n",
    "            map(list, zip(*all_gather_object(all_pred_results))),\n",
    "            [])[:dataset_length]\n",
    "        all_img_metas = sum(map(list, zip(*all_gather_object(all_img_metas))),\n",
    "                            [])[:dataset_length]\n",
    "        if get_rank() == 0:\n",
    "            self.evaluator.evaluate(all_pred_results, all_img_metas)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.basic_lr_per_img * self.batch_size_per_device * self.gpus\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-7)\n",
    "        scheduler = MultiStepLR(optimizer, [19, 23])\n",
    "        return [[optimizer], [scheduler]]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = NuscDetDataset(ida_aug_conf=self.ida_aug_conf,\n",
    "                                       bda_aug_conf=self.bda_aug_conf,\n",
    "                                       classes=self.class_names,\n",
    "                                       data_root=self.data_root,\n",
    "                                       info_paths=self.train_info_paths,\n",
    "                                       is_train=True,\n",
    "                                       use_cbgs=self.data_use_cbgs,\n",
    "                                       img_conf=self.img_conf,\n",
    "                                       num_sweeps=self.num_sweeps,\n",
    "                                       sweep_idxes=self.sweep_idxes,\n",
    "                                       key_idxes=self.key_idxes,\n",
    "                                       return_depth=self.data_return_depth,\n",
    "                                       use_fusion=self.use_fusion)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=4,\n",
    "            drop_last=True,\n",
    "            shuffle=False,\n",
    "            collate_fn=partial(collate_fn,\n",
    "                               is_return_depth=self.data_return_depth\n",
    "                               or self.use_fusion),\n",
    "            sampler=None,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = NuscDetDataset(ida_aug_conf=self.ida_aug_conf,\n",
    "                                     bda_aug_conf=self.bda_aug_conf,\n",
    "                                     classes=self.class_names,\n",
    "                                     data_root=self.data_root,\n",
    "                                     info_paths=self.val_info_paths,\n",
    "                                     is_train=False,\n",
    "                                     img_conf=self.img_conf,\n",
    "                                     num_sweeps=self.num_sweeps,\n",
    "                                     sweep_idxes=self.sweep_idxes,\n",
    "                                     key_idxes=self.key_idxes,\n",
    "                                     return_depth=self.use_fusion,\n",
    "                                     use_fusion=self.use_fusion)\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            shuffle=False,\n",
    "            collate_fn=partial(collate_fn, is_return_depth=self.use_fusion),\n",
    "            num_workers=4,\n",
    "            sampler=None,\n",
    "        )\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.val_dataloader()\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        predict_dataset = NuscDetDataset(ida_aug_conf=self.ida_aug_conf,\n",
    "                                         bda_aug_conf=self.bda_aug_conf,\n",
    "                                         classes=self.class_names,\n",
    "                                         data_root=self.data_root,\n",
    "                                         info_paths=self.predict_info_paths,\n",
    "                                         is_train=False,\n",
    "                                         img_conf=self.img_conf,\n",
    "                                         num_sweeps=self.num_sweeps,\n",
    "                                         sweep_idxes=self.sweep_idxes,\n",
    "                                         key_idxes=self.key_idxes,\n",
    "                                         return_depth=self.use_fusion,\n",
    "                                         use_fusion=self.use_fusion)\n",
    "        predict_loader = torch.utils.data.DataLoader(\n",
    "            predict_dataset,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            shuffle=False,\n",
    "            collate_fn=partial(collate_fn, is_return_depth=self.use_fusion),\n",
    "            num_workers=4,\n",
    "            sampler=None,\n",
    "        )\n",
    "        return predict_loader\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.eval_step(batch, batch_idx, 'test')\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        return self.eval_step(batch, batch_idx, 'predict')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.basic_lr_per_img * self.batch_size_per_device * self.gpus\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "        scheduler = MultiStepLR(optimizer, [16, 19])\n",
    "        return [[optimizer], [scheduler]]\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):  # pragma: no-cover\n",
    "        return parent_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0797d9-4fce-4822-b201-78c93356418d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Pose Network (from Manydepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf5935a4-5f18-47b4-9c49-cd0ab06de9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transformation_from_parameters(axisangle, translation, invert=False):\n",
    "    \"\"\"Convert the network's (axisangle, translation) output into a 4x4 matrix\n",
    "    \"\"\"\n",
    "    R = rot_from_axisangle(axisangle)\n",
    "    t = translation.clone()\n",
    "\n",
    "    if invert:\n",
    "        R = R.transpose(1, 2)\n",
    "        t *= -1\n",
    "\n",
    "    T = get_translation_matrix(t)\n",
    "    \n",
    "    # print(\"R: \", R)\n",
    "    # print(\"R[:3, :3]: \", R[:, :3, :3])\n",
    "    # print(\"T: \", T)\n",
    "    # print(\"T[~]: \", T[:, :3, -1])\n",
    "    \n",
    "    # R, T를 이어 붙여서 return\n",
    "    M = R.new_zeros((4, 4))\n",
    "    M[3, 3] = 1\n",
    "    M[:3, :3] = R[:, :3, :3]\n",
    "    M[:3, -1] = T[:, :3, -1]\n",
    "\n",
    "    # R, T 간의 행렬곱을 return\n",
    "    # if invert:\n",
    "    #     M = torch.matmul(R, T)\n",
    "    # else:\n",
    "    #     M = torch.matmul(T, R)\n",
    "\n",
    "    return M\n",
    "\n",
    "def get_translation_matrix(translation_vector):\n",
    "    \"\"\"Convert a translation vector into a 4x4 transformation matrix\n",
    "    \"\"\"\n",
    "    T = torch.zeros(translation_vector.shape[0], 4, 4).to(device=translation_vector.device)\n",
    "\n",
    "    t = translation_vector.contiguous().view(-1, 3, 1)\n",
    "\n",
    "    T[:, 0, 0] = 1\n",
    "    T[:, 1, 1] = 1\n",
    "    T[:, 2, 2] = 1\n",
    "    T[:, 3, 3] = 1\n",
    "    T[:, :3, 3, None] = t\n",
    "\n",
    "    return T\n",
    "\n",
    "def rot_from_axisangle(vec):\n",
    "    \"\"\"Convert an axisangle rotation into a 4x4 transformation matrix\n",
    "    (adapted from https://github.com/Wallacoloo/printipi)\n",
    "    Input 'vec' has to be Bx1x3\n",
    "    \"\"\"\n",
    "    angle = torch.norm(vec, 2, 2, True)\n",
    "    axis = vec / (angle + 1e-7)\n",
    "\n",
    "    ca = torch.cos(angle)\n",
    "    sa = torch.sin(angle)\n",
    "    C = 1 - ca\n",
    "\n",
    "    x = axis[..., 0].unsqueeze(1)\n",
    "    y = axis[..., 1].unsqueeze(1)\n",
    "    z = axis[..., 2].unsqueeze(1)\n",
    "\n",
    "    xs = x * sa\n",
    "    ys = y * sa\n",
    "    zs = z * sa\n",
    "    xC = x * C\n",
    "    yC = y * C\n",
    "    zC = z * C\n",
    "    xyC = x * yC\n",
    "    yzC = y * zC\n",
    "    zxC = z * xC\n",
    "\n",
    "    rot = torch.zeros((vec.shape[0], 4, 4)).to(device=vec.device)\n",
    "\n",
    "    rot[:, 0, 0] = torch.squeeze(x * xC + ca)\n",
    "    rot[:, 0, 1] = torch.squeeze(xyC - zs)\n",
    "    rot[:, 0, 2] = torch.squeeze(zxC + ys)\n",
    "    rot[:, 1, 0] = torch.squeeze(xyC + zs)\n",
    "    rot[:, 1, 1] = torch.squeeze(y * yC + ca)\n",
    "    rot[:, 1, 2] = torch.squeeze(yzC - xs)\n",
    "    rot[:, 2, 0] = torch.squeeze(zxC - ys)\n",
    "    rot[:, 2, 1] = torch.squeeze(yzC + xs)\n",
    "    rot[:, 2, 2] = torch.squeeze(z * zC + ca)\n",
    "    rot[:, 3, 3] = 1\n",
    "\n",
    "    return rot\n",
    "\n",
    "class ResNetMultiImageInput(models.ResNet):\n",
    "    \"\"\"Constructs a resnet model with varying number of input images.\n",
    "    Adapted from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, num_input_images=1):\n",
    "        super(ResNetMultiImageInput, self).__init__(block, layers)\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            num_input_images * 3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def resnet_multiimage_input(num_layers, pretrained=False, num_input_images=1):\n",
    "    \"\"\"Constructs a ResNet model.\n",
    "    Args:\n",
    "        num_layers (int): Number of resnet layers. Must be 18 or 50\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        num_input_images (int): Number of frames stacked as input\n",
    "    \"\"\"\n",
    "    assert num_layers in [18, 50], \"Can only run with 18 or 50 layer resnet\"\n",
    "    blocks = {18: [2, 2, 2, 2], 50: [3, 4, 6, 3]}[num_layers]\n",
    "    block_type = {18: models.resnet.BasicBlock, 50: models.resnet.Bottleneck}[num_layers]\n",
    "    model = ResNetMultiImageInput(block_type, blocks, num_input_images=num_input_images)\n",
    "\n",
    "    if pretrained:\n",
    "        loaded = model_zoo.load_url(models.resnet.model_urls['resnet{}'.format(num_layers)])\n",
    "        loaded['conv1.weight'] = torch.cat(\n",
    "            [loaded['conv1.weight']] * num_input_images, 1) / num_input_images\n",
    "        model.load_state_dict(loaded)\n",
    "    return model\n",
    "\n",
    "\"\"\"Pose Encoder\"\"\"\n",
    "class ResnetEncoder(nn.Module):\n",
    "    \"\"\"Pytorch module for a resnet encoder\"\"\"\n",
    "    def __init__(self, num_layers, pretrained, num_input_images=1, **kwargs):\n",
    "        super(ResnetEncoder, self).__init__()\n",
    "\n",
    "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
    "\n",
    "        resnets = {18: models.resnet18,\n",
    "                   34: models.resnet34,\n",
    "                   50: models.resnet50,\n",
    "                   101: models.resnet101,\n",
    "                   152: models.resnet152}\n",
    "\n",
    "        if num_layers not in resnets:\n",
    "            raise ValueError(\"{} is not a valid number of resnet layers\".format(num_layers))\n",
    "\n",
    "        if num_input_images > 1:\n",
    "            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n",
    "        else:\n",
    "            self.encoder = resnets[num_layers](pretrained)\n",
    "\n",
    "        if num_layers > 34:\n",
    "            self.num_ch_enc[1:] *= 4\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        self.features = []\n",
    "        x = (input_image - 0.45) / 0.225\n",
    "        x = self.encoder.conv1(x)\n",
    "        x = self.encoder.bn1(x)\n",
    "        self.features.append(self.encoder.relu(x))\n",
    "        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n",
    "        self.features.append(self.encoder.layer2(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer3(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer4(self.features[-1]))\n",
    "\n",
    "        return self.features\n",
    "\n",
    "\"\"\"Pose Decoder\"\"\"\n",
    "class PoseDecoder(nn.Module):\n",
    "    def __init__(self, num_ch_enc, num_input_features, num_frames_to_predict_for=None, stride=1):\n",
    "        super(PoseDecoder, self).__init__()\n",
    "\n",
    "        self.num_ch_enc = num_ch_enc\n",
    "        self.num_input_features = num_input_features\n",
    "\n",
    "        if num_frames_to_predict_for is None:\n",
    "            num_frames_to_predict_for = num_input_features - 1\n",
    "        self.num_frames_to_predict_for = num_frames_to_predict_for\n",
    "\n",
    "        self.convs = OrderedDict()\n",
    "        self.convs[(\"squeeze\")] = nn.Conv2d(self.num_ch_enc[-1], 256, 1)\n",
    "        self.convs[(\"pose\", 0)] = nn.Conv2d(num_input_features * 256, 256, 3, stride, 1)\n",
    "        self.convs[(\"pose\", 1)] = nn.Conv2d(256, 256, 3, stride, 1)\n",
    "        self.convs[(\"pose\", 2)] = nn.Conv2d(256, 6 * num_frames_to_predict_for, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.net = nn.ModuleList(list(self.convs.values()))\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        last_features = [f[-1] for f in input_features]\n",
    "\n",
    "        cat_features = [self.relu(self.convs[\"squeeze\"](f)) for f in last_features]\n",
    "        cat_features = torch.cat(cat_features, 1)\n",
    "\n",
    "        out = cat_features\n",
    "        for i in range(3):\n",
    "            out = self.convs[(\"pose\", i)](out)\n",
    "            if i != 2:\n",
    "                out = self.relu(out)\n",
    "\n",
    "        out = out.mean(3).mean(2)\n",
    "\n",
    "        out = 0.01 * out.view(-1, self.num_frames_to_predict_for, 1, 6)\n",
    "\n",
    "        axisangle = out[..., :3]\n",
    "        translation = out[..., 3:]\n",
    "\n",
    "        return axisangle, translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddddbd6-0ebe-4ba4-912e-9ba1dd5ce49b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tangent Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83ab1ced-bb85-413a-a623-2518284c38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createProjectGrid(erp_h, erp_w, tangent_h, tangent_w, num_rows, num_cols, phi_centers, fov):\n",
    "    height, width = tangent_h, tangent_w\n",
    "\n",
    "    FOV = fov\n",
    "    FOV = [FOV[0] / 360.0, FOV[1] / 180.0]\n",
    "    FOV = torch.tensor(FOV, dtype=torch.float32)\n",
    "\n",
    "    PI = math.pi\n",
    "    PI_2 = math.pi * 0.5\n",
    "    PI2 = math.pi * 2\n",
    "\n",
    "    yy, xx = torch.meshgrid(torch.linspace(0, 1, height), torch.linspace(0, 1, width))\n",
    "    screen_points = torch.stack([xx.flatten(), yy.flatten()], -1)\n",
    "    \n",
    "    num_rows = num_rows\n",
    "    num_cols = num_cols\n",
    "    phi_centers = phi_centers\n",
    "\n",
    "    phi_interval = 180 // num_rows\n",
    "    all_combos = []\n",
    "    erp_mask = []\n",
    "    \n",
    "    for i, n_cols in enumerate(num_cols):\n",
    "        for j in np.arange(n_cols): # 0 ~ num_cols.length\n",
    "            theta_interval = 360 / n_cols # 현재 row (위도)에서 쪼개질 경도 (col)의 위치\n",
    "            theta_center = j * theta_interval + theta_interval / 2\n",
    "            center = [theta_center, phi_centers[i]] # 각 tangent image의 center position\n",
    "            \n",
    "            # print(str(j) + \" th theta center \" + str(theta_center) + \" phi center \" + str(phi_centers[i]))\n",
    "            \n",
    "            all_combos.append(center)\n",
    "\n",
    "            # 구좌표계에서의 tangent image가 차지하는 영역에 대한 좌표들\n",
    "            up = phi_centers[i] + phi_interval / 2\n",
    "            down = phi_centers[i] - phi_interval / 2\n",
    "            left = theta_center - theta_interval / 2\n",
    "            right = theta_center + theta_interval / 2\n",
    "\n",
    "            # ERP image에서 현재 tangent가 차지하는 영역에 대한 pixel 위치들\n",
    "            up = int((up + 90) / 180 * erp_h)\n",
    "            down = int((down + 90) / 180 * erp_h)\n",
    "            left = int(left / 360 * erp_w)\n",
    "            right = int(right / 360 * erp_w)\n",
    "\n",
    "            # ERP 이미지에서 현재 tangent image 영역에 해당하는 부분에 1로 마스킹\n",
    "            mask = np.zeros((erp_h, erp_w), dtype=int)\n",
    "            mask[down:up, left:right] = 1\n",
    "            erp_mask.append(mask)\n",
    "\n",
    "    all_combos = np.vstack(all_combos)\n",
    "    shifts = np.arange(all_combos.shape[0]) * width\n",
    "    shifts = torch.from_numpy(shifts).float()\n",
    "    erp_mask = np.stack(erp_mask)\n",
    "    erp_mask = torch.from_numpy(erp_mask).float()\n",
    "    n_patch = all_combos.shape[0]\n",
    "    \n",
    "    center_point = torch.from_numpy(all_combos).float()  # -180 to 180, -90 to 90\n",
    "    center_point[:, 0] = (center_point[:, 0]) / 360  #0 to 1\n",
    "    center_point[:, 1] = (center_point[:, 1] + 90) / 180  #0 to 1\n",
    "\n",
    "    cp = center_point * 2 - 1\n",
    "    cp[:, 0] = cp[:, 0] * PI\n",
    "    cp[:, 1] = cp[:, 1] * PI_2\n",
    "    cp = cp.unsqueeze(1)\n",
    "\n",
    "    convertedCoord = screen_points * 2 - 1\n",
    "    convertedCoord[:, 0] = convertedCoord[:, 0] * PI\n",
    "    convertedCoord[:, 1] = convertedCoord[:, 1] * PI_2\n",
    "    convertedCoord = convertedCoord * (torch.ones(screen_points.shape, dtype=torch.float32) * FOV)\n",
    "    convertedCoord = convertedCoord.unsqueeze(0).repeat(cp.shape[0], 1, 1)\n",
    "    \n",
    "    x = convertedCoord[:, :, 0]\n",
    "    y = convertedCoord[:, :, 1]\n",
    "\n",
    "    rou = torch.sqrt(x ** 2 + y ** 2)\n",
    "    c = torch.atan(rou)\n",
    "    sin_c = torch.sin(c)\n",
    "    cos_c = torch.cos(c)\n",
    "    lat = torch.asin(cos_c * torch.sin(cp[:, :, 1]) + (y * sin_c * torch.cos(cp[:, :, 1])) / rou)\n",
    "    lon = cp[:, :, 0] + torch.atan2(x * sin_c, rou * torch.cos(cp[:, :, 1]) * cos_c - y * torch.sin(cp[:, :, 1]) * sin_c)\n",
    "    lat_new = lat / PI_2\n",
    "    lon_new = lon / PI\n",
    "    lon_new[lon_new > 1] -= 2\n",
    "    lon_new[lon_new<-1] += 2\n",
    "\n",
    "    lon_new = lon_new.view(1, n_patch, height, width).permute(0, 2, 1, 3).contiguous().view(height, n_patch*width)\n",
    "    lat_new = lat_new.view(1, n_patch, height, width).permute(0, 2, 1, 3).contiguous().view(height, n_patch*width)\n",
    "    \n",
    "    grid = torch.stack([lon_new, lat_new], -1)\n",
    "    grid = grid.unsqueeze(0)\n",
    "    grid = grid.to(device)\n",
    "\n",
    "    return n_patch, grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6f5d94-3538-4587-a6f2-e0a669c2f1c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1566f2b-727c-421e-8bfc-626d4fef0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_name_from_general_to_detection = {\n",
    "    'human.pedestrian.adult': 'pedestrian',\n",
    "    'human.pedestrian.child': 'pedestrian',\n",
    "    'human.pedestrian.wheelchair': 'ignore',\n",
    "    'human.pedestrian.stroller': 'ignore',\n",
    "    'human.pedestrian.personal_mobility': 'ignore',\n",
    "    'human.pedestrian.police_officer': 'pedestrian',\n",
    "    'human.pedestrian.construction_worker': 'pedestrian',\n",
    "    'animal': 'ignore',\n",
    "    'vehicle.car': 'car',\n",
    "    'vehicle.motorcycle': 'motorcycle',\n",
    "    'vehicle.bicycle': 'bicycle',\n",
    "    'vehicle.bus.bendy': 'bus',\n",
    "    'vehicle.bus.rigid': 'bus',\n",
    "    'vehicle.truck': 'truck',\n",
    "    'vehicle.construction': 'construction_vehicle',\n",
    "    'vehicle.emergency.ambulance': 'ignore',\n",
    "    'vehicle.emergency.police': 'ignore',\n",
    "    'vehicle.trailer': 'trailer',\n",
    "    'movable_object.barrier': 'barrier',\n",
    "    'movable_object.trafficcone': 'traffic_cone',\n",
    "    'movable_object.pushable_pullable': 'ignore',\n",
    "    'movable_object.debris': 'ignore',\n",
    "    'static_object.bicycle_rack': 'ignore',\n",
    "}\n",
    "\n",
    "def get_quaternion_from_euler(e):\n",
    "    \"\"\"\n",
    "    Convert an Euler angle to a quaternion.\n",
    "\n",
    "    Input\n",
    "    :param roll: The roll (rotation around x-axis) angle in radians.\n",
    "    :param pitch: The pitch (rotation around y-axis) angle in radians.\n",
    "    :param yaw: The yaw (rotation around z-axis) angle in radians.\n",
    "\n",
    "    Output\n",
    "    :return qx, qy, qz, qw: The orientation in quaternion [x,y,z,w] format\n",
    "    \"\"\"\n",
    "    roll = e[0]\n",
    "    pitch = e[1]\n",
    "    yaw = e[2]\n",
    "\n",
    "    qx = np.sin(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) - np.cos(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)\n",
    "    qy = np.cos(roll/2) * np.sin(pitch/2) * np.cos(yaw/2) + np.sin(roll/2) * np.cos(pitch/2) * np.sin(yaw/2)\n",
    "    qz = np.cos(roll/2) * np.cos(pitch/2) * np.sin(yaw/2) - np.sin(roll/2) * np.sin(pitch/2) * np.cos(yaw/2)\n",
    "    qw = np.cos(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) + np.sin(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)\n",
    "\n",
    "    return [qw, qx, qy, qz]\n",
    "\n",
    "def euler_from_quaternion(q):\n",
    "    \"\"\"\n",
    "    Convert a quaternion into euler angles (roll, pitch, yaw)\n",
    "    roll is rotation around x in radians (counterclockwise)\n",
    "    pitch is rotation around y in radians (counterclockwise)\n",
    "    yaw is rotation around z in radians (counterclockwise)\n",
    "    \"\"\"\n",
    "    import math\n",
    "    w = q[0]\n",
    "    x = q[1]\n",
    "    y = q[2]\n",
    "    z = q[3]\n",
    "    \n",
    "    t0 = +2.0 * (w * x + y * z)\n",
    "    t1 = +1.0 - 2.0 * (x * x + y * y)\n",
    "    # roll_x = math.atan2(t0, t1) / np.pi * 180 # degrees\n",
    "    roll_x = math.atan2(t0, t1)\n",
    "\n",
    "    t2 = +2.0 * (w * y - z * x)\n",
    "    t2 = +1.0 if t2 > +1.0 else t2\n",
    "    t2 = -1.0 if t2 < -1.0 else t2\n",
    "    # pitch_y = math.asin(t2) / np.pi * 180\n",
    "    pitch_y = math.asin(t2)\n",
    "\n",
    "    t3 = +2.0 * (w * z + x * y)\n",
    "    t4 = +1.0 - 2.0 * (y * y + z * z) \n",
    "    # yaw_z = math.atan2(t3, t4) / np.pi * 180\n",
    "    yaw_z = math.atan2(t3, t4)\n",
    "\n",
    "    return [roll_x, pitch_y, yaw_z] # in radian\n",
    "\n",
    "def get_rot(h):\n",
    "    return torch.Tensor([\n",
    "        [np.cos(h), np.sin(h)],\n",
    "        [-np.sin(h), np.cos(h)],\n",
    "    ])\n",
    "\n",
    "def img_transform(img, resize, resize_dims, crop, flip, rotate):\n",
    "    ida_rot = torch.eye(2)\n",
    "    ida_tran = torch.zeros(2)\n",
    "    # adjust image\n",
    "    img = img.resize(resize_dims)\n",
    "    img = img.crop(crop)\n",
    "    if flip:\n",
    "        img = img.transpose(method=Image.FLIP_LEFT_RIGHT)\n",
    "    img = img.rotate(rotate)\n",
    "\n",
    "    # post-homography transformation\n",
    "    ida_rot *= resize\n",
    "    ida_tran -= torch.Tensor(crop[:2])\n",
    "    if flip:\n",
    "        A = torch.Tensor([[-1, 0], [0, 1]])\n",
    "        b = torch.Tensor([crop[2] - crop[0], 0])\n",
    "        ida_rot = A.matmul(ida_rot)\n",
    "        ida_tran = A.matmul(ida_tran) + b\n",
    "    A = get_rot(rotate / 180 * np.pi)\n",
    "    b = torch.Tensor([crop[2] - crop[0], crop[3] - crop[1]]) / 2\n",
    "    b = A.matmul(-b) + b\n",
    "    ida_rot = A.matmul(ida_rot)\n",
    "    ida_tran = A.matmul(ida_tran) + b\n",
    "    ida_mat = ida_rot.new_zeros(4, 4)\n",
    "    ida_mat[3, 3] = 1\n",
    "    ida_mat[2, 2] = 1\n",
    "    ida_mat[:2, :2] = ida_rot\n",
    "    ida_mat[:2, 3] = ida_tran\n",
    "    return img, ida_mat\n",
    "\n",
    "\n",
    "def bev_transform(gt_boxes, rotate_angle, scale_ratio, flip_dx, flip_dy):\n",
    "    rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n",
    "    rot_sin = torch.sin(rotate_angle)\n",
    "    rot_cos = torch.cos(rotate_angle)\n",
    "    rot_mat = torch.Tensor([[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0],\n",
    "                            [0, 0, 1]])\n",
    "    scale_mat = torch.Tensor([[scale_ratio, 0, 0], [0, scale_ratio, 0],\n",
    "                              [0, 0, scale_ratio]])\n",
    "    flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    if flip_dx:\n",
    "        flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    if flip_dy:\n",
    "        flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n",
    "    rot_mat = flip_mat @ (scale_mat @ rot_mat)\n",
    "    if gt_boxes.shape[0] > 0:\n",
    "        gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n",
    "        gt_boxes[:, 3:6] *= scale_ratio\n",
    "        gt_boxes[:, 6] += rotate_angle\n",
    "        if flip_dx:\n",
    "            gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n",
    "        if flip_dy:\n",
    "            gt_boxes[:, 6] = -gt_boxes[:, 6]\n",
    "        gt_boxes[:, 7:] = (\n",
    "            rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(-1)\n",
    "    return gt_boxes, rot_mat\n",
    "\n",
    "\n",
    "def depth_transform(cam_depth, resize, resize_dims, crop, flip, rotate):\n",
    "    \"\"\"Transform depth based on ida augmentation configuration.\n",
    "\n",
    "    Args:\n",
    "        cam_depth (np array): Nx3, 3: x,y,d.\n",
    "        resize (float): Resize factor.\n",
    "        resize_dims (list): Final dimension.\n",
    "        crop (list): x1, y1, x2, y2\n",
    "        flip (bool): Whether to flip.\n",
    "        rotate (float): Rotation value.\n",
    "\n",
    "    Returns:\n",
    "        np array: [h/down_ratio, w/down_ratio, d]\n",
    "    \"\"\"\n",
    "\n",
    "    H, W = resize_dims\n",
    "    cam_depth[:, :2] = cam_depth[:, :2] * resize\n",
    "    cam_depth[:, 0] -= crop[0]\n",
    "    cam_depth[:, 1] -= crop[1]\n",
    "    if flip:\n",
    "        cam_depth[:, 0] = resize_dims[1] - cam_depth[:, 0]\n",
    "\n",
    "    cam_depth[:, 0] -= W / 2.0\n",
    "    cam_depth[:, 1] -= H / 2.0\n",
    "\n",
    "    h = rotate / 180 * np.pi\n",
    "    rot_matrix = [\n",
    "        [np.cos(h), np.sin(h)],\n",
    "        [-np.sin(h), np.cos(h)],\n",
    "    ]\n",
    "    cam_depth[:, :2] = np.matmul(rot_matrix, cam_depth[:, :2].T).T\n",
    "\n",
    "    cam_depth[:, 0] += W / 2.0\n",
    "    cam_depth[:, 1] += H / 2.0\n",
    "\n",
    "    depth_coords = cam_depth[:, :2].astype(np.int16)\n",
    "\n",
    "    depth_map = np.zeros(resize_dims)\n",
    "    valid_mask = ((depth_coords[:, 1] < resize_dims[0])\n",
    "                  & (depth_coords[:, 0] < resize_dims[1])\n",
    "                  & (depth_coords[:, 1] >= 0)\n",
    "                  & (depth_coords[:, 0] >= 0))\n",
    "    depth_map[depth_coords[valid_mask, 1],\n",
    "              depth_coords[valid_mask, 0]] = cam_depth[valid_mask, 2]\n",
    "\n",
    "    return torch.Tensor(depth_map)\n",
    "\n",
    "\n",
    "def map_pointcloud_to_image(\n",
    "    lidar_points,\n",
    "    img,\n",
    "    lidar_calibrated_sensor,\n",
    "    lidar_ego_pose,\n",
    "    cam_calibrated_sensor,\n",
    "    cam_ego_pose,\n",
    "    min_dist: float = 0.0,\n",
    "):\n",
    "\n",
    "    # Points live in the point sensor frame. So they need to be\n",
    "    # transformed via global to the image plane.\n",
    "    # First step: transform the pointcloud to the ego vehicle\n",
    "    # frame for the timestamp of the sweep.\n",
    "\n",
    "    lidar_points = LidarPointCloud(lidar_points.T)\n",
    "    lidar_points.rotate(\n",
    "        Quaternion(lidar_calibrated_sensor['rotation']).rotation_matrix)\n",
    "    lidar_points.translate(np.array(lidar_calibrated_sensor['translation']))\n",
    "\n",
    "    # Second step: transform from ego to the global frame.\n",
    "    lidar_points.rotate(Quaternion(lidar_ego_pose['rotation']).rotation_matrix)\n",
    "    lidar_points.translate(np.array(lidar_ego_pose['translation']))\n",
    "\n",
    "    # Third step: transform from global into the ego vehicle\n",
    "    # frame for the timestamp of the image.\n",
    "    lidar_points.translate(-np.array(cam_ego_pose['translation']))\n",
    "    lidar_points.rotate(Quaternion(cam_ego_pose['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fourth step: transform from ego into the camera.\n",
    "    lidar_points.translate(-np.array(cam_calibrated_sensor['translation']))\n",
    "    lidar_points.rotate(\n",
    "        Quaternion(cam_calibrated_sensor['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fifth step: actually take a \"picture\" of the point cloud.\n",
    "    # Grab the depths (camera frame z axis points away from the camera).\n",
    "    depths = lidar_points.points[2, :]\n",
    "    coloring = depths\n",
    "\n",
    "    # Take the actual picture (matrix multiplication with camera-matrix\n",
    "    # + renormalization).\n",
    "    points = view_points(lidar_points.points[:3, :],\n",
    "                         np.array(cam_calibrated_sensor['camera_intrinsic']),\n",
    "                         normalize=True)\n",
    "\n",
    "    # Remove points that are either outside or behind the camera.\n",
    "    # Leave a margin of 1 pixel for aesthetic reasons. Also make\n",
    "    # sure points are at least 1m in front of the camera to avoid\n",
    "    # seeing the lidar points on the camera casing for non-keyframes\n",
    "    # which are slightly out of sync.\n",
    "    mask = np.ones(depths.shape[0], dtype=bool)\n",
    "    mask = np.logical_and(mask, depths > min_dist)\n",
    "    mask = np.logical_and(mask, points[0, :] > 1)\n",
    "    mask = np.logical_and(mask, points[0, :] < img.size[0] - 1)\n",
    "    mask = np.logical_and(mask, points[1, :] > 1)\n",
    "    mask = np.logical_and(mask, points[1, :] < img.size[1] - 1)\n",
    "    points = points[:, mask]\n",
    "    coloring = coloring[mask]\n",
    "\n",
    "    return points, coloring\n",
    "\n",
    "def collate_fn(data, is_return_depth=False):\n",
    "    imgs_batch = list()\n",
    "    \n",
    "    # for pose network\n",
    "    pose_input_imgs_batch = list()\n",
    "    \n",
    "    sensor2ego_mats_batch = list()\n",
    "    intrin_mats_batch = list()\n",
    "    ida_mats_batch = list()\n",
    "    sensor2sensor_mats_batch = list()\n",
    "    bda_mat_batch = list()\n",
    "    timestamps_batch = list()\n",
    "    gt_boxes_batch = list()\n",
    "    gt_labels_batch = list()\n",
    "    img_metas_batch = list()\n",
    "    depth_labels_batch = list()\n",
    "    \n",
    "    for iter_data in data:\n",
    "        (\n",
    "            sweep_imgs,\n",
    "            sweep_sensor2ego_mats,\n",
    "            sweep_intrins,\n",
    "            sweep_ida_mats,\n",
    "            sweep_sensor2sensor_mats,\n",
    "            bda_mat,\n",
    "            sweep_timestamps,\n",
    "            img_metas,\n",
    "            gt_boxes,\n",
    "            gt_labels,\n",
    "            \n",
    "            # for pose network\n",
    "            pose_input_imgs\n",
    "        ) = iter_data[:11]\n",
    "        if is_return_depth:\n",
    "            gt_depth = iter_data[10]\n",
    "            depth_labels_batch.append(gt_depth)\n",
    "            \n",
    "        imgs_batch.append(sweep_imgs)\n",
    "        \n",
    "        # for pose network\n",
    "        pose_input_imgs_batch.append(pose_input_imgs)\n",
    "        \n",
    "        sensor2ego_mats_batch.append(sweep_sensor2ego_mats)\n",
    "        intrin_mats_batch.append(sweep_intrins)\n",
    "        ida_mats_batch.append(sweep_ida_mats)\n",
    "        sensor2sensor_mats_batch.append(sweep_sensor2sensor_mats)\n",
    "        bda_mat_batch.append(bda_mat)\n",
    "        timestamps_batch.append(sweep_timestamps)\n",
    "        img_metas_batch.append(img_metas)\n",
    "        gt_boxes_batch.append(gt_boxes)\n",
    "        gt_labels_batch.append(gt_labels)\n",
    "    mats_dict = dict()\n",
    "    mats_dict['sensor2ego_mats'] = torch.stack(sensor2ego_mats_batch)\n",
    "    mats_dict['intrin_mats'] = torch.stack(intrin_mats_batch)\n",
    "    mats_dict['ida_mats'] = torch.stack(ida_mats_batch)\n",
    "    mats_dict['sensor2sensor_mats'] = torch.stack(sensor2sensor_mats_batch)\n",
    "    mats_dict['bda_mat'] = torch.stack(bda_mat_batch)\n",
    "    \n",
    "    ret_list = [\n",
    "        torch.stack(imgs_batch),\n",
    "        mats_dict,\n",
    "        torch.stack(timestamps_batch),\n",
    "        img_metas_batch,\n",
    "        gt_boxes_batch,\n",
    "        gt_labels_batch,\n",
    "        \n",
    "        # for pose network\n",
    "        pose_input_imgs_batch\n",
    "    ]\n",
    "    if is_return_depth:\n",
    "        ret_list.append(torch.stack(depth_labels_batch))\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c376f3bd-70b9-47ab-bc22-c23c7c41993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NuscDetDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 ida_aug_conf,\n",
    "                 bda_aug_conf,\n",
    "                 classes,\n",
    "                 data_root,\n",
    "                 info_paths,\n",
    "                 is_train,\n",
    "                 \n",
    "                 # Dataset customization\n",
    "                 tangent_intrinsics=None,\n",
    "                 sensor2ego_rot_eulers=None,\n",
    "                 sensor2ego_trans=None,\n",
    "                 ego2global_rotation=None,\n",
    "                 ego2global_translation=None,\n",
    "                 \n",
    "                 infos=None,\n",
    "                 \n",
    "                 use_cbgs=False,\n",
    "                 num_sweeps=1,\n",
    "                 img_conf=dict(img_mean=[123.675, 116.28, 103.53],\n",
    "                               img_std=[58.395, 57.12, 57.375],\n",
    "                               to_rgb=True),\n",
    "                 return_depth=False,\n",
    "                 sweep_idxes=list(),\n",
    "                 key_idxes=list(),\n",
    "                 use_fusion=False):\n",
    "        \"\"\"Dataset used for bevdetection task.\n",
    "        Args:\n",
    "            ida_aug_conf (dict): Config for ida augmentation.\n",
    "            bda_aug_conf (dict): Config for bda augmentation.\n",
    "            classes (list): Class names.\n",
    "            use_cbgs (bool): Whether to use cbgs strategy,\n",
    "                Default: False.\n",
    "            num_sweeps (int): Number of sweeps to be used for each sample.\n",
    "                default: 1.\n",
    "            img_conf (dict): Config for image.\n",
    "            return_depth (bool): Whether to use depth gt.\n",
    "                default: False.\n",
    "            sweep_idxes (list): List of sweep idxes to be used.\n",
    "                default: list().\n",
    "            key_idxes (list): List of key idxes to be used.\n",
    "                default: list().\n",
    "            use_fusion (bool): Whether to use lidar data.\n",
    "                default: False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if infos is None:\n",
    "            if isinstance(info_paths, list):\n",
    "                self.infos = list()\n",
    "                for info_path in info_paths:\n",
    "                    self.infos.extend(mmcv.load(info_path))\n",
    "            else:\n",
    "                self.infos = mmcv.load(info_paths)\n",
    "        else:\n",
    "            self.infos = infos\n",
    "        \n",
    "        self.is_train = is_train\n",
    "        self.ida_aug_conf = ida_aug_conf\n",
    "        self.bda_aug_conf = bda_aug_conf\n",
    "        self.data_root = data_root\n",
    "        self.classes = classes\n",
    "        self.use_cbgs = use_cbgs\n",
    "        if self.use_cbgs:\n",
    "            self.cat2id = {name: i for i, name in enumerate(self.classes)}\n",
    "            self.sample_indices = self._get_sample_indices()\n",
    "        self.num_sweeps = num_sweeps\n",
    "        self.img_mean = np.array(img_conf['img_mean'], np.float32)\n",
    "        self.img_std = np.array(img_conf['img_std'], np.float32)\n",
    "        self.to_rgb = img_conf['to_rgb']\n",
    "        self.return_depth = return_depth\n",
    "        assert sum([sweep_idx >= 0 for sweep_idx in sweep_idxes]) \\\n",
    "            == len(sweep_idxes), 'All `sweep_idxes` must greater \\\n",
    "                than or equal to 0.'\n",
    "\n",
    "        self.sweeps_idx = sweep_idxes\n",
    "        assert sum([key_idx < 0 for key_idx in key_idxes]) == len(key_idxes),\\\n",
    "            'All `key_idxes` must less than 0.'\n",
    "        self.key_idxes = [0] + key_idxes\n",
    "        self.use_fusion = use_fusion\n",
    "        \n",
    "        # Dataset customization\n",
    "        self.tangent_intrinsics = tangent_intrinsics\n",
    "        self.sensor2ego_rot_eulers = sensor2ego_rot_eulers\n",
    "        self.sensor2ego_trans = sensor2ego_trans\n",
    "        self.ego2global_rotation = ego2global_rotation\n",
    "        self.ego2global_translation = ego2global_translation\n",
    "\n",
    "    def _get_sample_indices(self):\n",
    "        \"\"\"Load annotations from ann_file.\n",
    "\n",
    "        Args:\n",
    "            ann_file (str): Path of the annotation file.\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: List of annotations after class sampling.\n",
    "        \"\"\"\n",
    "        class_sample_idxs = {cat_id: [] for cat_id in self.cat2id.values()}\n",
    "        for idx, info in enumerate(self.infos):\n",
    "            gt_names = set(\n",
    "                [ann_info['category_name'] for ann_info in info['ann_infos']])\n",
    "            for gt_name in gt_names:\n",
    "                gt_name = map_name_from_general_to_detection[gt_name]\n",
    "                if gt_name not in self.classes:\n",
    "                    continue\n",
    "                class_sample_idxs[self.cat2id[gt_name]].append(idx)\n",
    "        duplicated_samples = sum(\n",
    "            [len(v) for _, v in class_sample_idxs.items()])\n",
    "        class_distribution = {\n",
    "            k: len(v) / duplicated_samples\n",
    "            for k, v in class_sample_idxs.items()\n",
    "        }\n",
    "\n",
    "        sample_indices = []\n",
    "\n",
    "        frac = 1.0 / len(self.classes)\n",
    "        ratios = [frac / v for v in class_distribution.values()]\n",
    "        for cls_inds, ratio in zip(list(class_sample_idxs.values()), ratios):\n",
    "            sample_indices += np.random.choice(cls_inds,\n",
    "                                               int(len(cls_inds) *\n",
    "                                                   ratio)).tolist()\n",
    "        return sample_indices\n",
    "\n",
    "    def sample_ida_augmentation(self):\n",
    "        \"\"\"Generate ida augmentation values based on ida_config.\"\"\"\n",
    "        H, W = self.ida_aug_conf['H'], self.ida_aug_conf['W']\n",
    "        fH, fW = self.ida_aug_conf['final_dim']\n",
    "        if self.is_train:\n",
    "            resize = np.random.uniform(*self.ida_aug_conf['resize_lim'])\n",
    "            resize_dims = (int(W * resize), int(H * resize))\n",
    "            newW, newH = resize_dims\n",
    "            crop_h = int(\n",
    "                (1 - np.random.uniform(*self.ida_aug_conf['bot_pct_lim'])) *\n",
    "                newH) - fH\n",
    "            crop_w = int(np.random.uniform(0, max(0, newW - fW)))\n",
    "            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n",
    "            flip = False\n",
    "            if self.ida_aug_conf['rand_flip'] and np.random.choice([0, 1]):\n",
    "                flip = True\n",
    "            rotate_ida = np.random.uniform(*self.ida_aug_conf['rot_lim'])\n",
    "        else:\n",
    "            resize = max(fH / H, fW / W)\n",
    "            resize_dims = (int(W * resize), int(H * resize))\n",
    "            newW, newH = resize_dims\n",
    "            crop_h = int(\n",
    "                (1 - np.mean(self.ida_aug_conf['bot_pct_lim'])) * newH) - fH\n",
    "            crop_w = int(max(0, newW - fW) / 2)\n",
    "            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n",
    "            flip = False\n",
    "            rotate_ida = 0\n",
    "        return resize, resize_dims, crop, flip, rotate_ida\n",
    "\n",
    "    def sample_bda_augmentation(self):\n",
    "        \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n",
    "        if self.is_train:\n",
    "            rotate_bda = np.random.uniform(*self.bda_aug_conf['rot_lim'])\n",
    "            scale_bda = np.random.uniform(*self.bda_aug_conf['scale_lim'])\n",
    "            flip_dx = np.random.uniform() < self.bda_aug_conf['flip_dx_ratio']\n",
    "            flip_dy = np.random.uniform() < self.bda_aug_conf['flip_dy_ratio']\n",
    "        else:\n",
    "            rotate_bda = 0\n",
    "            scale_bda = 1.0\n",
    "            flip_dx = False\n",
    "            flip_dy = False\n",
    "        return rotate_bda, scale_bda, flip_dx, flip_dy\n",
    "\n",
    "    def get_lidar_depth(self, lidar_points, img, lidar_info, cam_info):\n",
    "        lidar_calibrated_sensor = lidar_info['LIDAR_TOP']['calibrated_sensor']\n",
    "        lidar_ego_pose = lidar_info['LIDAR_TOP']['ego_pose']\n",
    "        cam_calibrated_sensor = cam_info['calibrated_sensor']\n",
    "        cam_ego_pose = cam_info['ego_pose']\n",
    "        pts_img, depth = map_pointcloud_to_image(\n",
    "            lidar_points.copy(), img, lidar_calibrated_sensor.copy(),\n",
    "            lidar_ego_pose.copy(), cam_calibrated_sensor, cam_ego_pose)\n",
    "        return np.concatenate([pts_img[:2, :].T, depth[:, None]],\n",
    "                              axis=1).astype(np.float32)\n",
    "\n",
    "    def get_image(self, cam_infos, cams, lidar_infos=None):\n",
    "        \"\"\"Given data and cam_names, return image data needed.\n",
    "\n",
    "        Args:\n",
    "            sweeps_data (list): Raw data used to generate the data we needed.\n",
    "            cams (list): Camera names.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Image data after processing.\n",
    "            Tensor: Transformation matrix from camera to ego.\n",
    "            Tensor: Intrinsic matrix.\n",
    "            Tensor: Transformation matrix for ida.\n",
    "            Tensor: Transformation matrix from key\n",
    "                frame camera to sweep frame camera.\n",
    "            Tensor: timestamps.\n",
    "            dict: meta infos needed for evaluation.\n",
    "        \"\"\"\n",
    "        assert len(cam_infos) > 0\n",
    "        sweep_imgs = list()\n",
    "        \n",
    "        # for pose network\n",
    "        pose_input_imgs = list()\n",
    "        \n",
    "        sweep_sensor2ego_mats = list()\n",
    "        sweep_intrin_mats = list()\n",
    "        sweep_ida_mats = list()\n",
    "        sweep_sensor2sensor_mats = list()\n",
    "        sweep_timestamps = list()\n",
    "        sweep_lidar_depth = list()\n",
    "        if self.return_depth or self.use_fusion:\n",
    "            sweep_lidar_points = list()\n",
    "            for lidar_info in lidar_infos:\n",
    "                lidar_path = lidar_info['LIDAR_TOP']['filename']\n",
    "                lidar_points = np.fromfile(os.path.join(\n",
    "                    self.data_root, lidar_path),\n",
    "                                           dtype=np.float32,\n",
    "                                           count=-1).reshape(-1, 5)[..., :4]\n",
    "                sweep_lidar_points.append(lidar_points)\n",
    "        for cam in cams:\n",
    "            imgs = list()\n",
    "            \n",
    "            # for pose network\n",
    "            campose_imgs = list()\n",
    "            \n",
    "            sensor2ego_mats = list()\n",
    "            intrin_mats = list()\n",
    "            ida_mats = list()\n",
    "            sensor2sensor_mats = list()\n",
    "            timestamps = list()\n",
    "            lidar_depth = list()\n",
    "            key_info = cam_infos[0]\n",
    "            resize, resize_dims, crop, flip, rotate_ida = self.sample_ida_augmentation()\n",
    "            for sweep_idx, cam_info in enumerate(cam_infos): # key_idxes [0, -1] 즉 현재 frmae과 직전 keyframe에 대해 enum\n",
    "\n",
    "                img = Image.open(os.path.join(self.data_root, cam_info[cam]['filename']))\n",
    "                # img = Image.fromarray(img)\n",
    "                \n",
    "                # sweep sensor to sweep ego\n",
    "                \"\"\"변경. sensor2ego \"\"\"\n",
    "                if self.sensor2ego_rot_eulers is None:\n",
    "                    w, x, y, z = cam_info[cam]['calibrated_sensor']['rotation'] # 기존\n",
    "                else:\n",
    "                    sensor2ego_degrees = self.sensor2ego_rot_eulers[cam]\n",
    "                    sensor2ego_radians = [degree * np.pi / 180 for degree in sensor2ego_degrees]\n",
    "                    sensor2ego_q = Quaternion(get_quaternion_from_euler(sensor2ego_radians))\n",
    "                    w, x, y, z = sensor2ego_q\n",
    "                    \n",
    "                sweepsensor2sweepego_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n",
    "                \n",
    "                if self.sensor2ego_trans is None:\n",
    "                    sweepsensor2sweepego_tran = torch.Tensor(cam_info[cam]['calibrated_sensor']['translation'])\n",
    "                else:\n",
    "                    sweepsensor2sweepego_tran = torch.Tensor(self.sensor2ego_trans) # 변경\n",
    "                \n",
    "                sweepsensor2sweepego = sweepsensor2sweepego_rot.new_zeros((4, 4))\n",
    "                sweepsensor2sweepego[3, 3] = 1\n",
    "                sweepsensor2sweepego[:3, :3] = sweepsensor2sweepego_rot\n",
    "                sweepsensor2sweepego[:3, -1] = sweepsensor2sweepego_tran\n",
    "                \n",
    "                # sweep ego to global\n",
    "                w, x, y, z = cam_info[cam]['ego_pose']['rotation']\n",
    "                sweepego2global_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n",
    "                sweepego2global_tran = torch.Tensor(cam_info[cam]['ego_pose']['translation'])\n",
    "                sweepego2global = sweepego2global_rot.new_zeros((4, 4))\n",
    "                sweepego2global[3, 3] = 1\n",
    "                sweepego2global[:3, :3] = sweepego2global_rot\n",
    "                sweepego2global[:3, -1] = sweepego2global_tran\n",
    "\n",
    "                # global sensor to cur ego\n",
    "                w, x, y, z = key_info[cam]['ego_pose']['rotation']\n",
    "                keyego2global_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n",
    "                keyego2global_tran = torch.Tensor(key_info[cam]['ego_pose']['translation'])\n",
    "                keyego2global = keyego2global_rot.new_zeros((4, 4))\n",
    "                keyego2global[3, 3] = 1\n",
    "                keyego2global[:3, :3] = keyego2global_rot\n",
    "                keyego2global[:3, -1] = keyego2global_tran\n",
    "                global2keyego = keyego2global.inverse()\n",
    "\n",
    "                # cur ego to sensor\n",
    "                \"\"\"변경. sensor2ego \"\"\"\n",
    "                if self.sensor2ego_rot_eulers is None:\n",
    "                    w, x, y, z = key_info[cam]['calibrated_sensor']['rotation'] # 기존\n",
    "                else:\n",
    "                    sensor2ego_degrees = self.sensor2ego_rot_eulers[cam]\n",
    "                    sensor2ego_radians = [degree * np.pi / 180 for degree in sensor2ego_degrees]\n",
    "                    sensor2ego_q = Quaternion(get_quaternion_from_euler(sensor2ego_radians))\n",
    "                    w, x, y, z = sensor2ego_q\n",
    "                keysensor2keyego_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n",
    "                \n",
    "                if self.sensor2ego_trans is None:\n",
    "                    keysensor2keyego_tran = torch.Tensor(key_info[cam]['calibrated_sensor']['translation'])\n",
    "                else:\n",
    "                    keysensor2keyego_tran = torch.Tensor(self.sensor2ego_trans) # 변경\n",
    "                \n",
    "                keysensor2keyego = keysensor2keyego_rot.new_zeros((4, 4))\n",
    "                keysensor2keyego[3, 3] = 1\n",
    "                keysensor2keyego[:3, :3] = keysensor2keyego_rot\n",
    "                keysensor2keyego[:3, -1] = keysensor2keyego_tran\n",
    "                keyego2keysensor = keysensor2keyego.inverse()\n",
    "                \n",
    "                \"\"\" final value (matrix) \"\"\"\n",
    "                sweepsensor2keyego = global2keyego @ sweepego2global @ sweepsensor2sweepego # sensor2ego\n",
    "                keysensor2sweepsensor = (keyego2keysensor @ sweepsensor2keyego).inverse() # sensor2sensor\n",
    "                # keysensor2sweepsensor = (keyego2keysensor @ global2keyego @ sweepego2global @ sweepsensor2sweepego).inverse()\n",
    "                \n",
    "                sensor2ego_mats.append(sweepsensor2keyego)\n",
    "                sensor2sensor_mats.append(keysensor2sweepsensor)\n",
    "                \n",
    "                \"\"\" 변경. intrinsics \"\"\"\n",
    "                intrin_mat = torch.zeros((4, 4))\n",
    "                intrin_mat[3, 3] = 1\n",
    "                if self.tangent_intrinsics is None:\n",
    "                    intrin_mat[:3, :3] = torch.Tensor(cam_info[cam]['calibrated_sensor']['camera_intrinsic'])\n",
    "                else:\n",
    "                    intrin_mat[:3, :3] = torch.Tensor(self.tangent_intrinsics[cam])\n",
    "                \n",
    "                if self.return_depth and (self.use_fusion or sweep_idx == 0):\n",
    "                    point_depth = self.get_lidar_depth(\n",
    "                        sweep_lidar_points[sweep_idx], img,\n",
    "                        lidar_infos[sweep_idx], cam_info[cam])\n",
    "                    point_depth_augmented = depth_transform(\n",
    "                        point_depth, resize, self.ida_aug_conf['final_dim'],\n",
    "                        crop, flip, rotate_ida)\n",
    "                    lidar_depth.append(point_depth_augmented)\n",
    "                \n",
    "                # for pose network\n",
    "                original_width, original_height = img.size\n",
    "                \"\"\"TEMP 임시로 resize_width 및 height 여기에서 정의\"\"\"\n",
    "                pose_input_img = img.resize((640, 192), Image.Resampling.LANCZOS)\n",
    "                pose_input_img = transforms.ToTensor()(pose_input_img).unsqueeze(0)\n",
    "                \n",
    "                # print(pose_input_img.shape)\n",
    "                \n",
    "                campose_imgs.append(pose_input_img)\n",
    "                \n",
    "                # for 3D object detection network\n",
    "                img, ida_mat = img_transform(\n",
    "                    img,\n",
    "                    resize=resize,\n",
    "                    resize_dims=resize_dims,\n",
    "                    crop=crop,\n",
    "                    flip=flip,\n",
    "                    rotate=rotate_ida,\n",
    "                )\n",
    "                \n",
    "                ida_mats.append(ida_mat)\n",
    "                img = mmcv.imnormalize(np.array(img), self.img_mean, self.img_std, self.to_rgb)\n",
    "                img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "                imgs.append(img)\n",
    "                intrin_mats.append(intrin_mat)\n",
    "                timestamps.append(cam_info[cam]['timestamp'])\n",
    "            \n",
    "            # for pose network: total cams' pose network inputs\n",
    "            pose_input_imgs.append(torch.stack(campose_imgs))\n",
    "            \n",
    "            sweep_imgs.append(torch.stack(imgs))\n",
    "            sweep_sensor2ego_mats.append(torch.stack(sensor2ego_mats))\n",
    "            sweep_intrin_mats.append(torch.stack(intrin_mats))\n",
    "            sweep_ida_mats.append(torch.stack(ida_mats))\n",
    "            sweep_sensor2sensor_mats.append(torch.stack(sensor2sensor_mats))\n",
    "            sweep_timestamps.append(torch.tensor(timestamps))\n",
    "            if self.return_depth:\n",
    "                sweep_lidar_depth.append(torch.stack(lidar_depth))\n",
    "                \n",
    "        \"\"\" ego pose 변경 \"\"\"\n",
    "        if self.ego2global_rotation is None and self.ego2global_translation is None:\n",
    "            # Get mean pose of all cams\n",
    "            ego2global_rotation = np.mean([key_info[cam]['ego_pose']['rotation'] for cam in cams], 0)\n",
    "            ego2global_translation = np.mean([key_info[cam]['ego_pose']['translation'] for cam in cams], 0)\n",
    "        else:\n",
    "            ego2global_rotation = self.ego2global_rotation\n",
    "            ego2global_translation = self.ego2global_translation\n",
    "        \n",
    "        img_metas = dict(\n",
    "            box_type_3d=LiDARInstance3DBoxes,\n",
    "            ego2global_translation=ego2global_translation,\n",
    "            ego2global_rotation=ego2global_rotation,\n",
    "        )\n",
    "\n",
    "        ret_list = [\n",
    "            torch.stack(sweep_imgs).permute(1, 0, 2, 3, 4),\n",
    "            torch.stack(sweep_sensor2ego_mats).permute(1, 0, 2, 3),\n",
    "            torch.stack(sweep_intrin_mats).permute(1, 0, 2, 3),\n",
    "            torch.stack(sweep_ida_mats).permute(1, 0, 2, 3),\n",
    "            torch.stack(sweep_sensor2sensor_mats).permute(1, 0, 2, 3),\n",
    "            torch.stack(sweep_timestamps).permute(1, 0),\n",
    "            img_metas,\n",
    "            \n",
    "            # for pose net\n",
    "            pose_input_imgs\n",
    "        ]\n",
    "        if self.return_depth:\n",
    "            ret_list.append(torch.stack(sweep_lidar_depth).permute(1, 0, 2, 3))\n",
    "        return ret_list\n",
    "\n",
    "    def get_gt(self, info, cams):\n",
    "        \"\"\"Generate gt labels from info.\n",
    "\n",
    "        Args:\n",
    "            info(dict): Infos needed to generate gt labels.\n",
    "            cams(list): Camera names.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: GT bboxes.\n",
    "            Tensor: GT labels.\n",
    "        \"\"\"\n",
    "        ego2global_rotation = np.mean(\n",
    "            [info['cam_infos'][cam]['ego_pose']['rotation'] for cam in cams],\n",
    "            0)\n",
    "        ego2global_translation = np.mean([\n",
    "            info['cam_infos'][cam]['ego_pose']['translation'] for cam in cams\n",
    "        ], 0)\n",
    "        trans = -np.array(ego2global_translation)\n",
    "        rot = Quaternion(ego2global_rotation).inverse\n",
    "        gt_boxes = list()\n",
    "        gt_labels = list()\n",
    "        for ann_info in info['ann_infos']:\n",
    "            # Use ego coordinate.\n",
    "            if (map_name_from_general_to_detection[ann_info['category_name']]\n",
    "                    not in self.classes\n",
    "                    or ann_info['num_lidar_pts'] + ann_info['num_radar_pts'] <=\n",
    "                    0):\n",
    "                continue\n",
    "            box = Box(\n",
    "                ann_info['translation'],\n",
    "                ann_info['size'],\n",
    "                Quaternion(ann_info['rotation']),\n",
    "                velocity=ann_info['velocity'],\n",
    "            )\n",
    "            box.translate(trans)\n",
    "            box.rotate(rot)\n",
    "            box_xyz = np.array(box.center)\n",
    "            box_dxdydz = np.array(box.wlh)[[1, 0, 2]]\n",
    "            box_yaw = np.array([box.orientation.yaw_pitch_roll[0]])\n",
    "            box_velo = np.array(box.velocity[:2])\n",
    "            gt_box = np.concatenate([box_xyz, box_dxdydz, box_yaw, box_velo])\n",
    "            gt_boxes.append(gt_box)\n",
    "            gt_labels.append(\n",
    "                self.classes.index(map_name_from_general_to_detection[\n",
    "                    ann_info['category_name']]))\n",
    "        return torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n",
    "\n",
    "    def choose_cams(self):\n",
    "        \"\"\"Choose cameras randomly.\n",
    "\n",
    "        Returns:\n",
    "            list: Cameras to be used.\n",
    "        \"\"\"\n",
    "        if self.is_train and self.ida_aug_conf['Ncams'] < len(\n",
    "                self.ida_aug_conf['cams']):\n",
    "            cams = np.random.choice(self.ida_aug_conf['cams'],\n",
    "                                    self.ida_aug_conf['Ncams'],\n",
    "                                    replace=False)\n",
    "        else:\n",
    "            cams = self.ida_aug_conf['cams']\n",
    "        return cams\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"__getitem__, idx: \", idx)\n",
    "        if self.use_cbgs:\n",
    "            idx = self.sample_indices[idx]\n",
    "        cam_infos = list()\n",
    "        lidar_infos = list()\n",
    "        # TODO: Check if it still works when number of cameras is reduced.\n",
    "        cams = self.choose_cams()\n",
    "        for key_idx in self.key_idxes:\n",
    "            cur_idx = key_idx + idx\n",
    "            # Handle scenarios when current idx doesn't have previous key\n",
    "            # frame or previous key frame is from another scene.\n",
    "            if cur_idx < 0:\n",
    "                cur_idx = idx\n",
    "            elif self.infos[cur_idx]['scene_token'] != self.infos[idx][\n",
    "                    'scene_token']:\n",
    "                cur_idx = idx\n",
    "            info = self.infos[cur_idx]\n",
    "            cam_infos.append(info['cam_infos'])\n",
    "            lidar_infos.append(info['lidar_infos'])\n",
    "            lidar_sweep_timestamps = [\n",
    "                lidar_sweep['LIDAR_TOP']['timestamp']\n",
    "                for lidar_sweep in info['lidar_sweeps']\n",
    "            ]\n",
    "            # print(\"push cam_info of key_idx \", str(key_idx))\n",
    "            \n",
    "            for sweep_idx in self.sweeps_idx:\n",
    "                # print(\"push sweep_info of sweep_idx \", str(sweep_idx))\n",
    "                if len(info['cam_sweeps']) == 0:\n",
    "                    cam_infos.append(info['cam_infos'])\n",
    "                    lidar_infos.append(info['lidar_infos'])\n",
    "                else:\n",
    "                    # Handle scenarios when current sweep doesn't have all\n",
    "                    # cam keys.\n",
    "                    for i in range(min(len(info['cam_sweeps']) - 1, sweep_idx),\n",
    "                                   -1, -1):\n",
    "                        if sum([cam in info['cam_sweeps'][i]\n",
    "                                for cam in cams]) == len(cams):\n",
    "                            cam_infos.append(info['cam_sweeps'][i])\n",
    "                            cam_timestamp = np.mean([\n",
    "                                val['timestamp']\n",
    "                                for val in info['cam_sweeps'][i].values()\n",
    "                            ])\n",
    "                            # Find the closest lidar frame to the cam frame.\n",
    "                            lidar_idx = np.abs(lidar_sweep_timestamps -\n",
    "                                               cam_timestamp).argmin()\n",
    "                            lidar_infos.append(info['lidar_sweeps'][lidar_idx])\n",
    "                            break\n",
    "        if self.return_depth or self.use_fusion:\n",
    "            image_data_list = self.get_image(cam_infos, cams, lidar_infos)\n",
    "        else:\n",
    "            image_data_list = self.get_image(cam_infos, cams)    \n",
    "        ret_list = list()\n",
    "        (\n",
    "            sweep_imgs,\n",
    "            sweep_sensor2ego_mats,\n",
    "            sweep_intrins,\n",
    "            sweep_ida_mats,\n",
    "            sweep_sensor2sensor_mats,\n",
    "            sweep_timestamps,\n",
    "            img_metas,\n",
    "            \n",
    "            # for pose network\n",
    "            pose_input_imgs\n",
    "        ) = image_data_list[:8]\n",
    "        \n",
    "        img_metas['token'] = self.infos[idx]['sample_token']\n",
    "        if self.is_train:\n",
    "            gt_boxes, gt_labels = self.get_gt(self.infos[idx], cams)\n",
    "        # Temporary solution for test.\n",
    "        else:\n",
    "            gt_boxes = sweep_imgs.new_zeros(0, 7)\n",
    "            gt_labels = sweep_imgs.new_zeros(0, )\n",
    "\n",
    "        rotate_bda, scale_bda, flip_dx, flip_dy = self.sample_bda_augmentation()\n",
    "        bda_mat = sweep_imgs.new_zeros(4, 4)\n",
    "        bda_mat[3, 3] = 1\n",
    "        gt_boxes, bda_rot = bev_transform(gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy)\n",
    "        bda_mat[:3, :3] = bda_rot\n",
    "        ret_list = [\n",
    "            sweep_imgs,\n",
    "            sweep_sensor2ego_mats,\n",
    "            sweep_intrins,\n",
    "            sweep_ida_mats,\n",
    "            sweep_sensor2sensor_mats,\n",
    "            bda_mat,\n",
    "            sweep_timestamps,\n",
    "            img_metas,\n",
    "            gt_boxes,\n",
    "            gt_labels,\n",
    "            \n",
    "            # for pose network\n",
    "            pose_input_imgs\n",
    "        ]\n",
    "        if self.return_depth:\n",
    "            ret_list.append(image_data_list[7])\n",
    "        return ret_list\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"NuscData: {len(self)} samples. Split: \\\n",
    "            {\"train\" if self.is_train else \"val\"}.\n",
    "                    Augmentation Conf: {self.ida_aug_conf}\"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.use_cbgs:\n",
    "            return len(self.sample_indices)\n",
    "        else:\n",
    "            return len(self.infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c465c-6c43-4419-b4aa-4ede29959743",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45066a59-2ee7-4584-a648-5aef4e3f0e90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_632208/721307558.py:253: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  torch.LongTensor([(row[1] - row[0]) / row[2]\n",
      "2023-01-15 21:09:50,401 - mmcv - INFO - initialize SECONDFPN with init_cfg [{'type': 'Kaiming', 'layer': 'ConvTranspose2d'}, {'type': 'Constant', 'layer': 'NaiveSyncBatchNorm2d', 'val': 1.0}]\n",
      "2023-01-15 21:09:50,413 - mmcv - INFO - \n",
      "deblocks.0.0.weight - torch.Size([128, 256, 4, 4]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:50,414 - mmcv - INFO - \n",
      "deblocks.0.1.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:50,414 - mmcv - INFO - \n",
      "deblocks.0.1.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:50,415 - mmcv - INFO - \n",
      "deblocks.1.0.weight - torch.Size([128, 512, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:50,416 - mmcv - INFO - \n",
      "deblocks.1.1.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:50,416 - mmcv - INFO - \n",
      "deblocks.1.1.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:50,417 - mmcv - INFO - \n",
      "deblocks.2.0.weight - torch.Size([1024, 128, 1, 1]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:50,418 - mmcv - INFO - \n",
      "deblocks.2.1.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:50,418 - mmcv - INFO - \n",
      "deblocks.2.1.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:50,418 - mmcv - INFO - \n",
      "deblocks.3.0.weight - torch.Size([2048, 128, 2, 2]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:50,419 - mmcv - INFO - \n",
      "deblocks.3.1.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:50,421 - mmcv - INFO - \n",
      "deblocks.3.1.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:50,430 - mmcv - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'}\n",
      "2023-01-15 21:09:50,431 - mmcv - INFO - load model from: torchvision://resnet50\n",
      "2023-01-15 21:09:50,432 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet50\n",
      "2023-01-15 21:09:50,542 - mmcv - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n",
      "2023-01-15 21:09:50,826 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]\n",
      "2023-01-15 21:09:51,026 - mmcv - INFO - initialize BasicBlock with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
      "2023-01-15 21:09:51,028 - mmcv - INFO - initialize BasicBlock with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
      "2023-01-15 21:09:51,031 - mmcv - INFO - initialize BasicBlock with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
      "2023-01-15 21:09:51,034 - mmcv - INFO - initialize BasicBlock with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
      "2023-01-15 21:09:51,037 - mmcv - INFO - initialize BasicBlock with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
      "2023-01-15 21:09:51,039 - mmcv - INFO - initialize BasicBlock with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
      "2023-01-15 21:09:51,043 - mmcv - INFO - \n",
      "conv1.weight - torch.Size([160, 160, 7, 7]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,044 - mmcv - INFO - \n",
      "bn1.weight - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,045 - mmcv - INFO - \n",
      "bn1.bias - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,046 - mmcv - INFO - \n",
      "layer1.0.conv1.weight - torch.Size([160, 160, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,046 - mmcv - INFO - \n",
      "layer1.0.bn1.weight - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,047 - mmcv - INFO - \n",
      "layer1.0.bn1.bias - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,047 - mmcv - INFO - \n",
      "layer1.0.conv2.weight - torch.Size([160, 160, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,048 - mmcv - INFO - \n",
      "layer1.0.bn2.weight - torch.Size([160]): \n",
      "ConstantInit: val=0, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,049 - mmcv - INFO - \n",
      "layer1.0.bn2.bias - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,049 - mmcv - INFO - \n",
      "layer1.1.conv1.weight - torch.Size([160, 160, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,050 - mmcv - INFO - \n",
      "layer1.1.bn1.weight - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,051 - mmcv - INFO - \n",
      "layer1.1.bn1.bias - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,051 - mmcv - INFO - \n",
      "layer1.1.conv2.weight - torch.Size([160, 160, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,052 - mmcv - INFO - \n",
      "layer1.1.bn2.weight - torch.Size([160]): \n",
      "ConstantInit: val=0, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,053 - mmcv - INFO - \n",
      "layer1.1.bn2.bias - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,053 - mmcv - INFO - \n",
      "layer2.0.conv1.weight - torch.Size([320, 160, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,054 - mmcv - INFO - \n",
      "layer2.0.bn1.weight - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,054 - mmcv - INFO - \n",
      "layer2.0.bn1.bias - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,055 - mmcv - INFO - \n",
      "layer2.0.conv2.weight - torch.Size([320, 320, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,055 - mmcv - INFO - \n",
      "layer2.0.bn2.weight - torch.Size([320]): \n",
      "ConstantInit: val=0, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,056 - mmcv - INFO - \n",
      "layer2.0.bn2.bias - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,057 - mmcv - INFO - \n",
      "layer2.0.downsample.0.weight - torch.Size([320, 160, 1, 1]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,057 - mmcv - INFO - \n",
      "layer2.0.downsample.1.weight - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,058 - mmcv - INFO - \n",
      "layer2.0.downsample.1.bias - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,058 - mmcv - INFO - \n",
      "layer2.1.conv1.weight - torch.Size([320, 320, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,059 - mmcv - INFO - \n",
      "layer2.1.bn1.weight - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,060 - mmcv - INFO - \n",
      "layer2.1.bn1.bias - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,060 - mmcv - INFO - \n",
      "layer2.1.conv2.weight - torch.Size([320, 320, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,061 - mmcv - INFO - \n",
      "layer2.1.bn2.weight - torch.Size([320]): \n",
      "ConstantInit: val=0, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,061 - mmcv - INFO - \n",
      "layer2.1.bn2.bias - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,062 - mmcv - INFO - \n",
      "layer3.0.conv1.weight - torch.Size([640, 320, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,062 - mmcv - INFO - \n",
      "layer3.0.bn1.weight - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,062 - mmcv - INFO - \n",
      "layer3.0.bn1.bias - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,063 - mmcv - INFO - \n",
      "layer3.0.conv2.weight - torch.Size([640, 640, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,063 - mmcv - INFO - \n",
      "layer3.0.bn2.weight - torch.Size([640]): \n",
      "ConstantInit: val=0, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,063 - mmcv - INFO - \n",
      "layer3.0.bn2.bias - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,064 - mmcv - INFO - \n",
      "layer3.0.downsample.0.weight - torch.Size([640, 320, 1, 1]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,064 - mmcv - INFO - \n",
      "layer3.0.downsample.1.weight - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,069 - mmcv - INFO - \n",
      "layer3.0.downsample.1.bias - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,069 - mmcv - INFO - \n",
      "layer3.1.conv1.weight - torch.Size([640, 640, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,070 - mmcv - INFO - \n",
      "layer3.1.bn1.weight - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,070 - mmcv - INFO - \n",
      "layer3.1.bn1.bias - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,070 - mmcv - INFO - \n",
      "layer3.1.conv2.weight - torch.Size([640, 640, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,071 - mmcv - INFO - \n",
      "layer3.1.bn2.weight - torch.Size([640]): \n",
      "ConstantInit: val=0, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,071 - mmcv - INFO - \n",
      "layer3.1.bn2.bias - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-15 21:09:51,112 - mmcv - INFO - initialize SECONDFPN with init_cfg [{'type': 'Kaiming', 'layer': 'ConvTranspose2d'}, {'type': 'Constant', 'layer': 'NaiveSyncBatchNorm2d', 'val': 1.0}]\n",
      "2023-01-15 21:09:51,145 - mmcv - INFO - \n",
      "deblocks.0.0.weight - torch.Size([160, 64, 1, 1]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,146 - mmcv - INFO - \n",
      "deblocks.0.1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:51,147 - mmcv - INFO - \n",
      "deblocks.0.1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:51,147 - mmcv - INFO - \n",
      "deblocks.1.0.weight - torch.Size([160, 64, 2, 2]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,148 - mmcv - INFO - \n",
      "deblocks.1.1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:51,148 - mmcv - INFO - \n",
      "deblocks.1.1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:51,151 - mmcv - INFO - \n",
      "deblocks.2.0.weight - torch.Size([320, 64, 4, 4]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,152 - mmcv - INFO - \n",
      "deblocks.2.1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:51,152 - mmcv - INFO - \n",
      "deblocks.2.1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:51,153 - mmcv - INFO - \n",
      "deblocks.3.0.weight - torch.Size([640, 64, 8, 8]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-15 21:09:51,153 - mmcv - INFO - \n",
      "deblocks.3.1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-15 21:09:51,154 - mmcv - INFO - \n",
      "deblocks.3.1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEVStereo and PoseNet loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jeholee/anaconda3/envs/omnicv/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "# device = 'cuda:2'\n",
    "# device = 'cpu'\n",
    "device = 'cuda:0'\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "\"\"\" 1. Init BEVStereo model and hyper-params \"\"\"\n",
    "exp_name = 'bevstereo_ema_da_key2'\n",
    "ckpt_path = './ckpt/bev_stereo_lss_r50_256x704_128x128_20e_cbgs_2key_da.pth'\n",
    "use_ema = False\n",
    "data_root_dir = '../data/nuscenes/'\n",
    "default_root_dir = os.path.join('./outputs/', exp_name)\n",
    "\n",
    "gpus = 1\n",
    "eval_interval = 1\n",
    "batch_size_per_device = 1\n",
    "basic_lr_per_img = 2e-4 / 64\n",
    "mmcv.mkdir_or_exist(default_root_dir)\n",
    "evaluator = DetNuscEvaluator(class_names=CLASSES, output_dir=default_root_dir)\n",
    "mode = 'valid'\n",
    "data_use_cbgs = False\n",
    "\n",
    "\"\"\"\n",
    "기본적으로 key_idxes [0, -1]만 사용하여 stereo matching을 하는 듯\n",
    "sweep_idxes는 계속 빈 리스트로 존재하는 반면, key_idxes는 [0, -1]이 됨\n",
    "코드 분석할 때 일단은 key_idxes만 보면 될 듯\n",
    "\"\"\"\n",
    "num_sweeps = 1\n",
    "sweep_idxes = list()\n",
    "key_idxes = list()\n",
    "\n",
    "data_return_depth = True\n",
    "downsample_factor = backbone_conf['downsample_factor']\n",
    "dbound = backbone_conf['d_bound']\n",
    "depth_channels = int((dbound[1] - dbound[0]) / dbound[2])\n",
    "use_fusion = False\n",
    "\n",
    "train_info_paths = os.path.join(data_root_dir, 'nuscenes_infos_train.pkl')\n",
    "val_info_paths = os.path.join(data_root_dir, 'nuscenes_infos_val.pkl')\n",
    "# predict_info_paths = os.path.join(data_root_dir, 'nuscenes_infos_test.pkl')\n",
    "predict_info_paths = os.path.join(data_root_dir, 'nuscenes_infos_val.pkl')\n",
    "\n",
    "\"\"\" BEVStereo configurations \"\"\"\n",
    "# base ckpt of bevstereo (2 key)\n",
    "key_idxes = [-1]\n",
    "head_conf['bev_backbone_conf']['in_channels'] = 80 * (len(key_idxes) + 1)\n",
    "head_conf['bev_neck_conf']['in_channels'] = [80 * (len(key_idxes) + 1), 160, 320, 640]\n",
    "head_conf['train_cfg']['code_weight'] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "head_conf['test_cfg']['thresh_scale'] = [0.6, 0.4, 0.4, 0.7, 0.8, 0.9]\n",
    "head_conf['test_cfg']['nms_type'] = 'size_aware_circle'\n",
    "\n",
    "# ckpt with data augmentation\n",
    "backbone_conf['use_da'] = True\n",
    "data_use_cbgs = True\n",
    "basic_lr_per_img = 2e-4 / 32\n",
    "\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "model = BEVStereo(backbone_conf, head_conf, is_train_depth=True)\n",
    "\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "model_weights = checkpoint[\"state_dict\"]\n",
    "\n",
    "for key in list(model_weights):\n",
    "    model_weights[key.replace(\"model.\", \"\")] = model_weights.pop(key)\n",
    "model.load_state_dict(model_weights)\n",
    "\n",
    "\"\"\" 2. Init PoseNet \"\"\"\n",
    "manydepth_model_path = './manydepth/ckpt/KITTI_MR/'\n",
    "\n",
    "pose_enc_dict = torch.load(os.path.join(manydepth_model_path, \"pose_encoder.pth\"), map_location=device)\n",
    "pose_dec_dict = torch.load(os.path.join(manydepth_model_path, \"pose.pth\"), map_location=device)\n",
    "\n",
    "pose_enc = ResnetEncoder(18, False, num_input_images=2)\n",
    "pose_dec = PoseDecoder(pose_enc.num_ch_enc, num_input_features=1, num_frames_to_predict_for=2)\n",
    "\n",
    "pose_enc.load_state_dict(pose_enc_dict, strict=True)\n",
    "pose_dec.load_state_dict(pose_dec_dict, strict=True)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "pose_enc.eval()\n",
    "pose_dec.eval()\n",
    "pose_enc.to(device)\n",
    "pose_dec.to(device)\n",
    "print(\"BEVStereo and PoseNet loaded\")\n",
    "\n",
    "\"\"\" 3. Init Tangent Projection Grid \"\"\"\n",
    "num_rows = 1\n",
    "num_cols = [6]\n",
    "phi_centers = [0]\n",
    "\n",
    "# fov 가로/세로 비율과 nuscenes input의 width/height 비율이 같음\n",
    "# 900/1600, 396/704: 1.777777...\n",
    "# 위 비율에 맞춰서 tangent patch size 결정하기\n",
    "tangent_h = 256 # 256 # 900 # 396\n",
    "tangent_w = 704 # 704 # 1600 # 704\n",
    "fov  = [70, 39.375]\n",
    "erp_h, erp_w = 1920, 3840\n",
    "\n",
    "n_patch, grid = createProjectGrid(erp_h, erp_w, tangent_h, tangent_w, num_rows, num_cols, phi_centers, fov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5520349f-a9eb-4606-9471-cd9beb0de1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Init Insta360 ERP Dataset Meta-data \"\"\"\n",
    "\n",
    "scene_dir = \"scene_1/\"\n",
    "erp_img_root = \"../data/daejeon_road_outdoor/erp_images/\"\n",
    "# filename = \"frame_0001.jpg\"\n",
    "# erp_img_path = os.path.join(erp_img_root, scene_dir, filename)\n",
    "\n",
    "erp_imgs = []\n",
    "for filename in os.listdir(erp_img_root + scene_dir):\n",
    "    erp_imgs.append(os.path.join(erp_img_root, scene_dir, filename))\n",
    "    \n",
    "# ida_aug_conf['cams'] = ['CAM_BACK_RIGHT',\n",
    "#                         'CAM_FRONT_LEFT',\n",
    "#                         'CAM_FRONT',\n",
    "#                         'CAM_FRONT_RIGHT',\n",
    "#                         'CAM_BACK_LEFT',\n",
    "#                         'CAM_BACK']\n",
    "\n",
    "ida_aug_conf['cams'] = ['CAM_FRONT_LEFT',\n",
    "                        'CAM_FRONT',\n",
    "                        'CAM_FRONT_RIGHT',\n",
    "                        'CAM_BACK_LEFT',\n",
    "                        'CAM_BACK',\n",
    "                        'CAM_BACK_RIGHT']\n",
    "\n",
    "\"\"\"sensor2ego (calibrated_sensor) rotation degree를 insta360에 맞게 변경\"\"\"\n",
    "# [roll, pitch, yaw]\n",
    "\n",
    "# BEVFormer에서 사용한 sensor2lidar\n",
    "# sensor2ego_rot_eulers = {'CAM_FRONT': [-88.89452145820958, -0.34991764317283675, 0.0],\n",
    "#                          'CAM_FRONT_RIGHT': [-89.73833720738743, 1.5337073678165962, -60.0],\n",
    "#                          'CAM_FRONT_LEFT': [-89.2717432442176, -1.221029004801636, 60.0],\n",
    "#                          'CAM_BACK': [-90.4448247670561, 0.552239989680819, 180.0],\n",
    "#                          'CAM_BACK_LEFT': [-91.67728722938543, -1.4284019448799024, 120.0],\n",
    "#                          'CAM_BACK_RIGHT': [-91.13862482181912, 2.0544464501438036, -120.0]}\n",
    "\n",
    "# BEVFormer에서 사용한 sensor2ego\n",
    "# sensor2ego_rot_eulers = {'CAM_FRONT': [-90.32322642770005, -0.046127194838589326, -90.0],\n",
    "#                          'CAM_FRONT_RIGHT': [-90.7820235885, 0.5188438566959973, -150.0],\n",
    "#                          'CAM_FRONT_LEFT': [-89.85977500319999, 0.12143609391200436, -30.0],\n",
    "#                          'CAM_BACK': [-89.0405962694, 0.22919685786400154, 90.0],\n",
    "#                          'CAM_BACK_LEFT': [-90.91736319750001, -0.21518275753700122, 30.0],\n",
    "#                          'CAM_BACK_RIGHT': [-90.93206677999999, 0.6190947610589966, -210.0]}\n",
    "\n",
    "# sensor2ego_rot_eulers = {'CAM_FRONT': [-90.32322642770005, -0.046127194838589326, -60.0],\n",
    "#                          'CAM_FRONT_RIGHT': [-90.7820235885, 0.5188438566959973, -120.0],\n",
    "#                          'CAM_FRONT_LEFT': [-89.85977500319999, 0.12143609391200436, -0.0],\n",
    "#                          'CAM_BACK': [-89.0405962694, 0.22919685786400154, 120.0],\n",
    "#                          'CAM_BACK_LEFT': [-90.91736319750001, -0.21518275753700122, 60.0],\n",
    "#                          'CAM_BACK_RIGHT': [-90.93206677999999, 0.6190947610589966, -180.0]}\n",
    "\n",
    "sensor2ego_rot_eulers = {'CAM_FRONT': [-90.0, 0.0, 0.0],\n",
    "                         'CAM_FRONT_RIGHT': [-90.0, 0.0, -60.0],\n",
    "                         'CAM_FRONT_LEFT': [-90.0, 0.0, 60.0],\n",
    "                         'CAM_BACK': [-90.0, 0.0, 180.0],\n",
    "                         'CAM_BACK_LEFT': [-90.0, 0.0, 120.0],\n",
    "                         'CAM_BACK_RIGHT': [-90.0, 0.0, -120.0]}\n",
    "\n",
    "# sensor2ego_rot_eulers = {'CAM_FRONT': [-90.0, 0.0, 90.0],\n",
    "#                          'CAM_FRONT_RIGHT': [-90.0, 0.0, 30.0],\n",
    "#                          'CAM_FRONT_LEFT': [-90.0, 0.0, 150.0],\n",
    "#                          'CAM_BACK': [-90.0, 0.0, -90.0],\n",
    "#                          'CAM_BACK_LEFT': [-90.0, 0.0, -150.0],\n",
    "#                          'CAM_BACK_RIGHT': [-90.0, 0.0, -30.0]}\n",
    "\n",
    "# sensor2ego_rots = [] # tcam2egocam_rots\n",
    "# for i in range(len(ida_aug_conf['cams'])):\n",
    "#     cam = ida_aug_conf['cams'][i]\n",
    "#     sensor2ego_degrees = sensor2ego_euler_degrees[cam]\n",
    "#     sensor2ego_radians = [degree * np.pi / 180 for degree in sensor2ego_degrees]\n",
    "#     sensor2ego_q = Quaternion(get_quaternion_from_euler(sensor2ego_radians))\n",
    "#     sensor2ego_r_mat = sensor2ego_q.rotation_matrix\n",
    "#     sensor2ego_rots.append(sensor2ego_r_mat)\n",
    "\n",
    "#     # 확인\n",
    "#     # radians = euler_from_quaternion(sensor2ego_q)\n",
    "#     # print(cam, sensor2ego_q)\n",
    "#     # print(cam, [euler / np.pi * 180 for euler in radians])\n",
    "\n",
    "sensor2ego_trans = [0.0, 0.0, 0.0]\n",
    "\n",
    "ego2global_rotation = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "ego2global_translation = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "# tangent_intrinsics = {'CAM_FRONT': [[1343.45019, 0.0, 820.183159], [0.0, 1280.23476, 442.850375], [0.0, 0.0, 1.0]],\n",
    "#                       'CAM_FRONT_RIGHT': [[1318.58226, 0.0, 748.797979], [0.0, 1307.51676, 433.683573], [0.0, 0.0, 1.0]],\n",
    "#                       'CAM_FRONT_LEFT': [[1326.72632, 0.0, 789.918136], [0.0, 1313.8785, 447.051964], [0.0, 0.0, 1.0]],\n",
    "#                       'CAM_BACK': [[1318.10344, 0.0, 760.164664], [0.0, 1307.00893, 433.459504], [0.0, 0.0, 1.0]],\n",
    "#                       'CAM_BACK_LEFT': [[1329.78802, 0.0, 794.247861], [0.0, 1318.65546, 422.083681], [0.0, 0.0, 1.0]],\n",
    "#                       'CAM_BACK_RIGHT': [[1342.27544, 0.0, 790.251605], [0.0, 1326.28658, 452.853747], [0.0, 0.0, 1.0]]}\n",
    "\n",
    "tangent_intrinsics = {'CAM_FRONT': [[1250.45019, 0.0, 820.183159], [0.0, 1250.23476, 442.850375], [0.0, 0.0, 1.0]],\n",
    "                      'CAM_FRONT_RIGHT': [[1250.58226, 0.0, 748.797979], [0.0, 1250.51676, 433.683573], [0.0, 0.0, 1.0]],\n",
    "                      'CAM_FRONT_LEFT': [[1250.72632, 0.0, 789.918136], [0.0, 1250.8785, 447.051964], [0.0, 0.0, 1.0]],\n",
    "                      'CAM_BACK': [[1250.10344, 0.0, 760.164664], [0.0, 1250.00893, 433.459504], [0.0, 0.0, 1.0]],\n",
    "                      'CAM_BACK_LEFT': [[1250.78802, 0.0, 794.247861], [0.0, 1250.65546, 422.083681], [0.0, 0.0, 1.0]],\n",
    "                      'CAM_BACK_RIGHT': [[1250.27544, 0.0, 790.251605], [0.0, 1250.28658, 452.853747], [0.0, 0.0, 1.0]]}\n",
    "\n",
    "\"\"\" Init Nusc Dataset \"\"\"\n",
    "dataset = NuscDetDataset(ida_aug_conf=ida_aug_conf,\n",
    "                         bda_aug_conf=bda_aug_conf,\n",
    "                         classes=CLASSES,\n",
    "                         data_root=data_root_dir,\n",
    "                         info_paths=predict_info_paths,\n",
    "                         is_train=False,\n",
    "                         \n",
    "                         # Dataset customization\n",
    "                         tangent_intrinsics=tangent_intrinsics,\n",
    "                         sensor2ego_rot_eulers=sensor2ego_rot_eulers,\n",
    "                         sensor2ego_trans=sensor2ego_trans,\n",
    "                         ego2global_rotation=ego2global_rotation,\n",
    "                         ego2global_translation=ego2global_translation,\n",
    "                         \n",
    "                         infos=infos,\n",
    "                         \n",
    "                         img_conf=img_conf,\n",
    "                         num_sweeps=num_sweeps,\n",
    "                         sweep_idxes=sweep_idxes,\n",
    "                         key_idxes=key_idxes,\n",
    "                         return_depth=use_fusion,\n",
    "                         use_fusion=use_fusion)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size_per_device,\n",
    "    shuffle=False,\n",
    "    collate_fn=partial(collate_fn, is_return_depth=use_fusion),\n",
    "    num_workers=4,\n",
    "    sampler=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7ad1355-ca57-4d19-8c78-29317a4709e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1\n",
    "data_iterator = iter(data_loader)\n",
    "data = next(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af4d80-65e5-42ee-9e38-72ee653eafa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterator\n",
    "idx = idx + 1\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66baa614-51a4-4b83-b1c3-dec3c6dcc67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(_, mats, _, img_metas, _, _, campose_imgs) = data\n",
    "\n",
    "for key, value in mats.items():\n",
    "    mats[key] = value.to(device)\n",
    "    \n",
    "# Collect adjacent erp images\n",
    "adj_erp_imgs = list()\n",
    "for key_idx in [0, -1]: # current idx first\n",
    "    cur_idx = key_idx + idx\n",
    "    if cur_idx < 0: # first frame of the scene video\n",
    "        cur_idx = idx\n",
    "    \n",
    "    # load images\n",
    "    fname = erp_imgs[cur_idx]\n",
    "    print(f\"Load {cur_idx}th erp image\")\n",
    "    erp_img = cv2.imread(fname, cv2.IMREAD_COLOR)\n",
    "    erp_img = erp_img.astype(np.float32) / 255\n",
    "    erp_img = np.transpose(erp_img, [2, 0, 1]) # permutation, 세 번째 axis가 첫 번째 axis로\n",
    "    erp_img = torch.from_numpy(erp_img) # Create Tensor from numpy array\n",
    "    erp_img = erp_img.unsqueeze(0) # Increase Tensor dimension by 1\n",
    "    \n",
    "    adj_erp_imgs.append(erp_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f411ed5-11f3-46c6-834a-fa351fc9df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작!\n",
    "with torch.no_grad():\n",
    "    starter.record()\n",
    "    \n",
    "    # to GPU memory (시간 측정?)\n",
    "    for i, erp_img in enumerate(adj_erp_imgs):\n",
    "        adj_erp_imgs[i] = erp_img.to(device)\n",
    "\n",
    "    # Tangent projection with GPU\n",
    "    persp_seqs = list()\n",
    "    for erp_img in adj_erp_imgs:\n",
    "        persp = F.grid_sample(erp_img, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "        persp_reshape = F.unfold(persp, kernel_size=(tangent_h, tangent_w), stride=(tangent_h, tangent_w))\n",
    "        persp_reshape = persp_reshape.reshape(1, 3, tangent_h, tangent_w, n_patch)\n",
    "        persp_seqs.append(persp_reshape)\n",
    "\n",
    "    # Re-shape patches and prepare camera pose estimation inputs\n",
    "    sweep_patches = list()\n",
    "    sweep_pose_inputs = list()\n",
    "    for persp_seq in persp_seqs:\n",
    "        patches = list()\n",
    "        campose_inputs = list()\n",
    "        for i in range(len(ida_aug_conf['cams'])):\n",
    "            patch = persp_seq[0, :, :, :, i]\n",
    "            \n",
    "            # Resize patch for camera pose input\n",
    "            campose_input = F.interpolate(patch.unsqueeze(0), size=(192, 640), # Image resize with interpolate function\n",
    "                                          mode='bicubic', align_corners=False) # bicubic, bilinear, ...\n",
    "            campose_inputs.append(campose_input)\n",
    "            \n",
    "            # image normalization for model input\n",
    "            patch = transforms.ToPILImage()(patch) # time (ms):  5.7\n",
    "            patch = mmcv.imnormalize(np.array(patch), \n",
    "                                     np.array(img_conf['img_mean'], np.float32), # TODO check: img_mean, img_std?\n",
    "                                     np.array(img_conf['img_std'], np.float32),\n",
    "                                     img_conf['to_rgb']) # time (ms):  5.79\n",
    "            patch = torch.from_numpy(patch).permute(2, 0, 1) # time (ms):  0.34\n",
    "            patch = patch.to(device) # time (ms):  1.20\n",
    "            \n",
    "            patches.append(patch)\n",
    "        sweep_patches.append(torch.stack(patches, 0))\n",
    "        sweep_pose_inputs.append(torch.stack(campose_inputs))\n",
    "    sweep_patches = torch.stack(sweep_patches, 0).unsqueeze(0)\n",
    "\n",
    "    # Camera pose estimation\n",
    "    campose_mats = list()\n",
    "    for cam_idx in range(len(ida_aug_conf['cams'])):\n",
    "        source_image = sweep_pose_inputs[0][cam_idx]\n",
    "        target_image = sweep_pose_inputs[1][cam_idx]\n",
    "        pose_inputs = [source_image, target_image]\n",
    "        # if cam_idx == 0:\n",
    "        #     print(torch.cat(pose_inputs, 1).shape)\n",
    "        pose_inputs = [pose_enc(torch.cat(pose_inputs, 1))]\n",
    "        axisangle, translation = pose_dec(pose_inputs)\n",
    "        # print(\"CAM\", ida_aug_conf['cams'][cam_idx])\n",
    "        # print(\"pose axis angle\", axisangle)\n",
    "        # print(\"pose translation\", translation)\n",
    "        pose = transformation_from_parameters(axisangle[:, 0], translation[:, 0], invert=False)\n",
    "        # print(f\"Pose matrix for image pair {cam_idx}: \\n{pose}\")\n",
    "        campose_mats.append(pose)\n",
    "\n",
    "    sensor2sensor_mats = list()\n",
    "    sensor2sensor_mats.append(torch.stack(campose_mats, 0).unsqueeze(0))\n",
    "    sensor2sensor_mats.append(sensor2sensor_mats[0].inverse())\n",
    "    \n",
    "    # 3D object detection\n",
    "    preds = model(sweep_patches, mats, posenet_outputs=sensor2sensor_mats)\n",
    "\n",
    "    ender.record()\n",
    "    torch.cuda.synchronize()\n",
    "    inference_time = starter.elapsed_time(ender)\n",
    "    print(\"inference time (ms): \", inference_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99993283-5230-4e1b-a875-a7c7ea573023",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.get_bboxes(preds, img_metas)\n",
    "for i in range(len(results)):\n",
    "    results[i][0] = results[i][0].detach().cpu().numpy()\n",
    "    results[i][1] = results[i][1].detach().cpu().numpy()\n",
    "    results[i][2] = results[i][2].detach().cpu().numpy()\n",
    "    results[i].append(img_metas[i])\n",
    "\n",
    "print(\"찾은 물체 개수: \", len(results[0][:3][0]))\n",
    "\n",
    "\"\"\" format bbox results \"\"\"\n",
    "pred_results = results[0][:3]\n",
    "img_metas = results[0][3]\n",
    "\n",
    "boxes, scores, labels = pred_results\n",
    "boxes = boxes\n",
    "sample_token = img_metas['token']\n",
    "trans = np.array(img_metas['ego2global_translation'])\n",
    "rot = Quaternion(img_metas['ego2global_rotation'])\n",
    "nusc_annos = {}\n",
    "annos = list()\n",
    "\n",
    "for i, box in enumerate(boxes):\n",
    "    name = CLASSES[labels[i]]\n",
    "    center = box[:3]\n",
    "    wlh = box[[4, 3, 5]]\n",
    "    box_yaw = box[6]\n",
    "    box_vel = box[7:].tolist()\n",
    "    box_vel.append(0)\n",
    "    quat = pyquaternion.Quaternion(axis=[0, 0, 1], radians=box_yaw)\n",
    "    \n",
    "    nusc_box = Box(center, wlh, quat, velocity=box_vel)\n",
    "    nusc_box.rotate(rot)\n",
    "    nusc_box.translate(trans)\n",
    "    \n",
    "    if np.sqrt(nusc_box.velocity[0]**2 + nusc_box.velocity[1]**2) > 0.2:\n",
    "        if name in ['car','construction_vehicle','bus','truck','trailer']:\n",
    "            attr = 'vehicle.moving'\n",
    "        elif name in ['bicycle', 'motorcycle']:\n",
    "            attr = 'cycle.with_rider'\n",
    "        else:\n",
    "            attr = DefaultAttribute[name]\n",
    "    else:\n",
    "        if name in ['pedestrian']:\n",
    "            attr = 'pedestrian.standing'\n",
    "        elif name in ['bus']:\n",
    "            attr = 'vehicle.stopped'\n",
    "        else:\n",
    "            attr = DefaultAttribute[name]\n",
    "    nusc_anno = dict(\n",
    "        sample_token=sample_token,\n",
    "        translation=nusc_box.center.tolist(),\n",
    "        size=nusc_box.wlh.tolist(),\n",
    "        rotation=nusc_box.orientation.elements.tolist(),\n",
    "        velocity=nusc_box.velocity[:2],\n",
    "        detection_name=name,\n",
    "        detection_score=float(scores[i]),\n",
    "        attribute_name=attr,\n",
    "    )\n",
    "    annos.append(nusc_anno)\n",
    "nusc_annos[sample_token] = annos\n",
    "\n",
    "modality=dict(use_lidar=False,\n",
    "              use_camera=True,\n",
    "              use_radar=False,\n",
    "              use_map=False,\n",
    "              use_external=False)\n",
    "nusc_submissions = {\n",
    "    'meta': modality,\n",
    "    'results': nusc_annos,\n",
    "}\n",
    "\n",
    "\"\"\" save results to json file\"\"\"\n",
    "jsonfile_prefix = os.path.dirname('./outputs/' + exp_name + '/')\n",
    "mmcv.mkdir_or_exist(jsonfile_prefix)\n",
    "res_path = os.path.join(jsonfile_prefix, 'results_nusc.json')\n",
    "print('Results writes to', res_path)\n",
    "mmcv.dump(nusc_submissions, res_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737192b-4611-46d4-a0e5-e1853058fa71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36760a1c-83a8-493f-91f8-ef7f0b0ee798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ego_box(box_dict, ego2global_rotation, ego2global_translation):\n",
    "    box = Box(\n",
    "        box_dict['translation'],\n",
    "        box_dict['size'],\n",
    "        Quaternion(box_dict['rotation']),\n",
    "    )\n",
    "    trans = -np.array(ego2global_translation)\n",
    "    rot = Quaternion(ego2global_rotation).inverse\n",
    "    box.translate(trans)\n",
    "    box.rotate(rot)\n",
    "    box_xyz = np.array(box.center)\n",
    "    box_dxdydz = np.array(box.wlh)[[1, 0, 2]]\n",
    "    box_yaw = np.array([box.orientation.yaw_pitch_roll[0]])\n",
    "    box_velo = np.array(box.velocity[:2])\n",
    "    return np.concatenate([box_xyz, box_dxdydz, box_yaw, box_velo])\n",
    "\n",
    "def rotate_points_along_z(points, angle):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        points: (B, N, 3 + C)\n",
    "        angle: (B), angle along z-axis, angle increases x ==> y\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    cosa = np.cos(angle)\n",
    "    sina = np.sin(angle)\n",
    "    zeros = np.zeros(points.shape[0])\n",
    "    ones = np.ones(points.shape[0])\n",
    "    rot_matrix = np.stack(\n",
    "        (cosa, sina, zeros, -sina, cosa, zeros, zeros, zeros, ones),\n",
    "        axis=1).reshape(-1, 3, 3)\n",
    "    points_rot = np.matmul(points[:, :, 0:3], rot_matrix)\n",
    "    points_rot = np.concatenate((points_rot, points[:, :, 3:]), axis=-1)\n",
    "    return points_rot\n",
    "\n",
    "def get_corners(boxes3d):\n",
    "    \"\"\"\n",
    "        7 -------- 4\n",
    "       /|         /|\n",
    "      6 -------- 5 .\n",
    "      | |        | |\n",
    "      . 3 -------- 0\n",
    "      |/         |/\n",
    "      2 -------- 1\n",
    "    Args:\n",
    "        boxes3d:  (N, 7) [x, y, z, dx, dy, dz, heading],\n",
    "            (x, y, z) is the box center\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    template = (np.array((\n",
    "        [1, 1, -1],\n",
    "        [1, -1, -1],\n",
    "        [-1, -1, -1],\n",
    "        [-1, 1, -1],\n",
    "        [1, 1, 1],\n",
    "        [1, -1, 1],\n",
    "        [-1, -1, 1],\n",
    "        [-1, 1, 1],\n",
    "    )) / 2)\n",
    "\n",
    "    corners3d = np.tile(boxes3d[:, None, 3:6], [1, 8, 1]) * template[None, :, :]\n",
    "    corners3d = rotate_points_along_z(corners3d.reshape(-1, 8, 3),boxes3d[:, 6]).reshape(-1, 8, 3)\n",
    "    corners3d += boxes3d[:, None, 0:3]\n",
    "\n",
    "    return corners3d\n",
    "\n",
    "def get_bev_lines(corners):\n",
    "    return [[[corners[i, 0], corners[(i + 1) % 4, 0]],\n",
    "             [corners[i, 1], corners[(i + 1) % 4, 1]]] for i in range(4)]\n",
    "\n",
    "def get_3d_lines(corners):\n",
    "    ret = []\n",
    "    for st, ed in [[0, 1], [1, 2], [2, 3], [3, 0], [4, 5], [5, 6], [6, 7],\n",
    "                   [7, 4], [0, 4], [1, 5], [2, 6], [3, 7]]:\n",
    "        if corners[st, -1] > 0 and corners[ed, -1] > 0:\n",
    "            ret.append([[corners[st, 0], corners[ed, 0]],\n",
    "                        [corners[st, 1], corners[ed, 1]]])\n",
    "    return ret\n",
    "\n",
    "def get_cam_corners(corners, translation, rotation, cam_intrinsics):\n",
    "    cam_corners = corners.copy()\n",
    "    cam_corners -= np.array(translation)\n",
    "    cam_corners = cam_corners @ Quaternion(rotation).inverse.rotation_matrix.T\n",
    "    cam_corners = cam_corners @ np.array(cam_intrinsics).T\n",
    "    valid = cam_corners[:, -1] > 0\n",
    "    cam_corners /= cam_corners[:, 2:3]\n",
    "    cam_corners[~valid] = 0\n",
    "    return cam_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d144f1a-9efc-4a28-a501-2cfb54582d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_tangent_h = 900 # visualization resolution\n",
    "vis_tangent_w = 1600\n",
    "\n",
    "n_patch, vis_grid = createProjectGrid(erp_h, erp_w, vis_tangent_h, vis_tangent_w, num_rows, num_cols, phi_centers, fov)\n",
    "\n",
    "# load erp image\n",
    "fname = erp_imgs[idx]\n",
    "vis_erp_img = cv2.imread(fname, cv2.IMREAD_COLOR)\n",
    "vis_erp_img = vis_erp_img.astype(np.float32) / 255\n",
    "vis_erp_img = np.transpose(vis_erp_img, [2, 0, 1]) # permutation, 세 번째 axis가 첫 번째 axis로\n",
    "vis_erp_img = torch.from_numpy(vis_erp_img) # Create Tensor from numpy array\n",
    "vis_erp_img = vis_erp_img.unsqueeze(0) # Increase Tensor dimension by 1\n",
    "vis_erp_img = vis_erp_img.to(device)\n",
    "\n",
    "vis_persp = F.grid_sample(vis_erp_img, vis_grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "vis_persp_reshape = F.unfold(vis_persp, kernel_size=(vis_tangent_h, vis_tangent_w), stride=(vis_tangent_h, vis_tangent_w))\n",
    "vis_persp_reshape = vis_persp_reshape.reshape(1, 3, vis_tangent_h, vis_tangent_w, n_patch).cpu()\n",
    "\n",
    "# Visualize Tangent patches\n",
    "\n",
    "# patch_num = 0\n",
    "# for num_col in num_cols:\n",
    "#     patch_num = num_col + patch_num\n",
    "\n",
    "# _, ax = plt.subplots(2, 3, figsize=(24, 10))\n",
    "# j = 0\n",
    "# for i in range(patch_num):\n",
    "#     patch_cnt = i\n",
    "#     if i == 3:\n",
    "#         j += 1\n",
    "#     i = i % 3\n",
    "\n",
    "#     cur_patch = vis_persp_reshape[0, :, :, :, patch_cnt].permute(1, 2, 0).numpy()\n",
    "#     cur_patch = cur_patch * 255\n",
    "#     ax[j, i].imshow(cur_patch[:,:,[2,1,0]].astype(np.uint8), aspect=1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac918f89-1655-4e1a-b04c-881865c4015f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_path = './outputs/bevstereo_ema_da_key2/results_nusc.json'\n",
    "save_path = './outputs/det_result_imgs/scene_5/'\n",
    "data_root = '../data/nuscenes/'\n",
    "results = mmcv.load(result_path)['results']\n",
    "show_classes=[\n",
    "    'car',\n",
    "    'truck',\n",
    "    'construction_vehicle',\n",
    "    'bus',\n",
    "    'trailer',\n",
    "    'barrier',\n",
    "    'motorcycle',\n",
    "    'bicycle',\n",
    "    'pedestrian',\n",
    "    'traffic_cone',\n",
    "]\n",
    "\n",
    "# IMG_KEYS = ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT']\n",
    "IMG_KEYS = ['CAM_BACK_RIGHT', 'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK']\n",
    "\n",
    "# Get data from dataset\n",
    "info = infos[0] # nusc dataset의 idx는 0으로 고정\n",
    "\n",
    "ego2global_rotation = img_metas['ego2global_rotation']\n",
    "ego2global_translation = img_metas['ego2global_translation']\n",
    "\n",
    "# Set cameras\n",
    "threshold = 0.35\n",
    "show_range = 60\n",
    "\n",
    "# Get prediction corners\n",
    "pred_corners, pred_class = [], []\n",
    "for box in results[info['sample_token']]:\n",
    "    if box['detection_score'] >= threshold and box['detection_name'] in show_classes:\n",
    "        box3d = get_ego_box(box, ego2global_rotation, ego2global_translation)\n",
    "        box3d[2] += 0.5 * box3d[5]  # NOTE\n",
    "        if np.linalg.norm(box3d[:2]) <= show_range:\n",
    "            corners = get_corners(box3d[None])[0]\n",
    "            pred_corners.append(corners)\n",
    "            pred_class.append(box['detection_name'])\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(21, 8))\n",
    "\n",
    "for i, k in enumerate(ida_aug_conf['cams']):\n",
    "    # Draw camera views\n",
    "    fig_idx = i + 1 if i < 3 else i + 1\n",
    "    plt.subplot(2, 3, fig_idx)\n",
    "\n",
    "    # Set camera attributes\n",
    "    plt.title(k)\n",
    "    plt.axis('off')\n",
    "    plt.xlim(0, 1600)\n",
    "    plt.ylim(900, 0)\n",
    "\n",
    "    cur_patch = vis_persp_reshape[0, :, :, :, i].permute(1, 2, 0).numpy()\n",
    "    cur_patch = cur_patch * 255\n",
    "    img = cur_patch[:,:,[2,1,0]].astype(np.uint8)\n",
    "    \n",
    "    # Draw images\n",
    "    plt.imshow(img)\n",
    "\n",
    "    # Draw 3D predictions\n",
    "    for corners, cls in zip(pred_corners, pred_class):\n",
    "        \n",
    "        if dataset.sensor2ego_trans is None:\n",
    "            sensor2ego_trans = info['cam_infos'][k]['calibrated_sensor']['translation']\n",
    "        else:\n",
    "            sensor2ego_trans = dataset.sensor2ego_trans\n",
    "        \n",
    "        if dataset.sensor2ego_rot_eulers is None:\n",
    "            sensor2ego_rot = info['cam_infos'][k]['calibrated_sensor']['rotation']\n",
    "        else:\n",
    "            sensor2ego_degrees = dataset.sensor2ego_rot_eulers[k]\n",
    "            sensor2ego_radians = [degree * np.pi / 180 for degree in sensor2ego_degrees]\n",
    "            # print(sensor2ego_degrees)\n",
    "            sensor2ego_q = Quaternion(get_quaternion_from_euler(sensor2ego_radians))\n",
    "            sensor2ego_rot = sensor2ego_q\n",
    "        \n",
    "        if dataset.tangent_intrinsics is None:\n",
    "            intrinsic = info['cam_infos'][k]['calibrated_sensor']['camera_intrinsic']\n",
    "        else:\n",
    "            intrinsic = tangent_intrinsics[k]\n",
    "            \n",
    "        cam_corners = get_cam_corners(corners, sensor2ego_trans, sensor2ego_rot, intrinsic)\n",
    "        # cam_corners = get_cam_corners(\n",
    "        #         corners,\n",
    "        #         info['cam_infos'][k]['calibrated_sensor']['translation'],\n",
    "        #         info['cam_infos'][k]['calibrated_sensor']['rotation'],\n",
    "        #         info['cam_infos'][k]['calibrated_sensor']['camera_intrinsic'])\n",
    "        \n",
    "        lines = get_3d_lines(cam_corners)\n",
    "        for line in lines:\n",
    "            plt.plot(line[0],\n",
    "                     line[1],\n",
    "                     c=cm.get_cmap('tab10')(show_classes.index(cls)))\n",
    "\n",
    "# Set legend\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(),\n",
    "           by_label.keys(),\n",
    "           loc='upper right',\n",
    "           framealpha=1)\n",
    "\n",
    "plt.tight_layout(w_pad=0, h_pad=2)\n",
    "# save_name ='output_%04d.jpg' % idx\n",
    "# plt.savefig(save_path+save_name)\n",
    "plt.show()\n",
    "\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicv",
   "language": "python",
   "name": "omnicv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
