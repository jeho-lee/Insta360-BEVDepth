{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2170d01d-b687-4da0-bd0e-173b80ee5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numba\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.models as models\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from torch.cuda.amp.autocast_mode import autocast\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch import nn\n",
    "from torch.cuda.amp.autocast_mode import autocast\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import mmcv\n",
    "from mmcv.cnn import build_conv_layer\n",
    "\n",
    "from mmdet.models import build_backbone\n",
    "from mmdet.models.backbones.resnet import BasicBlock\n",
    "from mmdet.core import reduce_mean\n",
    "from mmdet.models import build_backbone\n",
    "\n",
    "from mmdet3d.core import draw_heatmap_gaussian, gaussian_radius\n",
    "from mmdet3d.models import build_neck\n",
    "from mmdet3d.models.dense_heads.centerpoint_head import CenterHead, circle_nms\n",
    "from mmdet3d.models.utils import clip_sigmoid\n",
    "from mmdet3d.models import build_neck\n",
    "\n",
    "from bevdepth.datasets.nusc_det_dataset import NuscDetDataset, collate_fn\n",
    "from bevdepth.evaluators.det_evaluators import DetNuscEvaluator\n",
    "from bevdepth.callbacks.ema import EMACallback\n",
    "from bevdepth.utils.torch_dist import all_gather_object, get_rank, synchronize\n",
    "\n",
    "try:\n",
    "    from bevdepth.ops.voxel_pooling import voxel_pooling\n",
    "except ImportError:\n",
    "    print('Import VoxelPooling fail.')\n",
    "    \n",
    "from mmdet3d.core.bbox.structures.lidar_box3d import LiDARInstance3DBoxes\n",
    "from nuscenes.utils.data_classes import Box, LidarPointCloud\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from PIL import Image\n",
    "from pyquaternion import Quaternion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e19e5a-e686-44cc-af34-6f082a8413d5",
   "metadata": {},
   "source": [
    "# Base Backbone (BEVDepth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26ffedbd-6ef2-4da4-9fb9-a9ff379d88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from layers/backbones/base_lss_fpn\"\"\"\n",
    "\n",
    "class _ASPPModule(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size, padding, dilation,\n",
    "                 BatchNorm):\n",
    "        super(_ASPPModule, self).__init__()\n",
    "        self.atrous_conv = nn.Conv2d(inplanes,\n",
    "                                     planes,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     stride=1,\n",
    "                                     padding=padding,\n",
    "                                     dilation=dilation,\n",
    "                                     bias=False)\n",
    "        self.bn = BatchNorm(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.atrous_conv(x)\n",
    "        x = self.bn(x)\n",
    "\n",
    "        return self.relu(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "class ASPP(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, mid_channels=256, BatchNorm=nn.BatchNorm2d):\n",
    "        super(ASPP, self).__init__()\n",
    "\n",
    "        dilations = [1, 6, 12, 18]\n",
    "\n",
    "        self.aspp1 = _ASPPModule(inplanes,\n",
    "                                 mid_channels,\n",
    "                                 1,\n",
    "                                 padding=0,\n",
    "                                 dilation=dilations[0],\n",
    "                                 BatchNorm=BatchNorm)\n",
    "        self.aspp2 = _ASPPModule(inplanes,\n",
    "                                 mid_channels,\n",
    "                                 3,\n",
    "                                 padding=dilations[1],\n",
    "                                 dilation=dilations[1],\n",
    "                                 BatchNorm=BatchNorm)\n",
    "        self.aspp3 = _ASPPModule(inplanes,\n",
    "                                 mid_channels,\n",
    "                                 3,\n",
    "                                 padding=dilations[2],\n",
    "                                 dilation=dilations[2],\n",
    "                                 BatchNorm=BatchNorm)\n",
    "        self.aspp4 = _ASPPModule(inplanes,\n",
    "                                 mid_channels,\n",
    "                                 3,\n",
    "                                 padding=dilations[3],\n",
    "                                 dilation=dilations[3],\n",
    "                                 BatchNorm=BatchNorm)\n",
    "\n",
    "        self.global_avg_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(inplanes, mid_channels, 1, stride=1, bias=False),\n",
    "            BatchNorm(mid_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(int(mid_channels * 5),\n",
    "                               mid_channels,\n",
    "                               1,\n",
    "                               bias=False)\n",
    "        self.bn1 = BatchNorm(mid_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = self.global_avg_pool(x)\n",
    "        x5 = F.interpolate(x5,\n",
    "                           size=x4.size()[2:],\n",
    "                           mode='bilinear',\n",
    "                           align_corners=True)\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return self.dropout(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features=None,\n",
    "                 out_features=None,\n",
    "                 act_layer=nn.ReLU,\n",
    "                 drop=0.0):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop2 = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, act_layer=nn.ReLU, gate_layer=nn.Sigmoid):\n",
    "        super().__init__()\n",
    "        self.conv_reduce = nn.Conv2d(channels, channels, 1, bias=True)\n",
    "        self.act1 = act_layer()\n",
    "        self.conv_expand = nn.Conv2d(channels, channels, 1, bias=True)\n",
    "        self.gate = gate_layer()\n",
    "\n",
    "    def forward(self, x, x_se):\n",
    "        x_se = self.conv_reduce(x_se)\n",
    "        x_se = self.act1(x_se)\n",
    "        x_se = self.conv_expand(x_se)\n",
    "        return x * self.gate(x_se)\n",
    "\n",
    "class DepthAggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    pixel cloud feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(DepthAggregation, self).__init__()\n",
    "\n",
    "        self.reduce_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      mid_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels,\n",
    "                      mid_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels,\n",
    "                      mid_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.out_conv = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=True),\n",
    "            # nn.BatchNorm3d(out_channels),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    @autocast(False)\n",
    "    def forward(self, x):\n",
    "        x = self.reduce_conv(x)\n",
    "        x = self.conv(x) + x\n",
    "        x = self.out_conv(x)\n",
    "        return x\n",
    "\n",
    "class BaseLSSFPN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 x_bound,\n",
    "                 y_bound,\n",
    "                 z_bound,\n",
    "                 d_bound,\n",
    "                 final_dim,\n",
    "                 downsample_factor,\n",
    "                 output_channels,\n",
    "                 img_backbone_conf,\n",
    "                 img_neck_conf,\n",
    "                 depth_net_conf,\n",
    "                 use_da=False):\n",
    "        \"\"\"Modified from `https://github.com/nv-tlabs/lift-splat-shoot`.\n",
    "\n",
    "        Args:\n",
    "            x_bound (list): Boundaries for x.\n",
    "            y_bound (list): Boundaries for y.\n",
    "            z_bound (list): Boundaries for z.\n",
    "            d_bound (list): Boundaries for d.\n",
    "            final_dim (list): Dimension for input images.\n",
    "            downsample_factor (int): Downsample factor between feature map\n",
    "                and input image.\n",
    "            output_channels (int): Number of channels for the output\n",
    "                feature map.\n",
    "            img_backbone_conf (dict): Config for image backbone.\n",
    "            img_neck_conf (dict): Config for image neck.\n",
    "            depth_net_conf (dict): Config for depth net.\n",
    "        \"\"\"\n",
    "\n",
    "        super(BaseLSSFPN, self).__init__()\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.d_bound = d_bound\n",
    "        self.final_dim = final_dim\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        self.register_buffer(\n",
    "            'voxel_size',\n",
    "            torch.Tensor([row[2] for row in [x_bound, y_bound, z_bound]]))\n",
    "        self.register_buffer(\n",
    "            'voxel_coord',\n",
    "            torch.Tensor([\n",
    "                row[0] + row[2] / 2.0 for row in [x_bound, y_bound, z_bound]\n",
    "            ]))\n",
    "        self.register_buffer(\n",
    "            'voxel_num',\n",
    "            torch.LongTensor([(row[1] - row[0]) / row[2]\n",
    "                              for row in [x_bound, y_bound, z_bound]]))\n",
    "        self.register_buffer('frustum', self.create_frustum())\n",
    "        self.depth_channels, _, _, _ = self.frustum.shape\n",
    "\n",
    "        self.img_backbone = build_backbone(img_backbone_conf)\n",
    "        self.img_neck = build_neck(img_neck_conf)\n",
    "        self.depth_net = self._configure_depth_net(depth_net_conf)\n",
    "\n",
    "        self.img_neck.init_weights()\n",
    "        self.img_backbone.init_weights()\n",
    "        self.use_da = use_da\n",
    "        if self.use_da:\n",
    "            self.depth_aggregation_net = self._configure_depth_aggregation_net(\n",
    "            )\n",
    "\n",
    "    def _configure_depth_net(self, depth_net_conf):\n",
    "        return DepthNet(\n",
    "            depth_net_conf['in_channels'],\n",
    "            depth_net_conf['mid_channels'],\n",
    "            self.output_channels,\n",
    "            self.depth_channels,\n",
    "        )\n",
    "\n",
    "    def _configure_depth_aggregation_net(self):\n",
    "        \"\"\"build pixel cloud feature extractor\"\"\"\n",
    "        return DepthAggregation(self.output_channels, self.output_channels,\n",
    "                                self.output_channels)\n",
    "\n",
    "    def _forward_voxel_net(self, img_feat_with_depth):\n",
    "        if self.use_da:\n",
    "            # BEVConv2D [n, c, d, h, w] -> [n, h, c, w, d]\n",
    "            img_feat_with_depth = img_feat_with_depth.permute(\n",
    "                0, 3, 1, 4,\n",
    "                2).contiguous()  # [n, c, d, h, w] -> [n, h, c, w, d]\n",
    "            n, h, c, w, d = img_feat_with_depth.shape\n",
    "            img_feat_with_depth = img_feat_with_depth.view(-1, c, w, d)\n",
    "            img_feat_with_depth = (\n",
    "                self.depth_aggregation_net(img_feat_with_depth).view(\n",
    "                    n, h, c, w, d).permute(0, 2, 4, 1, 3).contiguous().float())\n",
    "        return img_feat_with_depth\n",
    "\n",
    "    def create_frustum(self):\n",
    "        \"\"\"Generate frustum\"\"\"\n",
    "        # make grid in image plane\n",
    "        ogfH, ogfW = self.final_dim\n",
    "        fH, fW = ogfH // self.downsample_factor, ogfW // self.downsample_factor\n",
    "        d_coords = torch.arange(*self.d_bound,\n",
    "                                dtype=torch.float).view(-1, 1,\n",
    "                                                        1).expand(-1, fH, fW)\n",
    "        D, _, _ = d_coords.shape\n",
    "        x_coords = torch.linspace(0, ogfW - 1, fW, dtype=torch.float).view(\n",
    "            1, 1, fW).expand(D, fH, fW)\n",
    "        y_coords = torch.linspace(0, ogfH - 1, fH,\n",
    "                                  dtype=torch.float).view(1, fH,\n",
    "                                                          1).expand(D, fH, fW)\n",
    "        paddings = torch.ones_like(d_coords)\n",
    "\n",
    "        # D x H x W x 3\n",
    "        frustum = torch.stack((x_coords, y_coords, d_coords, paddings), -1)\n",
    "        return frustum\n",
    "\n",
    "    def get_geometry(self, sensor2ego_mat, intrin_mat, ida_mat, bda_mat):\n",
    "        \"\"\"Transfer points from camera coord to ego coord.\n",
    "\n",
    "        Args:\n",
    "            rots(Tensor): Rotation matrix from camera to ego.\n",
    "            trans(Tensor): Translation matrix from camera to ego.\n",
    "            intrins(Tensor): Intrinsic matrix.\n",
    "            post_rots_ida(Tensor): Rotation matrix for ida.\n",
    "            post_trans_ida(Tensor): Translation matrix for ida\n",
    "            post_rot_bda(Tensor): Rotation matrix for bda.\n",
    "\n",
    "        Returns:\n",
    "            Tensors: points ego coord.\n",
    "        \"\"\"\n",
    "        batch_size, num_cams, _, _ = sensor2ego_mat.shape\n",
    "\n",
    "        # undo post-transformation\n",
    "        # B x N x D x H x W x 3\n",
    "        points = self.frustum\n",
    "        ida_mat = ida_mat.view(batch_size, num_cams, 1, 1, 1, 4, 4)\n",
    "        points = ida_mat.inverse().matmul(points.unsqueeze(-1))\n",
    "        # cam_to_ego\n",
    "        points = torch.cat(\n",
    "            (points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3],\n",
    "             points[:, :, :, :, :, 2:]), 5)\n",
    "\n",
    "        combine = sensor2ego_mat.matmul(torch.inverse(intrin_mat))\n",
    "        points = combine.view(batch_size, num_cams, 1, 1, 1, 4,\n",
    "                              4).matmul(points)\n",
    "        if bda_mat is not None:\n",
    "            bda_mat = bda_mat.unsqueeze(1).repeat(1, num_cams, 1, 1).view(\n",
    "                batch_size, num_cams, 1, 1, 1, 4, 4)\n",
    "            points = (bda_mat @ points).squeeze(-1)\n",
    "        else:\n",
    "            points = points.squeeze(-1)\n",
    "        return points[..., :3]\n",
    "\n",
    "    def get_cam_feats(self, imgs):\n",
    "        \"\"\"Get feature maps from images.\"\"\"\n",
    "        batch_size, num_sweeps, num_cams, num_channels, imH, imW = imgs.shape\n",
    "\n",
    "        imgs = imgs.flatten().view(batch_size * num_sweeps * num_cams,\n",
    "                                   num_channels, imH, imW)\n",
    "        img_feats = self.img_neck(self.img_backbone(imgs))[0]\n",
    "        img_feats = img_feats.reshape(batch_size, num_sweeps, num_cams,\n",
    "                                      img_feats.shape[1], img_feats.shape[2],\n",
    "                                      img_feats.shape[3])\n",
    "        return img_feats\n",
    "\n",
    "    def _forward_depth_net(self, feat, mats_dict):\n",
    "        return self.depth_net(feat, mats_dict)\n",
    "\n",
    "    def _forward_single_sweep(self,\n",
    "                              sweep_index,\n",
    "                              sweep_imgs,\n",
    "                              mats_dict,\n",
    "                              is_return_depth=False):\n",
    "        \"\"\"Forward function for single sweep.\n",
    "\n",
    "        Args:\n",
    "            sweep_index (int): Index of sweeps.\n",
    "            sweep_imgs (Tensor): Input images.\n",
    "            mats_dict (dict):\n",
    "                sensor2ego_mats(Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats(Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats(Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats(Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat(Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            is_return_depth (bool, optional): Whether to return depth.\n",
    "                Default: False.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: BEV feature map.\n",
    "        \"\"\"\n",
    "        batch_size, num_sweeps, num_cams, num_channels, img_height, \\\n",
    "            img_width = sweep_imgs.shape\n",
    "        img_feats = self.get_cam_feats(sweep_imgs)\n",
    "        source_features = img_feats[:, 0, ...]\n",
    "        depth_feature = self._forward_depth_net(\n",
    "            source_features.reshape(batch_size * num_cams,\n",
    "                                    source_features.shape[2],\n",
    "                                    source_features.shape[3],\n",
    "                                    source_features.shape[4]),\n",
    "            mats_dict,\n",
    "        )\n",
    "        depth = depth_feature[:, :self.depth_channels].softmax(1)\n",
    "        img_feat_with_depth = depth.unsqueeze(\n",
    "            1) * depth_feature[:, self.depth_channels:(\n",
    "                self.depth_channels + self.output_channels)].unsqueeze(2)\n",
    "\n",
    "        img_feat_with_depth = self._forward_voxel_net(img_feat_with_depth)\n",
    "\n",
    "        img_feat_with_depth = img_feat_with_depth.reshape(\n",
    "            batch_size,\n",
    "            num_cams,\n",
    "            img_feat_with_depth.shape[1],\n",
    "            img_feat_with_depth.shape[2],\n",
    "            img_feat_with_depth.shape[3],\n",
    "            img_feat_with_depth.shape[4],\n",
    "        )\n",
    "        geom_xyz = self.get_geometry(\n",
    "            mats_dict['sensor2ego_mats'][:, sweep_index, ...],\n",
    "            mats_dict['intrin_mats'][:, sweep_index, ...],\n",
    "            mats_dict['ida_mats'][:, sweep_index, ...],\n",
    "            mats_dict.get('bda_mat', None),\n",
    "        )\n",
    "        img_feat_with_depth = img_feat_with_depth.permute(0, 1, 3, 4, 5, 2)\n",
    "        geom_xyz = ((geom_xyz - (self.voxel_coord - self.voxel_size / 2.0)) /\n",
    "                    self.voxel_size).int()\n",
    "        feature_map = voxel_pooling(geom_xyz, img_feat_with_depth.contiguous(),\n",
    "                                    self.voxel_num.cuda())\n",
    "        if is_return_depth:\n",
    "            return feature_map.contiguous(), depth\n",
    "        return feature_map.contiguous()\n",
    "\n",
    "    def forward(self,\n",
    "                sweep_imgs,\n",
    "                mats_dict,\n",
    "                timestamps=None,\n",
    "                is_return_depth=False):\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            sweep_imgs(Tensor): Input images with shape of (B, num_sweeps,\n",
    "                num_cameras, 3, H, W).\n",
    "            mats_dict(dict):\n",
    "                sensor2ego_mats(Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats(Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats(Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats(Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat(Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            timestamps(Tensor): Timestamp for all images with the shape of(B,\n",
    "                num_sweeps, num_cameras).\n",
    "\n",
    "        Return:\n",
    "            Tensor: bev feature map.\n",
    "        \"\"\"\n",
    "        batch_size, num_sweeps, num_cams, num_channels, img_height, \\\n",
    "            img_width = sweep_imgs.shape\n",
    "\n",
    "        key_frame_res = self._forward_single_sweep(\n",
    "            0,\n",
    "            sweep_imgs[:, 0:1, ...],\n",
    "            mats_dict,\n",
    "            is_return_depth=is_return_depth)\n",
    "        if num_sweeps == 1:\n",
    "            return key_frame_res\n",
    "\n",
    "        key_frame_feature = key_frame_res[\n",
    "            0] if is_return_depth else key_frame_res\n",
    "\n",
    "        ret_feature_list = [key_frame_feature]\n",
    "        for sweep_index in range(1, num_sweeps):\n",
    "            with torch.no_grad():\n",
    "                feature_map = self._forward_single_sweep(\n",
    "                    sweep_index,\n",
    "                    sweep_imgs[:, sweep_index:sweep_index + 1, ...],\n",
    "                    mats_dict,\n",
    "                    is_return_depth=False)\n",
    "                ret_feature_list.append(feature_map)\n",
    "\n",
    "        if is_return_depth:\n",
    "            return torch.cat(ret_feature_list, 1), key_frame_res[1]\n",
    "        else:\n",
    "            return torch.cat(ret_feature_list, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aee07e-f23b-485a-91ab-ee611ed79bc0",
   "metadata": {},
   "source": [
    "# BEVStereo Backbone (LSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42db3824-356f-4f73-910d-3b7ce00ccce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from layers/backbones/bevstereo_lss_fpn.py\"\"\"\n",
    "\n",
    "class ConvBnReLU3D(nn.Module):\n",
    "    \"\"\"Implements of 3d convolution + batch normalization + ReLU.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        pad: int = 1,\n",
    "        dilation: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"initialization method for convolution3D +\n",
    "            batch normalization + relu module\n",
    "        Args:\n",
    "            in_channels: input channel number of convolution layer\n",
    "            out_channels: output channel number of convolution layer\n",
    "            kernel_size: kernel size of convolution layer\n",
    "            stride: stride of convolution layer\n",
    "            pad: pad of convolution layer\n",
    "            dilation: dilation of convolution layer\n",
    "        \"\"\"\n",
    "        super(ConvBnReLU3D, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels,\n",
    "                              out_channels,\n",
    "                              kernel_size,\n",
    "                              stride=stride,\n",
    "                              padding=pad,\n",
    "                              dilation=dilation,\n",
    "                              bias=False)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"forward method\"\"\"\n",
    "        return F.relu(self.bn(self.conv(x)), inplace=True)\n",
    "\n",
    "\n",
    "class DepthNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 mid_channels,\n",
    "                 context_channels,\n",
    "                 depth_channels,\n",
    "                 d_bound,\n",
    "                 num_ranges=4):\n",
    "        super(DepthNet, self).__init__()\n",
    "        self.reduce_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      mid_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.context_conv = nn.Conv2d(mid_channels,\n",
    "                                      context_channels,\n",
    "                                      kernel_size=1,\n",
    "                                      stride=1,\n",
    "                                      padding=0)\n",
    "        self.bn = nn.BatchNorm1d(27)\n",
    "        self.depth_mlp = Mlp(27, mid_channels, mid_channels)\n",
    "        self.depth_se = SELayer(mid_channels)  # NOTE: add camera-aware\n",
    "        self.context_mlp = Mlp(27, mid_channels, mid_channels)\n",
    "        self.context_se = SELayer(mid_channels)  # NOTE: add camera-aware\n",
    "        self.depth_feat_conv = nn.Sequential(\n",
    "            BasicBlock(mid_channels, mid_channels),\n",
    "            BasicBlock(mid_channels, mid_channels),\n",
    "            ASPP(mid_channels, mid_channels),\n",
    "            build_conv_layer(cfg=dict(\n",
    "                type='DCN',\n",
    "                in_channels=mid_channels,\n",
    "                out_channels=mid_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                groups=4,\n",
    "                im2col_step=128,\n",
    "            )),\n",
    "        )\n",
    "        self.mu_sigma_range_net = nn.Sequential(\n",
    "            BasicBlock(mid_channels, mid_channels),\n",
    "            nn.ConvTranspose2d(mid_channels,\n",
    "                               mid_channels,\n",
    "                               3,\n",
    "                               stride=2,\n",
    "                               padding=1,\n",
    "                               output_padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(mid_channels,\n",
    "                               mid_channels,\n",
    "                               3,\n",
    "                               stride=2,\n",
    "                               padding=1,\n",
    "                               output_padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels,\n",
    "                      num_ranges * 3,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "        )\n",
    "        self.mono_depth_net = nn.Sequential(\n",
    "            BasicBlock(mid_channels, mid_channels),\n",
    "            nn.Conv2d(mid_channels,\n",
    "                      depth_channels,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "        )\n",
    "        self.d_bound = d_bound\n",
    "        self.num_ranges = num_ranges\n",
    "\n",
    "    # @autocast(False)\n",
    "    def forward(self, x, mats_dict, scale_depth_factor=1000.0):\n",
    "        B, _, H, W = x.shape\n",
    "        intrins = mats_dict['intrin_mats'][:, 0:1, ..., :3, :3]\n",
    "        batch_size = intrins.shape[0]\n",
    "        num_cams = intrins.shape[2]\n",
    "        ida = mats_dict['ida_mats'][:, 0:1, ...]\n",
    "        sensor2ego = mats_dict['sensor2ego_mats'][:, 0:1, ..., :3, :]\n",
    "        bda = mats_dict['bda_mat'].view(batch_size, 1, 1, 4,\n",
    "                                        4).repeat(1, 1, num_cams, 1, 1)\n",
    "        mlp_input = torch.cat(\n",
    "            [\n",
    "                torch.stack(\n",
    "                    [\n",
    "                        intrins[:, 0:1, ..., 0, 0],\n",
    "                        intrins[:, 0:1, ..., 1, 1],\n",
    "                        intrins[:, 0:1, ..., 0, 2],\n",
    "                        intrins[:, 0:1, ..., 1, 2],\n",
    "                        ida[:, 0:1, ..., 0, 0],\n",
    "                        ida[:, 0:1, ..., 0, 1],\n",
    "                        ida[:, 0:1, ..., 0, 3],\n",
    "                        ida[:, 0:1, ..., 1, 0],\n",
    "                        ida[:, 0:1, ..., 1, 1],\n",
    "                        ida[:, 0:1, ..., 1, 3],\n",
    "                        bda[:, 0:1, ..., 0, 0],\n",
    "                        bda[:, 0:1, ..., 0, 1],\n",
    "                        bda[:, 0:1, ..., 1, 0],\n",
    "                        bda[:, 0:1, ..., 1, 1],\n",
    "                        bda[:, 0:1, ..., 2, 2],\n",
    "                    ],\n",
    "                    dim=-1,\n",
    "                ),\n",
    "                sensor2ego.view(batch_size, 1, num_cams, -1),\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        mlp_input = self.bn(mlp_input.reshape(-1, mlp_input.shape[-1]))\n",
    "        x = self.reduce_conv(x)\n",
    "        context_se = self.context_mlp(mlp_input)[..., None, None]\n",
    "        context = self.context_se(x, context_se)\n",
    "        context = self.context_conv(context)\n",
    "        depth_se = self.depth_mlp(mlp_input)[..., None, None]\n",
    "        depth_feat = self.depth_se(x, depth_se)\n",
    "        depth_feat = self.depth_feat_conv(depth_feat)\n",
    "        mono_depth = self.mono_depth_net(depth_feat)\n",
    "        mu_sigma_score = self.mu_sigma_range_net(depth_feat)\n",
    "        d_coords = torch.arange(*self.d_bound,\n",
    "                                dtype=torch.float).reshape(1, -1, 1, 1).cuda()\n",
    "        d_coords = d_coords.repeat(B, 1, H, W)\n",
    "        mu = mu_sigma_score[:, 0:self.num_ranges, ...]\n",
    "        sigma = mu_sigma_score[:, self.num_ranges:2 * self.num_ranges, ...]\n",
    "        range_score = mu_sigma_score[:,\n",
    "                                     2 * self.num_ranges:3 * self.num_ranges,\n",
    "                                     ...]\n",
    "        sigma = F.elu(sigma) + 1.0 + 1e-10\n",
    "        return x, context, mu, sigma, range_score, mono_depth\n",
    "\n",
    "\n",
    "class BEVStereoLSSFPN(BaseLSSFPN):\n",
    "\n",
    "    def __init__(self,\n",
    "                 x_bound,\n",
    "                 y_bound,\n",
    "                 z_bound,\n",
    "                 d_bound,\n",
    "                 final_dim,\n",
    "                 downsample_factor,\n",
    "                 output_channels,\n",
    "                 img_backbone_conf,\n",
    "                 img_neck_conf,\n",
    "                 depth_net_conf,\n",
    "                 use_da=False,\n",
    "                 sampling_range=3,\n",
    "                 num_samples=3,\n",
    "                 stereo_downsample_factor=4,\n",
    "                 em_iteration=3,\n",
    "                 min_sigma=1,\n",
    "                 num_groups=8,\n",
    "                 num_ranges=4,\n",
    "                 range_list=[[2, 8], [8, 16], [16, 28], [28, 58]],\n",
    "                 k_list=None,\n",
    "                 use_mask=True):\n",
    "        \"\"\"Modified from `https://github.com/nv-tlabs/lift-splat-shoot`.\n",
    "        Args:\n",
    "            x_bound (list): Boundaries for x.\n",
    "            y_bound (list): Boundaries for y.\n",
    "            z_bound (list): Boundaries for z.\n",
    "            d_bound (list): Boundaries for d.\n",
    "            final_dim (list): Dimension for input images.\n",
    "            downsample_factor (int): Downsample factor between feature map\n",
    "                and input image.\n",
    "            output_channels (int): Number of channels for the output\n",
    "                feature map.\n",
    "            img_backbone_conf (dict): Config for image backbone.\n",
    "            img_neck_conf (dict): Config for image neck.\n",
    "            depth_net_conf (dict): Config for depth net.\n",
    "            sampling_range (int): The base range of sampling candidates.\n",
    "                Defaults to 3.\n",
    "            num_samples (int): Number of samples. Defaults to 3.\n",
    "            stereo_downsample_factor (int): Downsample factor from input image\n",
    "                and stereo depth. Defaults to 4.\n",
    "            em_iteration (int): Number of iterations for em. Defaults to 3.\n",
    "            min_sigma (float): Minimal value for sigma. Defaults to 1.\n",
    "            num_groups (int): Number of groups to keep after inner product.\n",
    "                Defaults to 8.\n",
    "            num_ranges (int): Number of split ranges. Defaults to 1.\n",
    "            range_list (list): Start and end of every range, Defaults to None.\n",
    "            k_list (list): Depth of all candidates inside the range.\n",
    "                Defaults to None.\n",
    "            use_mask (bool): Whether to use mask_net. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.num_ranges = num_ranges\n",
    "        self.sampling_range = sampling_range\n",
    "        self.num_samples = num_samples\n",
    "        super(BEVStereoLSSFPN,\n",
    "              self).__init__(x_bound, y_bound, z_bound, d_bound, final_dim,\n",
    "                             downsample_factor, output_channels,\n",
    "                             img_backbone_conf, img_neck_conf, depth_net_conf,\n",
    "                             use_da)\n",
    "\n",
    "        self.depth_channels, _, _, _ = self.frustum.shape\n",
    "        self.use_mask = use_mask\n",
    "        if k_list is None:\n",
    "            self.register_buffer('k_list', torch.Tensor(self.depth_sampling()))\n",
    "        else:\n",
    "            self.register_buffer('k_list', torch.Tensor(k_list))\n",
    "        self.stereo_downsample_factor = stereo_downsample_factor\n",
    "        self.em_iteration = em_iteration\n",
    "        self.register_buffer(\n",
    "            'depth_values',\n",
    "            torch.arange((self.d_bound[1] - self.d_bound[0]) / self.d_bound[2],\n",
    "                         dtype=torch.float))\n",
    "        self.num_groups = num_groups\n",
    "        self.similarity_net = nn.Sequential(\n",
    "            ConvBnReLU3D(in_channels=num_groups,\n",
    "                         out_channels=16,\n",
    "                         kernel_size=1,\n",
    "                         stride=1,\n",
    "                         pad=0),\n",
    "            ConvBnReLU3D(in_channels=16,\n",
    "                         out_channels=8,\n",
    "                         kernel_size=1,\n",
    "                         stride=1,\n",
    "                         pad=0),\n",
    "            nn.Conv3d(in_channels=8,\n",
    "                      out_channels=1,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "        )\n",
    "        if range_list is None:\n",
    "            range_length = (d_bound[1] - d_bound[0]) / num_ranges\n",
    "            self.range_list = [[\n",
    "                d_bound[0] + range_length * i,\n",
    "                d_bound[0] + range_length * (i + 1)\n",
    "            ] for i in range(num_ranges)]\n",
    "        else:\n",
    "            assert len(range_list) == num_ranges\n",
    "            self.range_list = range_list\n",
    "\n",
    "        self.min_sigma = min_sigma\n",
    "        self.depth_downsample_net = nn.Sequential(\n",
    "            nn.Conv2d(self.depth_channels, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, self.depth_channels, 1, 1, 0),\n",
    "        )\n",
    "        self.context_downsample_net = nn.Identity()\n",
    "        if self.use_mask:\n",
    "            self.mask_net = nn.Sequential(\n",
    "                nn.Conv2d(224, 64, 3, 1, 1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                BasicBlock(64, 64),\n",
    "                BasicBlock(64, 64),\n",
    "                nn.Conv2d(64, 1, 1, 1, 0),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def depth_sampling(self):\n",
    "        \"\"\"Generate sampling range of candidates.\n",
    "        Returns:\n",
    "            list[float]: List of all candidates.\n",
    "        \"\"\"\n",
    "        P_total = erf(self.sampling_range /\n",
    "                      np.sqrt(2))  # Probability covered by the sampling range\n",
    "        idx_list = np.arange(0, self.num_samples + 1)\n",
    "        p_list = (1 - P_total) / 2 + ((idx_list / self.num_samples) * P_total)\n",
    "        k_list = norm.ppf(p_list)\n",
    "        k_list = (k_list[1:] + k_list[:-1]) / 2\n",
    "        return list(k_list)\n",
    "\n",
    "    def _generate_cost_volume(\n",
    "        self,\n",
    "        sweep_index,\n",
    "        stereo_feats_all_sweeps,\n",
    "        mats_dict,\n",
    "        depth_sample,\n",
    "        depth_sample_frustum,\n",
    "        sensor2sensor_mats,\n",
    "    ):\n",
    "        \"\"\"Generate cost volume based on depth sample.\n",
    "        Args:\n",
    "            sweep_index (int): Index of sweep.\n",
    "            stereo_feats_all_sweeps (list[Tensor]): Stereo feature\n",
    "                of all sweeps.\n",
    "            mats_dict (dict):\n",
    "                sensor2ego_mats (Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats (Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats (Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats (Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat (Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            depth_sample (Tensor): Depth map of all candidates.\n",
    "            depth_sample_frustum (Tensor): Pre-generated frustum.\n",
    "            sensor2sensor_mats (Tensor): Transformation matrix from reference\n",
    "                sensor to source sensor.\n",
    "        Returns:\n",
    "            Tensor: Depth score for all sweeps.\n",
    "        \"\"\"\n",
    "        batch_size, num_channels, height, width = stereo_feats_all_sweeps[0].shape\n",
    "        num_sweeps = len(stereo_feats_all_sweeps)\n",
    "        depth_score_all_sweeps = list()\n",
    "        for idx in range(num_sweeps):\n",
    "            if idx == sweep_index:\n",
    "                continue\n",
    "            warped_stereo_fea = self.homo_warping(\n",
    "                stereo_feats_all_sweeps[idx],\n",
    "                mats_dict['intrin_mats'][:, sweep_index, ...],\n",
    "                mats_dict['intrin_mats'][:, idx, ...],\n",
    "                sensor2sensor_mats[idx],\n",
    "                mats_dict['ida_mats'][:, sweep_index, ...],\n",
    "                mats_dict['ida_mats'][:, idx, ...],\n",
    "                depth_sample,\n",
    "                depth_sample_frustum.type_as(stereo_feats_all_sweeps[idx]),\n",
    "            )\n",
    "            warped_stereo_fea = warped_stereo_fea.reshape(\n",
    "                batch_size, self.num_groups, num_channels // self.num_groups,\n",
    "                self.num_samples, height, width)\n",
    "            ref_stereo_feat = stereo_feats_all_sweeps[sweep_index].reshape(\n",
    "                batch_size, self.num_groups, num_channels // self.num_groups,\n",
    "                height, width)\n",
    "            feat_cost = torch.mean(\n",
    "                (ref_stereo_feat.unsqueeze(3) * warped_stereo_fea), axis=2)\n",
    "            depth_score = self.similarity_net(feat_cost).squeeze(1)\n",
    "            depth_score_all_sweeps.append(depth_score)\n",
    "        return torch.stack(depth_score_all_sweeps).mean(0)\n",
    "\n",
    "    def homo_warping(\n",
    "        self,\n",
    "        stereo_feat,\n",
    "        key_intrin_mats,\n",
    "        sweep_intrin_mats,\n",
    "        sensor2sensor_mats,\n",
    "        key_ida_mats,\n",
    "        sweep_ida_mats,\n",
    "        depth_sample,\n",
    "        frustum,\n",
    "    ):\n",
    "        \"\"\"Used for mvs method to transfer sweep image feature to\n",
    "            key image feature.\n",
    "        Args:\n",
    "            src_fea(Tensor): image features.\n",
    "            key_intrin_mats(Tensor): Intrin matrix for key sensor.\n",
    "            sweep_intrin_mats(Tensor): Intrin matrix for sweep sensor.\n",
    "            sensor2sensor_mats(Tensor): Transformation matrix from key\n",
    "                sensor to sweep sensor.\n",
    "            key_ida_mats(Tensor): Ida matrix for key frame.\n",
    "            sweep_ida_mats(Tensor): Ida matrix for sweep frame.\n",
    "            depth_sample (Tensor): Depth map of all candidates.\n",
    "            depth_sample_frustum (Tensor): Pre-generated frustum.\n",
    "        \"\"\"\n",
    "        batch_size_with_num_cams, channels = stereo_feat.shape[\n",
    "            0], stereo_feat.shape[1]\n",
    "        height, width = stereo_feat.shape[2], stereo_feat.shape[3]\n",
    "        with torch.no_grad():\n",
    "            points = frustum\n",
    "            points = points.reshape(points.shape[0], -1, points.shape[-1])\n",
    "            points[..., 2] = 1\n",
    "            # Undo ida for key frame.\n",
    "            points = key_ida_mats.reshape(batch_size_with_num_cams, *\n",
    "                                          key_ida_mats.shape[2:]).inverse(\n",
    "                                          ).unsqueeze(1) @ points.unsqueeze(-1)\n",
    "            # Convert points from pixel coord to key camera coord.\n",
    "            points[..., :3, :] *= depth_sample.reshape(\n",
    "                batch_size_with_num_cams, -1, 1, 1)\n",
    "            num_depth = frustum.shape[1]\n",
    "            points = (key_intrin_mats.reshape(\n",
    "                batch_size_with_num_cams, *\n",
    "                key_intrin_mats.shape[2:]).inverse().unsqueeze(1) @ points)\n",
    "            points = (sensor2sensor_mats.reshape(\n",
    "                batch_size_with_num_cams, *\n",
    "                sensor2sensor_mats.shape[2:]).unsqueeze(1) @ points)\n",
    "            # points in sweep sensor coord.\n",
    "            points = (sweep_intrin_mats.reshape(\n",
    "                batch_size_with_num_cams, *\n",
    "                sweep_intrin_mats.shape[2:]).unsqueeze(1) @ points)\n",
    "            # points in sweep pixel coord.\n",
    "            points[..., :2, :] = points[..., :2, :] / points[\n",
    "                ..., 2:3, :]  # [B, 2, Ndepth, H*W]\n",
    "\n",
    "            points = (sweep_ida_mats.reshape(\n",
    "                batch_size_with_num_cams, *\n",
    "                sweep_ida_mats.shape[2:]).unsqueeze(1) @ points).squeeze(-1)\n",
    "            neg_mask = points[..., 2] < 1e-3\n",
    "            points[..., 0][neg_mask] = width * self.stereo_downsample_factor\n",
    "            points[..., 1][neg_mask] = height * self.stereo_downsample_factor\n",
    "            points[..., 2][neg_mask] = 1\n",
    "            proj_x_normalized = points[..., 0] / (\n",
    "                (width * self.stereo_downsample_factor - 1) / 2) - 1\n",
    "            proj_y_normalized = points[..., 1] / (\n",
    "                (height * self.stereo_downsample_factor - 1) / 2) - 1\n",
    "            grid = torch.stack([proj_x_normalized, proj_y_normalized],\n",
    "                               dim=2)  # [B, Ndepth, H*W, 2]\n",
    "\n",
    "        warped_stereo_fea = F.grid_sample(\n",
    "            stereo_feat,\n",
    "            grid.view(batch_size_with_num_cams, num_depth * height, width, 2),\n",
    "            mode='bilinear',\n",
    "            padding_mode='zeros',\n",
    "        )\n",
    "        warped_stereo_fea = warped_stereo_fea.view(batch_size_with_num_cams,\n",
    "                                                   channels, num_depth, height,\n",
    "                                                   width)\n",
    "\n",
    "        return warped_stereo_fea\n",
    "\n",
    "    def _forward_stereo(\n",
    "        self,\n",
    "        sweep_index,\n",
    "        stereo_feats_all_sweeps,\n",
    "        mono_depth_all_sweeps,\n",
    "        mats_dict,\n",
    "        sensor2sensor_mats,\n",
    "        mu_all_sweeps,\n",
    "        sigma_all_sweeps,\n",
    "        range_score_all_sweeps,\n",
    "        depth_feat_all_sweeps,\n",
    "    ):\n",
    "        \"\"\"Forward function to generate stereo depth.\n",
    "        Args:\n",
    "            sweep_index (int): Index of sweep.\n",
    "            stereo_feats_all_sweeps (list[Tensor]): Stereo feature of all sweeps.\n",
    "            mono_depth_all_sweeps (list[Tensor]):\n",
    "            mats_dict (dict):\n",
    "                sensor2ego_mats (Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats (Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats (Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats (Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat (Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            sensor2sensor_mats(Tensor): Transformation matrix from key\n",
    "                sensor to sweep sensor.\n",
    "            mu_all_sweeps (list[Tensor]): List of mu for all sweeps.\n",
    "            sigma_all_sweeps (list[Tensor]): List of sigma for all sweeps.\n",
    "            range_score_all_sweeps (list[Tensor]): List of all range score\n",
    "                for all sweeps.\n",
    "            depth_feat_all_sweeps (list[Tensor]): List of all depth feat for\n",
    "                all sweeps.\n",
    "        Returns:\n",
    "            Tensor: stereo_depth\n",
    "        \"\"\"\n",
    "        batch_size_with_cams, _, feat_height, feat_width = stereo_feats_all_sweeps[0].shape\n",
    "        device = stereo_feats_all_sweeps[0].device\n",
    "        d_coords = torch.arange(*self.d_bound,\n",
    "                                dtype=torch.float,\n",
    "                                device=device).reshape(1, -1, 1, 1)\n",
    "        d_coords = d_coords.repeat(batch_size_with_cams, 1, feat_height,\n",
    "                                   feat_width)\n",
    "        stereo_depth = stereo_feats_all_sweeps[0].new_zeros(\n",
    "            batch_size_with_cams, self.depth_channels, feat_height, feat_width)\n",
    "        mask_score = stereo_feats_all_sweeps[0].new_zeros(\n",
    "            batch_size_with_cams,\n",
    "            self.depth_channels,\n",
    "            feat_height * self.stereo_downsample_factor //\n",
    "            self.downsample_factor,\n",
    "            feat_width * self.stereo_downsample_factor //\n",
    "            self.downsample_factor,\n",
    "        )\n",
    "        score_all_ranges = list()\n",
    "        range_score = range_score_all_sweeps[sweep_index].softmax(1)\n",
    "        \n",
    "        for range_idx in range(self.num_ranges):\n",
    "            # Map mu to the corresponding interval.\n",
    "            range_start = self.range_list[range_idx][0]\n",
    "            mu_all_sweeps_single_range = [\n",
    "                mu[:, range_idx:range_idx + 1, ...].sigmoid() *\n",
    "                (self.range_list[range_idx][1] - self.range_list[range_idx][0])\n",
    "                + range_start for mu in mu_all_sweeps\n",
    "            ]\n",
    "            sigma_all_sweeps_single_range = [\n",
    "                sigma[:, range_idx:range_idx + 1, ...]\n",
    "                for sigma in sigma_all_sweeps\n",
    "            ]\n",
    "            batch_size_with_cams, _, feat_height, feat_width = stereo_feats_all_sweeps[0].shape\n",
    "            mu = mu_all_sweeps_single_range[sweep_index]\n",
    "            sigma = sigma_all_sweeps_single_range[sweep_index]\n",
    "            for _ in range(self.em_iteration):\n",
    "                depth_sample = torch.cat([mu + sigma * k for k in self.k_list], 1)\n",
    "                depth_sample_frustum = self.create_depth_sample_frustum(depth_sample, self.stereo_downsample_factor)\n",
    "                \n",
    "                \"\"\"\n",
    "                cost volumn  stereo features depth sample (mu sigma   ) input \n",
    "                mu sigma  source image features  ? YES!\n",
    "                \n",
    "                BEVStereo key idea cost volumn    depth samples (depth map  pixels)   ,\n",
    "                 depth samples (gen from mu and sigma) sparse cost volumn !\n",
    "                \"\"\"\n",
    "                \n",
    "                mu_score = self._generate_cost_volume(\n",
    "                    sweep_index,\n",
    "                    stereo_feats_all_sweeps,\n",
    "                    mats_dict,\n",
    "                    depth_sample,\n",
    "                    depth_sample_frustum,\n",
    "                    sensor2sensor_mats,\n",
    "                )\n",
    "                mu_score = mu_score.softmax(1)\n",
    "                scale_factor = torch.clamp(\n",
    "                    0.5 / (1e-4 + mu_score[:, self.num_samples //\n",
    "                                           2:self.num_samples // 2 + 1, ...]),\n",
    "                    min=0.1,\n",
    "                    max=10)\n",
    "\n",
    "                sigma = torch.clamp(sigma * scale_factor, min=0.1, max=10)\n",
    "                mu = (depth_sample * mu_score).sum(1, keepdim=True)\n",
    "                del depth_sample\n",
    "                del depth_sample_frustum\n",
    "            range_length = int((self.range_list[range_idx][1] - self.range_list[range_idx][0]) // self.d_bound[2])\n",
    "            if self.use_mask:\n",
    "                depth_sample = F.avg_pool2d(\n",
    "                    mu,\n",
    "                    self.downsample_factor // self.stereo_downsample_factor,\n",
    "                    self.downsample_factor // self.stereo_downsample_factor,\n",
    "                )\n",
    "                depth_sample_frustum = self.create_depth_sample_frustum(depth_sample, self.downsample_factor)\n",
    "                mask = self._forward_mask(\n",
    "                    sweep_index,\n",
    "                    mono_depth_all_sweeps,\n",
    "                    mats_dict,\n",
    "                    depth_sample,\n",
    "                    depth_sample_frustum,\n",
    "                    sensor2sensor_mats,\n",
    "                )\n",
    "                mask_score[:,\n",
    "                           int((range_start - self.d_bound[0]) //\n",
    "                               self.d_bound[2]):range_length +\n",
    "                           int((range_start - self.d_bound[0]) //\n",
    "                               self.d_bound[2]), ..., ] += mask\n",
    "                del depth_sample\n",
    "                del depth_sample_frustum\n",
    "            sigma = torch.clamp(sigma, self.min_sigma)\n",
    "            mu_repeated = mu.repeat(1, range_length, 1, 1)\n",
    "            eps = 1e-6\n",
    "            depth_score_single_range = (-1 / 2 * (\n",
    "                (d_coords[:,\n",
    "                          int((range_start - self.d_bound[0]) //\n",
    "                              self.d_bound[2]):range_length + int(\n",
    "                                  (range_start - self.d_bound[0]) //\n",
    "                                  self.d_bound[2]), ..., ] - mu_repeated) /\n",
    "                torch.sqrt(sigma))**2)\n",
    "            depth_score_single_range = depth_score_single_range.exp()\n",
    "            score_all_ranges.append(mu_score.sum(1).unsqueeze(1))\n",
    "            depth_score_single_range = depth_score_single_range / (sigma * math.sqrt(2 * math.pi) + eps)\n",
    "            stereo_depth[:,\n",
    "                         int((range_start - self.d_bound[0]) //\n",
    "                             self.d_bound[2]):range_length +\n",
    "                         int((range_start - self.d_bound[0]) //\n",
    "                             self.d_bound[2]), ..., ] = (\n",
    "                                 depth_score_single_range *\n",
    "                                 range_score[:, range_idx:range_idx + 1, ...])\n",
    "            del depth_score_single_range\n",
    "            del mu_repeated\n",
    "        if self.use_mask:\n",
    "            return stereo_depth, mask_score\n",
    "        else:\n",
    "            return stereo_depth\n",
    "\n",
    "    def create_depth_sample_frustum(self, depth_sample, downsample_factor=16):\n",
    "        \"\"\"Generate frustum\"\"\"\n",
    "        # make grid in image plane\n",
    "        ogfH, ogfW = self.final_dim\n",
    "        fH, fW = ogfH // downsample_factor, ogfW // downsample_factor\n",
    "        batch_size, num_depth, _, _ = depth_sample.shape\n",
    "        x_coords = (torch.linspace(0,\n",
    "                                   ogfW - 1,\n",
    "                                   fW,\n",
    "                                   dtype=torch.float,\n",
    "                                   device=depth_sample.device).view(1, 1, 1, fW).expand(batch_size, num_depth, fH, fW))\n",
    "        y_coords = (torch.linspace(0,\n",
    "                                   ogfH - 1,\n",
    "                                   fH,\n",
    "                                   dtype=torch.float,\n",
    "                                   device=depth_sample.device).view(1, 1, fH, 1).expand(batch_size, num_depth, fH, fW))\n",
    "        paddings = torch.ones_like(depth_sample)\n",
    "\n",
    "        # D x H x W x 3\n",
    "        frustum = torch.stack((x_coords, y_coords, depth_sample, paddings), -1)\n",
    "        return frustum\n",
    "\n",
    "    def _configure_depth_net(self, depth_net_conf):\n",
    "        return DepthNet(\n",
    "            depth_net_conf['in_channels'],\n",
    "            depth_net_conf['mid_channels'],\n",
    "            self.output_channels,\n",
    "            self.depth_channels,\n",
    "            self.d_bound,\n",
    "            self.num_ranges,\n",
    "        )\n",
    "\n",
    "    def get_cam_feats(self, imgs):\n",
    "        \"\"\"Get feature maps from images.\"\"\"\n",
    "        batch_size, num_sweeps, num_cams, num_channels, imH, imW = imgs.shape\n",
    "\n",
    "        imgs = imgs.flatten().view(batch_size * num_sweeps * num_cams,\n",
    "                                   num_channels, imH, imW)\n",
    "        backbone_feats = self.img_backbone(imgs)\n",
    "        img_feats = self.img_neck(backbone_feats)[0]\n",
    "        img_feats_reshape = img_feats.reshape(batch_size, num_sweeps, num_cams,\n",
    "                                              img_feats.shape[1],\n",
    "                                              img_feats.shape[2],\n",
    "                                              img_feats.shape[3])\n",
    "        return img_feats_reshape, backbone_feats[0].detach()\n",
    "\n",
    "    def _forward_mask(\n",
    "        self,\n",
    "        sweep_index,\n",
    "        mono_depth_all_sweeps,\n",
    "        mats_dict,\n",
    "        depth_sample,\n",
    "        depth_sample_frustum,\n",
    "        sensor2sensor_mats,\n",
    "    ):\n",
    "        \"\"\"Forward function to generate mask.\n",
    "        Args:\n",
    "            sweep_index (int): Index of sweep.\n",
    "            mono_depth_all_sweeps (list[Tensor]): List of mono_depth for\n",
    "                all sweeps.\n",
    "            mats_dict (dict):\n",
    "                sensor2ego_mats (Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats (Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats (Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats (Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat (Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            depth_sample (Tensor): Depth map of all candidates.\n",
    "            depth_sample_frustum (Tensor): Pre-generated frustum.\n",
    "            sensor2sensor_mats (Tensor): Transformation matrix from reference\n",
    "                sensor to source sensor.\n",
    "        Returns:\n",
    "            Tensor: Generated mask.\n",
    "        \"\"\"\n",
    "        num_sweeps = len(mono_depth_all_sweeps)\n",
    "        mask_all_sweeps = list()\n",
    "        for idx in range(num_sweeps):\n",
    "            if idx == sweep_index:\n",
    "                continue\n",
    "            warped_mono_depth = self.homo_warping(\n",
    "                mono_depth_all_sweeps[idx],\n",
    "                mats_dict['intrin_mats'][:, sweep_index, ...],\n",
    "                mats_dict['intrin_mats'][:, idx, ...],\n",
    "                sensor2sensor_mats[idx],\n",
    "                mats_dict['ida_mats'][:, sweep_index, ...],\n",
    "                mats_dict['ida_mats'][:, idx, ...],\n",
    "                depth_sample,\n",
    "                depth_sample_frustum.type_as(mono_depth_all_sweeps[idx]),\n",
    "            )\n",
    "            mask = self.mask_net(\n",
    "                torch.cat([\n",
    "                    mono_depth_all_sweeps[sweep_index].detach(),\n",
    "                    warped_mono_depth.mean(2).detach()\n",
    "                ], 1))\n",
    "            mask_all_sweeps.append(mask)\n",
    "        return torch.stack(mask_all_sweeps).mean(0)\n",
    "\n",
    "    def _forward_single_sweep(self,\n",
    "                              sweep_index,\n",
    "                              context,\n",
    "                              mats_dict,\n",
    "                              depth_score,\n",
    "                              is_return_depth=False):\n",
    "        \"\"\"Forward function for single sweep.\n",
    "        Args:\n",
    "            sweep_index (int): Index of sweeps.\n",
    "            sweep_imgs (Tensor): Input images.\n",
    "            mats_dict (dict):\n",
    "                sensor2ego_mats(Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats(Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats(Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats(Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat(Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            is_return_depth (bool, optional): Whether to return depth.\n",
    "                Default: False.\n",
    "        Returns:\n",
    "            Tensor: BEV feature map.\n",
    "        \"\"\"\n",
    "        batch_size, num_cams = context.shape[0], context.shape[1]\n",
    "        context = context.reshape(batch_size * num_cams, *context.shape[2:])\n",
    "        depth = depth_score\n",
    "        img_feat_with_depth = depth.unsqueeze(1) * context.unsqueeze(2)\n",
    "\n",
    "        img_feat_with_depth = self._forward_voxel_net(img_feat_with_depth)\n",
    "\n",
    "        img_feat_with_depth = img_feat_with_depth.reshape(\n",
    "            batch_size,\n",
    "            num_cams,\n",
    "            img_feat_with_depth.shape[1],\n",
    "            img_feat_with_depth.shape[2],\n",
    "            img_feat_with_depth.shape[3],\n",
    "            img_feat_with_depth.shape[4],\n",
    "        )\n",
    "        geom_xyz = self.get_geometry(\n",
    "            mats_dict['sensor2ego_mats'][:, sweep_index, ...],\n",
    "            mats_dict['intrin_mats'][:, sweep_index, ...],\n",
    "            mats_dict['ida_mats'][:, sweep_index, ...],\n",
    "            mats_dict.get('bda_mat', None),\n",
    "        )\n",
    "        img_feat_with_depth = img_feat_with_depth.permute(0, 1, 3, 4, 5, 2)\n",
    "        geom_xyz = ((geom_xyz - (self.voxel_coord - self.voxel_size / 2.0)) / self.voxel_size).int()\n",
    "        feature_map = voxel_pooling(geom_xyz,\n",
    "                                    img_feat_with_depth.contiguous().float(),\n",
    "                                    self.voxel_num.cuda())\n",
    "        if is_return_depth:\n",
    "            return feature_map.contiguous(), depth\n",
    "        return feature_map.contiguous()\n",
    "\n",
    "    def forward(self,\n",
    "                sweep_imgs,\n",
    "                mats_dict,\n",
    "                timestamps=None,\n",
    "                is_return_depth=False):\n",
    "        \"\"\"Forward function.\n",
    "        Args:\n",
    "            sweep_imgs(Tensor): Input images with shape of (B, num_sweeps,\n",
    "                num_cameras, 3, H, W).\n",
    "            mats_dict(dict):\n",
    "                sensor2ego_mats(Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats(Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats(Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats(Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat(Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            timestamps(Tensor): Timestamp for all images with the shape of(B,\n",
    "                num_sweeps, num_cameras).\n",
    "        Return:\n",
    "            Tensor: bev feature map.\n",
    "        \"\"\"\n",
    "        batch_size, num_sweeps, num_cams, num_channels, img_height, img_width = sweep_imgs.shape\n",
    "        context_all_sweeps = list()\n",
    "        depth_feat_all_sweeps = list()\n",
    "        img_feats_all_sweeps = list()\n",
    "        stereo_feats_all_sweeps = list()\n",
    "        mu_all_sweeps = list()\n",
    "        sigma_all_sweeps = list()\n",
    "        mono_depth_all_sweeps = list()\n",
    "        range_score_all_sweeps = list()\n",
    "        \n",
    "        \"\"\"Mono depth map (mu, sigma ) image features \"\"\"\n",
    "        for sweep_index in range(0, num_sweeps):\n",
    "            if sweep_index > 0:\n",
    "                with torch.no_grad():\n",
    "                    img_feats, stereo_feats = self.get_cam_feats(sweep_imgs[:, sweep_index:sweep_index + 1, ...])\n",
    "                    \n",
    "                    img_feats_all_sweeps.append(img_feats.view(batch_size * num_cams, *img_feats.shape[3:]))\n",
    "                    stereo_feats_all_sweeps.append(stereo_feats)\n",
    "                    \n",
    "                    depth_feat, context, mu, sigma, range_score, mono_depth =\\\n",
    "                        self.depth_net(img_feats.view(batch_size * num_cams, *img_feats.shape[3:]), mats_dict)\n",
    "                    \n",
    "                    context_all_sweeps.append(\n",
    "                        self.context_downsample_net(\n",
    "                            context.reshape(batch_size * num_cams, *context.shape[1:])))\n",
    "                    depth_feat_all_sweeps.append(depth_feat)\n",
    "            else:\n",
    "                img_feats, stereo_feats = self.get_cam_feats(sweep_imgs[:, sweep_index:sweep_index + 1, ...])\n",
    "                \n",
    "                img_feats_all_sweeps.append(img_feats.view(batch_size * num_cams, *img_feats.shape[3:]))\n",
    "                stereo_feats_all_sweeps.append(stereo_feats)\n",
    "                \n",
    "                depth_feat, context, mu, sigma, range_score, mono_depth =\\\n",
    "                    self.depth_net(img_feats.view(batch_size * num_cams, *img_feats.shape[3:]), mats_dict)\n",
    "                \n",
    "                context_all_sweeps.append(\n",
    "                    self.context_downsample_net(\n",
    "                        context.reshape(batch_size * num_cams, *context.shape[1:])))\n",
    "                depth_feat_all_sweeps.append(depth_feat)\n",
    "                \n",
    "            mu_all_sweeps.append(mu)\n",
    "            sigma_all_sweeps.append(sigma)\n",
    "            mono_depth_all_sweeps.append(mono_depth)\n",
    "            range_score_all_sweeps.append(range_score)\n",
    "        depth_score_all_sweeps = list()\n",
    "\n",
    "        \"\"\"Stereo depth  (_forward_stereo() function)\"\"\"\n",
    "        for ref_idx in range(num_sweeps):\n",
    "            sensor2sensor_mats = list()\n",
    "            for src_idx in range(num_sweeps):\n",
    "                ref2keysensor_mats = mats_dict['sensor2sensor_mats'][:, ref_idx, ...].inverse()\n",
    "                key2srcsensor_mats = mats_dict['sensor2sensor_mats'][:, src_idx, ...]\n",
    "                ref2srcsensor_mats = key2srcsensor_mats @ ref2keysensor_mats\n",
    "                sensor2sensor_mats.append(ref2srcsensor_mats)\n",
    "            if ref_idx == 0:\n",
    "                # last iteration on stage 1 does not have propagation\n",
    "                # (photometric consistency filtering)\n",
    "                if self.use_mask:\n",
    "                    stereo_depth, mask = self._forward_stereo(\n",
    "                        ref_idx,\n",
    "                        stereo_feats_all_sweeps,\n",
    "                        mono_depth_all_sweeps,\n",
    "                        mats_dict,\n",
    "                        sensor2sensor_mats,\n",
    "                        mu_all_sweeps,\n",
    "                        sigma_all_sweeps,\n",
    "                        range_score_all_sweeps,\n",
    "                        depth_feat_all_sweeps,\n",
    "                    )\n",
    "                else:\n",
    "                    stereo_depth = self._forward_stereo(\n",
    "                        ref_idx,\n",
    "                        stereo_feats_all_sweeps,\n",
    "                        mono_depth_all_sweeps,\n",
    "                        mats_dict,\n",
    "                        sensor2sensor_mats,\n",
    "                        mu_all_sweeps,\n",
    "                        sigma_all_sweeps,\n",
    "                        range_score_all_sweeps,\n",
    "                        depth_feat_all_sweeps,\n",
    "                    )\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    # last iteration on stage 1 does not have propagation (photometric consistency filtering)\n",
    "                    if self.use_mask:\n",
    "                        stereo_depth, mask = self._forward_stereo(\n",
    "                            ref_idx,\n",
    "                            stereo_feats_all_sweeps,\n",
    "                            mono_depth_all_sweeps,\n",
    "                            mats_dict,\n",
    "                            sensor2sensor_mats,\n",
    "                            mu_all_sweeps,\n",
    "                            sigma_all_sweeps,\n",
    "                            range_score_all_sweeps,\n",
    "                            depth_feat_all_sweeps,\n",
    "                        )\n",
    "                    else:\n",
    "                        stereo_depth = self._forward_stereo(\n",
    "                            ref_idx,\n",
    "                            stereo_feats_all_sweeps,\n",
    "                            mono_depth_all_sweeps,\n",
    "                            mats_dict,\n",
    "                            sensor2sensor_mats,\n",
    "                            mu_all_sweeps,\n",
    "                            sigma_all_sweeps,\n",
    "                            range_score_all_sweeps,\n",
    "                            depth_feat_all_sweeps,\n",
    "                        )\n",
    "            if self.use_mask:\n",
    "                depth_score = (\n",
    "                    mono_depth_all_sweeps[ref_idx] +\n",
    "                    self.depth_downsample_net(stereo_depth) * mask).softmax(1)\n",
    "            else:\n",
    "                depth_score = (\n",
    "                    mono_depth_all_sweeps[ref_idx] +\n",
    "                    self.depth_downsample_net(stereo_depth)).softmax(1)\n",
    "            depth_score_all_sweeps.append(depth_score)\n",
    "\n",
    "        key_frame_res = self._forward_single_sweep(\n",
    "            0,\n",
    "            context_all_sweeps[0].reshape(batch_size, num_cams, *context_all_sweeps[0].shape[1:]),\n",
    "            mats_dict,\n",
    "            depth_score_all_sweeps[0],\n",
    "            is_return_depth=is_return_depth,\n",
    "        )\n",
    "        if num_sweeps == 1:\n",
    "            return key_frame_res\n",
    "\n",
    "        key_frame_feature = key_frame_res[0] if is_return_depth else key_frame_res\n",
    "\n",
    "        ret_feature_list = [key_frame_feature]\n",
    "        for sweep_index in range(1, num_sweeps):\n",
    "            with torch.no_grad():\n",
    "                feature_map = self._forward_single_sweep(\n",
    "                    sweep_index,\n",
    "                    context_all_sweeps[sweep_index].reshape(batch_size, num_cams, *context_all_sweeps[sweep_index].shape[1:]),\n",
    "                    mats_dict,\n",
    "                    depth_score_all_sweeps[sweep_index],\n",
    "                    is_return_depth=False,\n",
    "                )\n",
    "                ret_feature_list.append(feature_map)\n",
    "\n",
    "        if is_return_depth:\n",
    "            return torch.cat(ret_feature_list, 1), depth_score_all_sweeps[0]\n",
    "        else:\n",
    "            return torch.cat(ret_feature_list, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824fd719-6505-40fd-aa6a-b2ebd0680a08",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BEVStereo Head (inherit from BEVDepth)\n",
    "from layers/heads/bev_depth_head.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edc55b94-0d3d-43ea-b896-2a45d1f90c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bev_backbone_conf = dict(\n",
    "    type='ResNet',\n",
    "    in_channels=80,\n",
    "    depth=18,\n",
    "    num_stages=3,\n",
    "    strides=(1, 2, 2),\n",
    "    dilations=(1, 1, 1),\n",
    "    out_indices=[0, 1, 2],\n",
    "    norm_eval=False,\n",
    "    base_channels=160,\n",
    ")\n",
    "\n",
    "bev_neck_conf = dict(type='SECONDFPN',\n",
    "                     in_channels=[160, 320, 640],\n",
    "                     upsample_strides=[2, 4, 8],\n",
    "                     out_channels=[64, 64, 128])\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def size_aware_circle_nms(dets, thresh_scale, post_max_size=83):\n",
    "    \"\"\"Circular NMS.\n",
    "    An object is only counted as positive if no other center\n",
    "    with a higher confidence exists within a radius r using a\n",
    "    bird-eye view distance metric.\n",
    "    Args:\n",
    "        dets (torch.Tensor): Detection results with the shape of [N, 3].\n",
    "        thresh (float): Value of threshold.\n",
    "        post_max_size (int): Max number of prediction to be kept. Defaults\n",
    "            to 83\n",
    "    Returns:\n",
    "        torch.Tensor: Indexes of the detections to be kept.\n",
    "    \"\"\"\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    dx1 = dets[:, 2]\n",
    "    dy1 = dets[:, 3]\n",
    "    yaws = dets[:, 4]\n",
    "    scores = dets[:, -1]\n",
    "    order = scores.argsort()[::-1].astype(np.int32)  # highest->lowest\n",
    "    ndets = dets.shape[0]\n",
    "    suppressed = np.zeros((ndets), dtype=np.int32)\n",
    "    keep = []\n",
    "    for _i in range(ndets):\n",
    "        i = order[_i]  # start with highest score box\n",
    "        if suppressed[\n",
    "                i] == 1:  # if any box have enough iou with this, remove it\n",
    "            continue\n",
    "        keep.append(i)\n",
    "        for _j in range(_i + 1, ndets):\n",
    "            j = order[_j]\n",
    "            if suppressed[j] == 1:\n",
    "                continue\n",
    "            # calculate center distance between i and j box\n",
    "            dist_x = abs(x1[i] - x1[j])\n",
    "            dist_y = abs(y1[i] - y1[j])\n",
    "            dist_x_th = (abs(dx1[i] * np.cos(yaws[i])) +\n",
    "                         abs(dx1[j] * np.cos(yaws[j])) +\n",
    "                         abs(dy1[i] * np.sin(yaws[i])) +\n",
    "                         abs(dy1[j] * np.sin(yaws[j])))\n",
    "            dist_y_th = (abs(dx1[i] * np.sin(yaws[i])) +\n",
    "                         abs(dx1[j] * np.sin(yaws[j])) +\n",
    "                         abs(dy1[i] * np.cos(yaws[i])) +\n",
    "                         abs(dy1[j] * np.cos(yaws[j])))\n",
    "            # ovr = inter / areas[j]\n",
    "            if dist_x <= dist_x_th * thresh_scale / 2 and \\\n",
    "               dist_y <= dist_y_th * thresh_scale / 2:\n",
    "                suppressed[j] = 1\n",
    "    return keep[:post_max_size]\n",
    "\n",
    "class BEVDepthHead(CenterHead):\n",
    "    \"\"\"Head for BevDepth.\n",
    "\n",
    "    Args:\n",
    "        in_channels(int): Number of channels after bev_neck.\n",
    "        tasks(dict): Tasks for head.\n",
    "        bbox_coder(dict): Config of bbox coder.\n",
    "        common_heads(dict): Config of head for each task.\n",
    "        loss_cls(dict): Config of classification loss.\n",
    "        loss_bbox(dict): Config of regression loss.\n",
    "        gaussian_overlap(float): Gaussian overlap used for `get_targets`.\n",
    "        min_radius(int): Min radius used for `get_targets`.\n",
    "        train_cfg(dict): Config used in the training process.\n",
    "        test_cfg(dict): Config used in the test process.\n",
    "        bev_backbone_conf(dict): Cnfig of bev_backbone.\n",
    "        bev_neck_conf(dict): Cnfig of bev_neck.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=256,\n",
    "        tasks=None,\n",
    "        bbox_coder=None,\n",
    "        common_heads=dict(),\n",
    "        loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),\n",
    "        loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),\n",
    "        gaussian_overlap=0.1,\n",
    "        min_radius=2,\n",
    "        train_cfg=None,\n",
    "        test_cfg=None,\n",
    "        bev_backbone_conf=bev_backbone_conf,\n",
    "        bev_neck_conf=bev_neck_conf,\n",
    "        separate_head=dict(type='SeparateHead',\n",
    "                           init_bias=-2.19,\n",
    "                           final_kernel=3),\n",
    "    ):\n",
    "        super(BEVDepthHead, self).__init__(\n",
    "            in_channels=in_channels,\n",
    "            tasks=tasks,\n",
    "            bbox_coder=bbox_coder,\n",
    "            common_heads=common_heads,\n",
    "            loss_cls=loss_cls,\n",
    "            loss_bbox=loss_bbox,\n",
    "            separate_head=separate_head,\n",
    "        )\n",
    "        self.trunk = build_backbone(bev_backbone_conf)\n",
    "        self.trunk.init_weights()\n",
    "        self.neck = build_neck(bev_neck_conf)\n",
    "        self.neck.init_weights()\n",
    "        del self.trunk.maxpool\n",
    "        self.gaussian_overlap = gaussian_overlap\n",
    "        self.min_radius = min_radius\n",
    "        self.train_cfg = train_cfg\n",
    "        self.test_cfg = test_cfg\n",
    "\n",
    "    @autocast(False)\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            feats (list[torch.Tensor]): Multi-level features, e.g.,\n",
    "                features produced by FPN.\n",
    "\n",
    "        Returns:\n",
    "            tuple(list[dict]): Output results for tasks.\n",
    "        \"\"\"\n",
    "        # FPN\n",
    "        trunk_outs = [x]\n",
    "        if self.trunk.deep_stem:\n",
    "            x = self.trunk.stem(x)\n",
    "        else:\n",
    "            x = self.trunk.conv1(x)\n",
    "            x = self.trunk.norm1(x)\n",
    "            x = self.trunk.relu(x)\n",
    "        for i, layer_name in enumerate(self.trunk.res_layers):\n",
    "            res_layer = getattr(self.trunk, layer_name)\n",
    "            x = res_layer(x)\n",
    "            if i in self.trunk.out_indices:\n",
    "                trunk_outs.append(x)\n",
    "        fpn_output = self.neck(trunk_outs)\n",
    "        ret_values = super().forward(fpn_output)\n",
    "        return ret_values\n",
    "\n",
    "    def get_targets_single(self, gt_bboxes_3d, gt_labels_3d):\n",
    "        \"\"\"Generate training targets for a single sample.\n",
    "\n",
    "        Args:\n",
    "            gt_bboxes_3d (:obj:`LiDARInstance3DBoxes`): Ground truth gt boxes.\n",
    "            gt_labels_3d (torch.Tensor): Labels of boxes.\n",
    "\n",
    "        Returns:\n",
    "            tuple[list[torch.Tensor]]: Tuple of target including \\\n",
    "                the following results in order.\n",
    "\n",
    "                - list[torch.Tensor]: Heatmap scores.\n",
    "                - list[torch.Tensor]: Ground truth boxes.\n",
    "                - list[torch.Tensor]: Indexes indicating the position \\\n",
    "                    of the valid boxes.\n",
    "                - list[torch.Tensor]: Masks indicating which boxes \\\n",
    "                    are valid.\n",
    "        \"\"\"\n",
    "        max_objs = self.train_cfg['max_objs'] * self.train_cfg['dense_reg']\n",
    "        grid_size = torch.tensor(self.train_cfg['grid_size'])\n",
    "        pc_range = torch.tensor(self.train_cfg['point_cloud_range'])\n",
    "        voxel_size = torch.tensor(self.train_cfg['voxel_size'])\n",
    "\n",
    "        feature_map_size = grid_size[:2] // self.train_cfg['out_size_factor']\n",
    "\n",
    "        # reorganize the gt_dict by tasks\n",
    "        task_masks = []\n",
    "        flag = 0\n",
    "        for class_name in self.class_names:\n",
    "            task_masks.append([\n",
    "                torch.where(gt_labels_3d == class_name.index(i) + flag)\n",
    "                for i in class_name\n",
    "            ])\n",
    "            flag += len(class_name)\n",
    "\n",
    "        task_boxes = []\n",
    "        task_classes = []\n",
    "        flag2 = 0\n",
    "        for idx, mask in enumerate(task_masks):\n",
    "            task_box = []\n",
    "            task_class = []\n",
    "            for m in mask:\n",
    "                task_box.append(gt_bboxes_3d[m])\n",
    "                # 0 is background for each task, so we need to add 1 here.\n",
    "                task_class.append(gt_labels_3d[m] + 1 - flag2)\n",
    "            task_boxes.append(\n",
    "                torch.cat(task_box, axis=0).to(gt_bboxes_3d.device))\n",
    "            task_classes.append(\n",
    "                torch.cat(task_class).long().to(gt_bboxes_3d.device))\n",
    "            flag2 += len(mask)\n",
    "        draw_gaussian = draw_heatmap_gaussian\n",
    "        heatmaps, anno_boxes, inds, masks = [], [], [], []\n",
    "\n",
    "        for idx, task_head in enumerate(self.task_heads):\n",
    "            heatmap = gt_bboxes_3d.new_zeros(\n",
    "                (len(self.class_names[idx]), feature_map_size[1],\n",
    "                 feature_map_size[0]),\n",
    "                device='cuda')\n",
    "\n",
    "            anno_box = gt_bboxes_3d.new_zeros(\n",
    "                (max_objs, len(self.train_cfg['code_weights'])),\n",
    "                dtype=torch.float32,\n",
    "                device='cuda')\n",
    "\n",
    "            ind = gt_labels_3d.new_zeros((max_objs),\n",
    "                                         dtype=torch.int64,\n",
    "                                         device='cuda')\n",
    "            mask = gt_bboxes_3d.new_zeros((max_objs),\n",
    "                                          dtype=torch.uint8,\n",
    "                                          device='cuda')\n",
    "\n",
    "            num_objs = min(task_boxes[idx].shape[0], max_objs)\n",
    "\n",
    "            for k in range(num_objs):\n",
    "                cls_id = task_classes[idx][k] - 1\n",
    "\n",
    "                width = task_boxes[idx][k][3]\n",
    "                length = task_boxes[idx][k][4]\n",
    "                width = width / voxel_size[0] / self.train_cfg[\n",
    "                    'out_size_factor']\n",
    "                length = length / voxel_size[1] / self.train_cfg[\n",
    "                    'out_size_factor']\n",
    "\n",
    "                if width > 0 and length > 0:\n",
    "                    radius = gaussian_radius(\n",
    "                        (length, width),\n",
    "                        min_overlap=self.train_cfg['gaussian_overlap'])\n",
    "                    radius = max(self.train_cfg['min_radius'], int(radius))\n",
    "\n",
    "                    # be really careful for the coordinate system of\n",
    "                    # your box annotation.\n",
    "                    x, y, z = task_boxes[idx][k][0], task_boxes[idx][k][\n",
    "                        1], task_boxes[idx][k][2]\n",
    "\n",
    "                    coor_x = (\n",
    "                        x - pc_range[0]\n",
    "                    ) / voxel_size[0] / self.train_cfg['out_size_factor']\n",
    "                    coor_y = (\n",
    "                        y - pc_range[1]\n",
    "                    ) / voxel_size[1] / self.train_cfg['out_size_factor']\n",
    "\n",
    "                    center = torch.tensor([coor_x, coor_y],\n",
    "                                          dtype=torch.float32,\n",
    "                                          device='cuda')\n",
    "                    center_int = center.to(torch.int32)\n",
    "\n",
    "                    # throw out not in range objects to avoid out of array\n",
    "                    # area when creating the heatmap\n",
    "                    if not (0 <= center_int[0] < feature_map_size[0]\n",
    "                            and 0 <= center_int[1] < feature_map_size[1]):\n",
    "                        continue\n",
    "\n",
    "                    draw_gaussian(heatmap[cls_id], center_int, radius)\n",
    "\n",
    "                    new_idx = k\n",
    "                    x, y = center_int[0], center_int[1]\n",
    "\n",
    "                    assert y * feature_map_size[0] + x < feature_map_size[\n",
    "                        0] * feature_map_size[1]\n",
    "\n",
    "                    ind[new_idx] = y * feature_map_size[0] + x\n",
    "                    mask[new_idx] = 1\n",
    "                    # TODO: support other outdoor dataset\n",
    "                    if len(task_boxes[idx][k]) > 7:\n",
    "                        vx, vy = task_boxes[idx][k][7:]\n",
    "                    rot = task_boxes[idx][k][6]\n",
    "                    box_dim = task_boxes[idx][k][3:6]\n",
    "                    if self.norm_bbox:\n",
    "                        box_dim = box_dim.log()\n",
    "                    if len(task_boxes[idx][k]) > 7:\n",
    "                        anno_box[new_idx] = torch.cat([\n",
    "                            center - torch.tensor([x, y], device='cuda'),\n",
    "                            z.unsqueeze(0),\n",
    "                            box_dim,\n",
    "                            torch.sin(rot).unsqueeze(0),\n",
    "                            torch.cos(rot).unsqueeze(0),\n",
    "                            vx.unsqueeze(0),\n",
    "                            vy.unsqueeze(0),\n",
    "                        ])\n",
    "                    else:\n",
    "                        anno_box[new_idx] = torch.cat([\n",
    "                            center - torch.tensor([x, y], device='cuda'),\n",
    "                            z.unsqueeze(0), box_dim,\n",
    "                            torch.sin(rot).unsqueeze(0),\n",
    "                            torch.cos(rot).unsqueeze(0)\n",
    "                        ])\n",
    "\n",
    "            heatmaps.append(heatmap)\n",
    "            anno_boxes.append(anno_box)\n",
    "            masks.append(mask)\n",
    "            inds.append(ind)\n",
    "        return heatmaps, anno_boxes, inds, masks\n",
    "\n",
    "    def loss(self, targets, preds_dicts, **kwargs):\n",
    "        \"\"\"Loss function for BEVDepthHead.\n",
    "\n",
    "        Args:\n",
    "            gt_bboxes_3d (list[:obj:`LiDARInstance3DBoxes`]): Ground\n",
    "                truth gt boxes.\n",
    "            gt_labels_3d (list[torch.Tensor]): Labels of boxes.\n",
    "            preds_dicts (dict): Output of forward function.\n",
    "\n",
    "        Returns:\n",
    "            dict[str:torch.Tensor]: Loss of heatmap and bbox of each task.\n",
    "        \"\"\"\n",
    "        heatmaps, anno_boxes, inds, masks = targets\n",
    "        return_loss = 0\n",
    "        for task_id, preds_dict in enumerate(preds_dicts):\n",
    "            # heatmap focal loss\n",
    "            preds_dict[0]['heatmap'] = clip_sigmoid(preds_dict[0]['heatmap'])\n",
    "            num_pos = heatmaps[task_id].eq(1).float().sum().item()\n",
    "            cls_avg_factor = torch.clamp(reduce_mean(\n",
    "                heatmaps[task_id].new_tensor(num_pos)),\n",
    "                                         min=1).item()\n",
    "            loss_heatmap = self.loss_cls(preds_dict[0]['heatmap'],\n",
    "                                         heatmaps[task_id],\n",
    "                                         avg_factor=cls_avg_factor)\n",
    "            target_box = anno_boxes[task_id]\n",
    "            # reconstruct the anno_box from multiple reg heads\n",
    "            if 'vel' in preds_dict[0].keys():\n",
    "                preds_dict[0]['anno_box'] = torch.cat(\n",
    "                    (preds_dict[0]['reg'], preds_dict[0]['height'],\n",
    "                     preds_dict[0]['dim'], preds_dict[0]['rot'],\n",
    "                     preds_dict[0]['vel']),\n",
    "                    dim=1,\n",
    "                )\n",
    "            else:\n",
    "                preds_dict[0]['anno_box'] = torch.cat(\n",
    "                    (preds_dict[0]['reg'], preds_dict[0]['height'],\n",
    "                     preds_dict[0]['dim'], preds_dict[0]['rot']),\n",
    "                    dim=1,\n",
    "                )\n",
    "            # Regression loss for dimension, offset, height, rotation\n",
    "            num = masks[task_id].float().sum()\n",
    "            ind = inds[task_id]\n",
    "            pred = preds_dict[0]['anno_box'].permute(0, 2, 3, 1).contiguous()\n",
    "            pred = pred.view(pred.size(0), -1, pred.size(3))\n",
    "            pred = self._gather_feat(pred, ind)\n",
    "            mask = masks[task_id].unsqueeze(2).expand_as(target_box).float()\n",
    "            num = torch.clamp(reduce_mean(target_box.new_tensor(num)),\n",
    "                              min=1e-4).item()\n",
    "            isnotnan = (~torch.isnan(target_box)).float()\n",
    "            mask *= isnotnan\n",
    "            code_weights = self.train_cfg['code_weights']\n",
    "            bbox_weights = mask * mask.new_tensor(code_weights)\n",
    "            loss_bbox = self.loss_bbox(pred,\n",
    "                                       target_box,\n",
    "                                       bbox_weights,\n",
    "                                       avg_factor=num)\n",
    "            return_loss += loss_bbox\n",
    "            return_loss += loss_heatmap\n",
    "        return return_loss\n",
    "\n",
    "    def get_bboxes(self, preds_dicts, img_metas, img=None, rescale=False):\n",
    "        \"\"\"Generate bboxes from bbox head predictions.\n",
    "\n",
    "        Args:\n",
    "            preds_dicts (tuple[list[dict]]): Prediction results.\n",
    "            img_metas (list[dict]): Point cloud and image's meta info.\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: Decoded bbox, scores and labels after nms.\n",
    "        \"\"\"\n",
    "        rets = []\n",
    "        for task_id, preds_dict in enumerate(preds_dicts):\n",
    "            num_class_with_bg = self.num_classes[task_id]\n",
    "            batch_size = preds_dict[0]['heatmap'].shape[0]\n",
    "            batch_heatmap = preds_dict[0]['heatmap'].sigmoid()\n",
    "\n",
    "            batch_reg = preds_dict[0]['reg']\n",
    "            batch_hei = preds_dict[0]['height']\n",
    "\n",
    "            if self.norm_bbox:\n",
    "                batch_dim = torch.exp(preds_dict[0]['dim'])\n",
    "            else:\n",
    "                batch_dim = preds_dict[0]['dim']\n",
    "\n",
    "            batch_rots = preds_dict[0]['rot'][:, 0].unsqueeze(1)\n",
    "            batch_rotc = preds_dict[0]['rot'][:, 1].unsqueeze(1)\n",
    "\n",
    "            if 'vel' in preds_dict[0]:\n",
    "                batch_vel = preds_dict[0]['vel']\n",
    "            else:\n",
    "                batch_vel = None\n",
    "            temp = self.bbox_coder.decode(batch_heatmap,\n",
    "                                          batch_rots,\n",
    "                                          batch_rotc,\n",
    "                                          batch_hei,\n",
    "                                          batch_dim,\n",
    "                                          batch_vel,\n",
    "                                          reg=batch_reg,\n",
    "                                          task_id=task_id)\n",
    "            assert self.test_cfg['nms_type'] in [\n",
    "                'size_aware_circle', 'circle', 'rotate'\n",
    "            ]\n",
    "            batch_reg_preds = [box['bboxes'] for box in temp]\n",
    "            batch_cls_preds = [box['scores'] for box in temp]\n",
    "            batch_cls_labels = [box['labels'] for box in temp]\n",
    "            if self.test_cfg['nms_type'] == 'circle':\n",
    "                ret_task = []\n",
    "                for i in range(batch_size):\n",
    "                    boxes3d = temp[i]['bboxes']\n",
    "                    scores = temp[i]['scores']\n",
    "                    labels = temp[i]['labels']\n",
    "                    centers = boxes3d[:, [0, 1]]\n",
    "                    boxes = torch.cat([centers, scores.view(-1, 1)], dim=1)\n",
    "                    keep = torch.tensor(circle_nms(\n",
    "                        boxes.detach().cpu().numpy(),\n",
    "                        self.test_cfg['min_radius'][task_id],\n",
    "                        post_max_size=self.test_cfg['post_max_size']),\n",
    "                                        dtype=torch.long,\n",
    "                                        device=boxes.device)\n",
    "\n",
    "                    boxes3d = boxes3d[keep]\n",
    "                    scores = scores[keep]\n",
    "                    labels = labels[keep]\n",
    "                    ret = dict(bboxes=boxes3d, scores=scores, labels=labels)\n",
    "                    ret_task.append(ret)\n",
    "                rets.append(ret_task)\n",
    "            elif self.test_cfg['nms_type'] == 'size_aware_circle':\n",
    "                ret_task = []\n",
    "                for i in range(batch_size):\n",
    "                    boxes3d = temp[i]['bboxes']\n",
    "                    scores = temp[i]['scores']\n",
    "                    labels = temp[i]['labels']\n",
    "                    boxes_2d = boxes3d[:, [0, 1, 3, 4, 6]]\n",
    "                    boxes = torch.cat([boxes_2d, scores.view(-1, 1)], dim=1)\n",
    "                    keep = torch.tensor(\n",
    "                        size_aware_circle_nms(\n",
    "                            boxes.detach().cpu().numpy(),\n",
    "                            self.test_cfg['thresh_scale'][task_id],\n",
    "                            post_max_size=self.test_cfg['post_max_size'],\n",
    "                        ),\n",
    "                        dtype=torch.long,\n",
    "                        device=boxes.device,\n",
    "                    )\n",
    "\n",
    "                    boxes3d = boxes3d[keep]\n",
    "                    scores = scores[keep]\n",
    "                    labels = labels[keep]\n",
    "                    ret = dict(bboxes=boxes3d, scores=scores, labels=labels)\n",
    "                    ret_task.append(ret)\n",
    "                rets.append(ret_task)\n",
    "            else:\n",
    "                rets.append(\n",
    "                    self.get_task_detections(num_class_with_bg,\n",
    "                                             batch_cls_preds, batch_reg_preds,\n",
    "                                             batch_cls_labels, img_metas))\n",
    "\n",
    "        # Merge branches results\n",
    "        num_samples = len(rets[0])\n",
    "\n",
    "        ret_list = []\n",
    "        for i in range(num_samples):\n",
    "            for k in rets[0][i].keys():\n",
    "                if k == 'bboxes':\n",
    "                    bboxes = torch.cat([ret[i][k] for ret in rets])\n",
    "                elif k == 'scores':\n",
    "                    scores = torch.cat([ret[i][k] for ret in rets])\n",
    "                elif k == 'labels':\n",
    "                    flag = 0\n",
    "                    for j, num_class in enumerate(self.num_classes):\n",
    "                        rets[j][i][k] += flag\n",
    "                        flag += num_class\n",
    "                    labels = torch.cat([ret[i][k].int() for ret in rets])\n",
    "            ret_list.append([bboxes, scores, labels])\n",
    "        return ret_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fc5bb-dc2d-4656-b55c-5da06497f711",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BEVStereo Model\n",
    "- from models/bev_stereo.py\n",
    "- Motification by Jeho\n",
    "- BEVDepth    BEVStereo class model functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d78655e4-a7a8-42aa-9484-4fa129f80a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BEVStereo(nn.Module):\n",
    "    \"\"\"Source code of `BEVStereo`, `https://arxiv.org/abs/2209.10248`.\n",
    "\n",
    "    Args:\n",
    "        backbone_conf (dict): Config of backbone.\n",
    "        head_conf (dict): Config of head.\n",
    "        is_train_depth (bool): Whether to return depth.\n",
    "            Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Reduce grid_conf and data_aug_conf\n",
    "    def __init__(self, backbone_conf, head_conf, is_train_depth=False):\n",
    "        super(BEVStereo, self).__init__()\n",
    "        self.backbone = BEVStereoLSSFPN(**backbone_conf)\n",
    "        self.head = BEVDepthHead(**head_conf)\n",
    "        self.is_train_depth = is_train_depth\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        mats_dict,\n",
    "        timestamps=None,\n",
    "    ):\n",
    "        \"\"\"Forward function for BEVDepth\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input ferature map.\n",
    "            mats_dict(dict):\n",
    "                sensor2ego_mats(Tensor): Transformation matrix from\n",
    "                    camera to ego with shape of (B, num_sweeps,\n",
    "                    num_cameras, 4, 4).\n",
    "                intrin_mats(Tensor): Intrinsic matrix with shape\n",
    "                    of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                ida_mats(Tensor): Transformation matrix for ida with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                sensor2sensor_mats(Tensor): Transformation matrix\n",
    "                    from key frame camera to sweep frame camera with\n",
    "                    shape of (B, num_sweeps, num_cameras, 4, 4).\n",
    "                bda_mat(Tensor): Rotation matrix for bda with shape\n",
    "                    of (B, 4, 4).\n",
    "            timestamps (long): Timestamp.\n",
    "                Default: None.\n",
    "\n",
    "        Returns:\n",
    "            tuple(list[dict]): Output results for tasks.\n",
    "        \"\"\"\n",
    "        if self.is_train_depth and self.training:\n",
    "            x, depth_pred = self.backbone(x,\n",
    "                                          mats_dict,\n",
    "                                          timestamps,\n",
    "                                          is_return_depth=True)\n",
    "            preds = self.head(x)\n",
    "            return preds, depth_pred\n",
    "        else:\n",
    "            x = self.backbone(x, mats_dict, timestamps)\n",
    "            preds = self.head(x)\n",
    "            return preds\n",
    "\n",
    "    def get_targets(self, gt_boxes, gt_labels):\n",
    "        \"\"\"Generate training targets for a single sample.\n",
    "\n",
    "        Args:\n",
    "            gt_bboxes_3d (:obj:`LiDARInstance3DBoxes`): Ground truth gt boxes.\n",
    "            gt_labels_3d (torch.Tensor): Labels of boxes.\n",
    "\n",
    "        Returns:\n",
    "            tuple[list[torch.Tensor]]: Tuple of target including \\\n",
    "                the following results in order.\n",
    "\n",
    "                - list[torch.Tensor]: Heatmap scores.\n",
    "                - list[torch.Tensor]: Ground truth boxes.\n",
    "                - list[torch.Tensor]: Indexes indicating the position \\\n",
    "                    of the valid boxes.\n",
    "                - list[torch.Tensor]: Masks indicating which boxes \\\n",
    "                    are valid.\n",
    "        \"\"\"\n",
    "        return self.head.get_targets(gt_boxes, gt_labels)\n",
    "\n",
    "    def loss(self, targets, preds_dicts):\n",
    "        \"\"\"Loss function for BEVDepth.\n",
    "\n",
    "        Args:\n",
    "            gt_bboxes_3d (list[:obj:`LiDARInstance3DBoxes`]): Ground\n",
    "                truth gt boxes.\n",
    "            gt_labels_3d (list[torch.Tensor]): Labels of boxes.\n",
    "            preds_dicts (dict): Output of forward function.\n",
    "\n",
    "        Returns:\n",
    "            dict[str:torch.Tensor]: Loss of heatmap and bbox of each task.\n",
    "        \"\"\"\n",
    "        return self.head.loss(targets, preds_dicts)\n",
    "\n",
    "    def get_bboxes(self, preds_dicts, img_metas=None, img=None, rescale=False):\n",
    "        \"\"\"Generate bboxes from bbox head predictions.\n",
    "\n",
    "        Args:\n",
    "            preds_dicts (tuple[list[dict]]): Prediction results.\n",
    "            img_metas (list[dict]): Point cloud and image's meta info.\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: Decoded bbox, scores and labels after nms.\n",
    "        \"\"\"\n",
    "        return self.head.get_bboxes(preds_dicts, img_metas, img, rescale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524aaaf-c8a2-41d4-a98a-2290a24a6aae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BEVStereo Detector (based on BEVDepth) (PyTorch Lightning Module)\n",
    "- from exps/nuscenes/base_exp.py, exps/nuscenes/mv/bevstereo.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7318ffd5-5ab0-46dd-9fc8-26fbfab81360",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a9939e-b3f6-44cb-9dfa-8cb550f3e38b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "H = 900\n",
    "W = 1600\n",
    "final_dim = (256, 704)\n",
    "img_conf = dict(img_mean=[123.675, 116.28, 103.53],\n",
    "                img_std=[58.395, 57.12, 57.375],\n",
    "                to_rgb=True)\n",
    "\n",
    "backbone_conf = {\n",
    "    'x_bound': [-51.2, 51.2, 0.8],\n",
    "    'y_bound': [-51.2, 51.2, 0.8],\n",
    "    'z_bound': [-5, 3, 8],\n",
    "    'd_bound': [2.0, 58.0, 0.5],\n",
    "    'final_dim':\n",
    "    final_dim,\n",
    "    'output_channels': 80,\n",
    "    'downsample_factor': 16,\n",
    "    'img_backbone_conf':\n",
    "    dict(\n",
    "        type='ResNet',\n",
    "        depth=50,\n",
    "        frozen_stages=0,\n",
    "        out_indices=[0, 1, 2, 3],\n",
    "        norm_eval=False,\n",
    "        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),\n",
    "    ),\n",
    "    'img_neck_conf':\n",
    "    dict(\n",
    "        type='SECONDFPN',\n",
    "        in_channels=[256, 512, 1024, 2048],\n",
    "        upsample_strides=[0.25, 0.5, 1, 2],\n",
    "        out_channels=[128, 128, 128, 128],\n",
    "    ),\n",
    "    'depth_net_conf':\n",
    "    dict(in_channels=512, mid_channels=512)\n",
    "}\n",
    "\n",
    "ida_aug_conf = {\n",
    "    'resize_lim': (0.386, 0.55),\n",
    "    'final_dim': final_dim,\n",
    "    'rot_lim': (-5.4, 5.4),\n",
    "    'H': H,\n",
    "    'W': W,\n",
    "    'rand_flip': True,\n",
    "    'bot_pct_lim': (0.0, 0.0),\n",
    "    'cams': [\n",
    "        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT',\n",
    "        'CAM_BACK', 'CAM_BACK_RIGHT'\n",
    "    ],\n",
    "    'Ncams': 6,\n",
    "}\n",
    "\n",
    "bda_aug_conf = {\n",
    "    'rot_lim': (-22.5, 22.5),\n",
    "    'scale_lim': (0.95, 1.05),\n",
    "    'flip_dx_ratio': 0.5,\n",
    "    'flip_dy_ratio': 0.5\n",
    "}\n",
    "\n",
    "bev_backbone = dict(\n",
    "    type='ResNet',\n",
    "    in_channels=80,\n",
    "    depth=18,\n",
    "    num_stages=3,\n",
    "    strides=(1, 2, 2),\n",
    "    dilations=(1, 1, 1),\n",
    "    out_indices=[0, 1, 2],\n",
    "    norm_eval=False,\n",
    "    base_channels=160,\n",
    ")\n",
    "\n",
    "bev_neck = dict(type='SECONDFPN',\n",
    "                in_channels=[80, 160, 320, 640],\n",
    "                upsample_strides=[1, 2, 4, 8],\n",
    "                out_channels=[64, 64, 64, 64])\n",
    "\n",
    "CLASSES = [\n",
    "    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',\n",
    "    'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone',\n",
    "]\n",
    "\n",
    "TASKS = [\n",
    "    dict(num_class=1, class_names=['car']),\n",
    "    dict(num_class=2, class_names=['truck', 'construction_vehicle']),\n",
    "    dict(num_class=2, class_names=['bus', 'trailer']),\n",
    "    dict(num_class=1, class_names=['barrier']),\n",
    "    dict(num_class=2, class_names=['motorcycle', 'bicycle']),\n",
    "    dict(num_class=2, class_names=['pedestrian', 'traffic_cone']),\n",
    "]\n",
    "\n",
    "common_heads = dict(reg=(2, 2),\n",
    "                    height=(1, 2),\n",
    "                    dim=(3, 2),\n",
    "                    rot=(2, 2),\n",
    "                    vel=(2, 2))\n",
    "\n",
    "bbox_coder = dict(\n",
    "    type='CenterPointBBoxCoder',\n",
    "    post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],\n",
    "    max_num=500,\n",
    "    score_threshold=0.1,\n",
    "    out_size_factor=4,\n",
    "    voxel_size=[0.2, 0.2, 8],\n",
    "    pc_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n",
    "    code_size=9,\n",
    ")\n",
    "\n",
    "train_cfg = dict(\n",
    "    point_cloud_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n",
    "    grid_size=[512, 512, 1],\n",
    "    voxel_size=[0.2, 0.2, 8],\n",
    "    out_size_factor=4,\n",
    "    dense_reg=1,\n",
    "    gaussian_overlap=0.1,\n",
    "    max_objs=500,\n",
    "    min_radius=2,\n",
    "    code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5],\n",
    ")\n",
    "\n",
    "test_cfg = dict(\n",
    "    post_center_limit_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],\n",
    "    max_per_img=500,\n",
    "    max_pool_nms=False,\n",
    "    min_radius=[4, 12, 10, 1, 0.85, 0.175],\n",
    "    score_threshold=0.1,\n",
    "    out_size_factor=4,\n",
    "    voxel_size=[0.2, 0.2, 8],\n",
    "    nms_type='circle',\n",
    "    pre_max_size=1000,\n",
    "    post_max_size=83,\n",
    "    nms_thr=0.2,\n",
    ")\n",
    "\n",
    "head_conf = {\n",
    "    'bev_backbone_conf': bev_backbone,\n",
    "    'bev_neck_conf': bev_neck,\n",
    "    'tasks': TASKS,\n",
    "    'common_heads': common_heads,\n",
    "    'bbox_coder': bbox_coder,\n",
    "    'train_cfg': train_cfg,\n",
    "    'test_cfg': test_cfg,\n",
    "    'in_channels': 256,  # Equal to bev_neck output_channels.\n",
    "    'loss_cls': dict(type='GaussianFocalLoss', reduction='mean'),\n",
    "    'loss_bbox': dict(type='L1Loss', reduction='mean', loss_weight=0.25),\n",
    "    'gaussian_overlap': 0.1,\n",
    "    'min_radius': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb3d30-27f2-4eb5-85f9-924ef190b55a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Detector (Lightning module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96a1acb8-7d85-4179-947c-3f8b5628fbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class BEVDepthLightningModel(LightningModule):\n",
    "class BEVStereoLightningDetector(LightningModule):\n",
    "    MODEL_NAMES = sorted(name for name in models.__dict__\n",
    "                         if name.islower() and not name.startswith('__')\n",
    "                         and callable(models.__dict__[name]))\n",
    "\n",
    "    def __init__(self,\n",
    "                 gpus: int = 1,\n",
    "                 data_root='data/nuscenes/',\n",
    "                 eval_interval=1,\n",
    "                 batch_size_per_device=8,\n",
    "                 class_names=CLASSES,\n",
    "                 backbone_conf=backbone_conf,\n",
    "                 head_conf=head_conf,\n",
    "                 ida_aug_conf=ida_aug_conf,\n",
    "                 bda_aug_conf=bda_aug_conf,\n",
    "                 default_root_dir='./outputs/',\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.gpus = gpus\n",
    "        self.eval_interval = eval_interval\n",
    "        self.batch_size_per_device = batch_size_per_device\n",
    "        self.data_root = data_root\n",
    "        self.basic_lr_per_img = 2e-4 / 64\n",
    "        self.class_names = class_names\n",
    "        self.backbone_conf = backbone_conf\n",
    "        self.head_conf = head_conf\n",
    "        self.ida_aug_conf = ida_aug_conf\n",
    "        self.bda_aug_conf = bda_aug_conf\n",
    "        mmcv.mkdir_or_exist(default_root_dir)\n",
    "        self.default_root_dir = default_root_dir\n",
    "        self.evaluator = DetNuscEvaluator(class_names=self.class_names,\n",
    "                                          output_dir=self.default_root_dir)\n",
    "        # self.model = BaseBEVDepth(self.backbone_conf,\n",
    "        #                           self.head_conf,\n",
    "        #                           is_train_depth=True)\n",
    "        self.mode = 'valid'\n",
    "        self.img_conf = img_conf\n",
    "        self.data_use_cbgs = False\n",
    "        self.num_sweeps = 1\n",
    "        self.sweep_idxes = list()\n",
    "        self.key_idxes = list()\n",
    "        self.data_return_depth = True\n",
    "        self.downsample_factor = self.backbone_conf['downsample_factor']\n",
    "        self.dbound = self.backbone_conf['d_bound']\n",
    "        self.depth_channels = int((self.dbound[1] - self.dbound[0]) / self.dbound[2])\n",
    "        self.use_fusion = False\n",
    "        \n",
    "        self.train_info_paths = os.path.join(self.data_root, 'nuscenes_infos_train.pkl')\n",
    "        self.val_info_paths = os.path.join(self.data_root, 'nuscenes_infos_val.pkl')\n",
    "        # self.predict_info_paths = os.path.join(self.data_root, 'nuscenes_infos_test.pkl')\n",
    "        self.predict_info_paths = os.path.join(self.data_root, 'nuscenes_infos_val.pkl')\n",
    "        \n",
    "        \"\"\"BEVStereo configurations\"\"\"\n",
    "        # base ckpt of bevstereo (2 key)\n",
    "        self.key_idxes = [-1]\n",
    "        self.head_conf['bev_backbone_conf']['in_channels'] = 80 * (len(self.key_idxes) + 1)\n",
    "        self.head_conf['bev_neck_conf']['in_channels'] = [80 * (len(self.key_idxes) + 1), 160, 320, 640]\n",
    "        self.head_conf['train_cfg']['code_weight'] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "        self.head_conf['test_cfg']['thresh_scale'] = [0.6, 0.4, 0.4, 0.7, 0.8, 0.9]\n",
    "        self.head_conf['test_cfg']['nms_type'] = 'size_aware_circle'\n",
    "        \n",
    "        # ckpt with data augmentation\n",
    "        self.backbone_conf['use_da'] = True\n",
    "        self.data_use_cbgs = True\n",
    "        self.basic_lr_per_img = 2e-4 / 32\n",
    "        \n",
    "        self.model = BEVStereo(self.backbone_conf,\n",
    "                               self.head_conf,\n",
    "                               is_train_depth=True)\n",
    "        \n",
    "        self.starter, self.ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        # device = 'cuda:0'\n",
    "        # torch.cuda.set_device(device)\n",
    "        # self.model.to(device)\n",
    "        \n",
    "    def forward(self, sweep_imgs, mats):\n",
    "        return self.model(sweep_imgs, mats)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        (sweep_imgs, mats, _, _, gt_boxes, gt_labels, depth_labels) = batch\n",
    "        if torch.cuda.is_available():\n",
    "            for key, value in mats.items():\n",
    "                mats[key] = value.cuda()\n",
    "            sweep_imgs = sweep_imgs.cuda()\n",
    "            gt_boxes = [gt_box.cuda() for gt_box in gt_boxes]\n",
    "            gt_labels = [gt_label.cuda() for gt_label in gt_labels]\n",
    "        preds, depth_preds = self(sweep_imgs, mats)\n",
    "        if isinstance(self.model, torch.nn.parallel.DistributedDataParallel):\n",
    "            targets = self.model.module.get_targets(gt_boxes, gt_labels)\n",
    "            detection_loss = self.model.module.loss(targets, preds)\n",
    "        else:\n",
    "            targets = self.model.get_targets(gt_boxes, gt_labels)\n",
    "            detection_loss = self.model.loss(targets, preds)\n",
    "\n",
    "        if len(depth_labels.shape) == 5:\n",
    "            # only key-frame will calculate depth loss\n",
    "            depth_labels = depth_labels[:, 0, ...]\n",
    "        depth_loss = self.get_depth_loss(depth_labels.cuda(), depth_preds)\n",
    "        self.log('detection_loss', detection_loss)\n",
    "        self.log('depth_loss', depth_loss)\n",
    "        return detection_loss + depth_loss\n",
    "\n",
    "    def get_depth_loss(self, depth_labels, depth_preds):\n",
    "        depth_labels = self.get_downsampled_gt_depth(depth_labels)\n",
    "        depth_preds = depth_preds.permute(0, 2, 3, 1).contiguous().view(\n",
    "            -1, self.depth_channels)\n",
    "        fg_mask = torch.max(depth_labels, dim=1).values > 0.0\n",
    "\n",
    "        with autocast(enabled=False):\n",
    "            depth_loss = (F.binary_cross_entropy(\n",
    "                depth_preds[fg_mask],\n",
    "                depth_labels[fg_mask],\n",
    "                reduction='none',\n",
    "            ).sum() / max(1.0, fg_mask.sum()))\n",
    "\n",
    "        return 3.0 * depth_loss\n",
    "\n",
    "    def get_downsampled_gt_depth(self, gt_depths):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            gt_depths: [B, N, H, W]\n",
    "        Output:\n",
    "            gt_depths: [B*N*h*w, d]\n",
    "        \"\"\"\n",
    "        B, N, H, W = gt_depths.shape\n",
    "        gt_depths = gt_depths.view(\n",
    "            B * N,\n",
    "            H // self.downsample_factor,\n",
    "            self.downsample_factor,\n",
    "            W // self.downsample_factor,\n",
    "            self.downsample_factor,\n",
    "            1,\n",
    "        )\n",
    "        gt_depths = gt_depths.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
    "        gt_depths = gt_depths.view(\n",
    "            -1, self.downsample_factor * self.downsample_factor)\n",
    "        gt_depths_tmp = torch.where(gt_depths == 0.0,\n",
    "                                    1e5 * torch.ones_like(gt_depths),\n",
    "                                    gt_depths)\n",
    "        gt_depths = torch.min(gt_depths_tmp, dim=-1).values\n",
    "        gt_depths = gt_depths.view(B * N, H // self.downsample_factor,\n",
    "                                   W // self.downsample_factor)\n",
    "\n",
    "        gt_depths = (gt_depths - (self.dbound[0] - self.dbound[2])) / self.dbound[2]\n",
    "        gt_depths = torch.where(\n",
    "            (gt_depths < self.depth_channels + 1) & (gt_depths >= 0.0),\n",
    "            gt_depths, torch.zeros_like(gt_depths))\n",
    "        gt_depths = F.one_hot(gt_depths.long(),\n",
    "                              num_classes=self.depth_channels + 1).view(\n",
    "                                  -1, self.depth_channels + 1)[:, 1:]\n",
    "\n",
    "        return gt_depths.float()\n",
    "\n",
    "    def eval_step(self, batch, batch_idx, prefix: str):\n",
    "        (sweep_imgs, mats, _, img_metas, _, _) = batch\n",
    "                \n",
    "        if torch.cuda.is_available():\n",
    "            for key, value in mats.items():\n",
    "                mats[key] = value.cuda()\n",
    "            sweep_imgs = sweep_imgs.cuda()\n",
    "        \n",
    "        self.starter.record()\n",
    "        \n",
    "        preds = self.model(sweep_imgs, mats)\n",
    "        \n",
    "        self.ender.record()\n",
    "        # wait for gpu sync\n",
    "        torch.cuda.synchronize()\n",
    "        inference_time = self.starter.elapsed_time(self.ender)\n",
    "        print(\"inference time (ms): \", inference_time)\n",
    "        \n",
    "        if isinstance(self.model, torch.nn.parallel.DistributedDataParallel):\n",
    "            results = self.model.module.get_bboxes(preds, img_metas)\n",
    "        else:\n",
    "            results = self.model.get_bboxes(preds, img_metas)\n",
    "        for i in range(len(results)):\n",
    "            results[i][0] = results[i][0].detach().cpu().numpy()\n",
    "            results[i][1] = results[i][1].detach().cpu().numpy()\n",
    "            results[i][2] = results[i][2].detach().cpu().numpy()\n",
    "            results[i].append(img_metas[i])\n",
    "        return results\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.eval_step(batch, batch_idx, 'val')\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        all_pred_results = list()\n",
    "        all_img_metas = list()\n",
    "        for validation_step_output in validation_step_outputs:\n",
    "            for i in range(len(validation_step_output)):\n",
    "                all_pred_results.append(validation_step_output[i][:3])\n",
    "                all_img_metas.append(validation_step_output[i][3])\n",
    "        synchronize()\n",
    "        len_dataset = len(self.val_dataloader().dataset)\n",
    "        all_pred_results = sum(\n",
    "            map(list, zip(*all_gather_object(all_pred_results))),\n",
    "            [])[:len_dataset]\n",
    "        all_img_metas = sum(map(list, zip(*all_gather_object(all_img_metas))),\n",
    "                            [])[:len_dataset]\n",
    "        if get_rank() == 0:\n",
    "            self.evaluator.evaluate(all_pred_results, all_img_metas)\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        all_pred_results = list()\n",
    "        all_img_metas = list()\n",
    "        for test_step_output in test_step_outputs:\n",
    "            for i in range(len(test_step_output)):\n",
    "                all_pred_results.append(test_step_output[i][:3])\n",
    "                all_img_metas.append(test_step_output[i][3])\n",
    "        synchronize()\n",
    "        # TODO: Change another way.\n",
    "        dataset_length = len(self.val_dataloader().dataset)\n",
    "        all_pred_results = sum(\n",
    "            map(list, zip(*all_gather_object(all_pred_results))),\n",
    "            [])[:dataset_length]\n",
    "        all_img_metas = sum(map(list, zip(*all_gather_object(all_img_metas))),\n",
    "                            [])[:dataset_length]\n",
    "        if get_rank() == 0:\n",
    "            self.evaluator.evaluate(all_pred_results, all_img_metas)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.basic_lr_per_img * self.batch_size_per_device * self.gpus\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-7)\n",
    "        scheduler = MultiStepLR(optimizer, [19, 23])\n",
    "        return [[optimizer], [scheduler]]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = NuscDetDataset(ida_aug_conf=self.ida_aug_conf,\n",
    "                                       bda_aug_conf=self.bda_aug_conf,\n",
    "                                       classes=self.class_names,\n",
    "                                       data_root=self.data_root,\n",
    "                                       info_paths=self.train_info_paths,\n",
    "                                       is_train=True,\n",
    "                                       use_cbgs=self.data_use_cbgs,\n",
    "                                       img_conf=self.img_conf,\n",
    "                                       num_sweeps=self.num_sweeps,\n",
    "                                       sweep_idxes=self.sweep_idxes,\n",
    "                                       key_idxes=self.key_idxes,\n",
    "                                       return_depth=self.data_return_depth,\n",
    "                                       use_fusion=self.use_fusion)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=4,\n",
    "            drop_last=True,\n",
    "            shuffle=False,\n",
    "            collate_fn=partial(collate_fn,\n",
    "                               is_return_depth=self.data_return_depth\n",
    "                               or self.use_fusion),\n",
    "            sampler=None,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = NuscDetDataset(ida_aug_conf=self.ida_aug_conf,\n",
    "                                     bda_aug_conf=self.bda_aug_conf,\n",
    "                                     classes=self.class_names,\n",
    "                                     data_root=self.data_root,\n",
    "                                     info_paths=self.val_info_paths,\n",
    "                                     is_train=False,\n",
    "                                     img_conf=self.img_conf,\n",
    "                                     num_sweeps=self.num_sweeps,\n",
    "                                     sweep_idxes=self.sweep_idxes,\n",
    "                                     key_idxes=self.key_idxes,\n",
    "                                     return_depth=self.use_fusion,\n",
    "                                     use_fusion=self.use_fusion)\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            shuffle=False,\n",
    "            collate_fn=partial(collate_fn, is_return_depth=self.use_fusion),\n",
    "            num_workers=4,\n",
    "            sampler=None,\n",
    "        )\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.val_dataloader()\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        predict_dataset = NuscDetDataset(ida_aug_conf=self.ida_aug_conf,\n",
    "                                         bda_aug_conf=self.bda_aug_conf,\n",
    "                                         classes=self.class_names,\n",
    "                                         data_root=self.data_root,\n",
    "                                         info_paths=self.predict_info_paths,\n",
    "                                         is_train=False,\n",
    "                                         img_conf=self.img_conf,\n",
    "                                         num_sweeps=self.num_sweeps,\n",
    "                                         sweep_idxes=self.sweep_idxes,\n",
    "                                         key_idxes=self.key_idxes,\n",
    "                                         return_depth=self.use_fusion,\n",
    "                                         use_fusion=self.use_fusion)\n",
    "        predict_loader = torch.utils.data.DataLoader(\n",
    "            predict_dataset,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            shuffle=False,\n",
    "            collate_fn=partial(collate_fn, is_return_depth=self.use_fusion),\n",
    "            num_workers=4,\n",
    "            sampler=None,\n",
    "        )\n",
    "        return predict_loader\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.eval_step(batch, batch_idx, 'test')\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        return self.eval_step(batch, batch_idx, 'predict')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.basic_lr_per_img * self.batch_size_per_device * self.gpus\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "        scheduler = MultiStepLR(optimizer, [16, 19])\n",
    "        return [[optimizer], [scheduler]]\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):  # pragma: no-cover\n",
    "        return parent_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb976c0-1ec1-4ae5-8cf9-a1339cdfacf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2bc0f-2439-4282-b599-7d64841c24ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46191ae-1062-439e-b7f4-3d0190255b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd0db86-df29-4bfe-968e-092b6b5bb587",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2312963/1130927445.py:252: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  torch.LongTensor([(row[1] - row[0]) / row[2]\n",
      "2023-01-04 15:00:39,611 - mmcv - INFO - initialize SECONDFPN with init_cfg [{'type': 'Kaiming', 'layer': 'ConvTranspose2d'}, {'type': 'Constant', 'layer': 'NaiveSyncBatchNorm2d', 'val': 1.0}]\n",
      "2023-01-04 15:00:39,626 - mmcv - INFO - \n",
      "deblocks.0.0.weight - torch.Size([128, 256, 4, 4]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:39,627 - mmcv - INFO - \n",
      "deblocks.0.1.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:39,627 - mmcv - INFO - \n",
      "deblocks.0.1.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:39,628 - mmcv - INFO - \n",
      "deblocks.1.0.weight - torch.Size([128, 512, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:39,629 - mmcv - INFO - \n",
      "deblocks.1.1.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:39,629 - mmcv - INFO - \n",
      "deblocks.1.1.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:39,630 - mmcv - INFO - \n",
      "deblocks.2.0.weight - torch.Size([1024, 128, 1, 1]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:39,631 - mmcv - INFO - \n",
      "deblocks.2.1.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:39,631 - mmcv - INFO - \n",
      "deblocks.2.1.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:39,632 - mmcv - INFO - \n",
      "deblocks.3.0.weight - torch.Size([2048, 128, 2, 2]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:39,633 - mmcv - INFO - \n",
      "deblocks.3.1.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:39,633 - mmcv - INFO - \n",
      "deblocks.3.1.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:39,642 - mmcv - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'}\n",
      "2023-01-04 15:00:39,643 - mmcv - INFO - load model from: torchvision://resnet50\n",
      "2023-01-04 15:00:39,645 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet50\n",
      "2023-01-04 15:00:39,781 - mmcv - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n",
      "2023-01-04 15:00:40,061 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]\n",
      "2023-01-04 15:00:40,190 - mmcv - INFO - initialize BasicBlock with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
      "2023-01-04 15:00:40,192 - mmcv - INFO - initialize BasicBlock with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
      "2023-01-04 15:00:40,194 - mmcv - INFO - initialize BasicBlock with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
      "2023-01-04 15:00:40,196 - mmcv - INFO - initialize BasicBlock with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
      "2023-01-04 15:00:40,198 - mmcv - INFO - initialize BasicBlock with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
      "2023-01-04 15:00:40,200 - mmcv - INFO - initialize BasicBlock with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
      "2023-01-04 15:00:40,204 - mmcv - INFO - \n",
      "conv1.weight - torch.Size([160, 160, 7, 7]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,205 - mmcv - INFO - \n",
      "bn1.weight - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,206 - mmcv - INFO - \n",
      "bn1.bias - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,206 - mmcv - INFO - \n",
      "layer1.0.conv1.weight - torch.Size([160, 160, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,207 - mmcv - INFO - \n",
      "layer1.0.bn1.weight - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,208 - mmcv - INFO - \n",
      "layer1.0.bn1.bias - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,208 - mmcv - INFO - \n",
      "layer1.0.conv2.weight - torch.Size([160, 160, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,209 - mmcv - INFO - \n",
      "layer1.0.bn2.weight - torch.Size([160]): \n",
      "ConstantInit: val=0, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,210 - mmcv - INFO - \n",
      "layer1.0.bn2.bias - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,211 - mmcv - INFO - \n",
      "layer1.1.conv1.weight - torch.Size([160, 160, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,211 - mmcv - INFO - \n",
      "layer1.1.bn1.weight - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,212 - mmcv - INFO - \n",
      "layer1.1.bn1.bias - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,212 - mmcv - INFO - \n",
      "layer1.1.conv2.weight - torch.Size([160, 160, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,213 - mmcv - INFO - \n",
      "layer1.1.bn2.weight - torch.Size([160]): \n",
      "ConstantInit: val=0, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,214 - mmcv - INFO - \n",
      "layer1.1.bn2.bias - torch.Size([160]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,215 - mmcv - INFO - \n",
      "layer2.0.conv1.weight - torch.Size([320, 160, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,216 - mmcv - INFO - \n",
      "layer2.0.bn1.weight - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,216 - mmcv - INFO - \n",
      "layer2.0.bn1.bias - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,217 - mmcv - INFO - \n",
      "layer2.0.conv2.weight - torch.Size([320, 320, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,217 - mmcv - INFO - \n",
      "layer2.0.bn2.weight - torch.Size([320]): \n",
      "ConstantInit: val=0, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,218 - mmcv - INFO - \n",
      "layer2.0.bn2.bias - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,218 - mmcv - INFO - \n",
      "layer2.0.downsample.0.weight - torch.Size([320, 160, 1, 1]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,219 - mmcv - INFO - \n",
      "layer2.0.downsample.1.weight - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,220 - mmcv - INFO - \n",
      "layer2.0.downsample.1.bias - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,221 - mmcv - INFO - \n",
      "layer2.1.conv1.weight - torch.Size([320, 320, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,221 - mmcv - INFO - \n",
      "layer2.1.bn1.weight - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,222 - mmcv - INFO - \n",
      "layer2.1.bn1.bias - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,222 - mmcv - INFO - \n",
      "layer2.1.conv2.weight - torch.Size([320, 320, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,223 - mmcv - INFO - \n",
      "layer2.1.bn2.weight - torch.Size([320]): \n",
      "ConstantInit: val=0, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,223 - mmcv - INFO - \n",
      "layer2.1.bn2.bias - torch.Size([320]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,224 - mmcv - INFO - \n",
      "layer3.0.conv1.weight - torch.Size([640, 320, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,224 - mmcv - INFO - \n",
      "layer3.0.bn1.weight - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,225 - mmcv - INFO - \n",
      "layer3.0.bn1.bias - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,225 - mmcv - INFO - \n",
      "layer3.0.conv2.weight - torch.Size([640, 640, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,226 - mmcv - INFO - \n",
      "layer3.0.bn2.weight - torch.Size([640]): \n",
      "ConstantInit: val=0, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,226 - mmcv - INFO - \n",
      "layer3.0.bn2.bias - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,227 - mmcv - INFO - \n",
      "layer3.0.downsample.0.weight - torch.Size([640, 320, 1, 1]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,227 - mmcv - INFO - \n",
      "layer3.0.downsample.1.weight - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,230 - mmcv - INFO - \n",
      "layer3.0.downsample.1.bias - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,231 - mmcv - INFO - \n",
      "layer3.1.conv1.weight - torch.Size([640, 640, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,231 - mmcv - INFO - \n",
      "layer3.1.bn1.weight - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,232 - mmcv - INFO - \n",
      "layer3.1.bn1.bias - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,233 - mmcv - INFO - \n",
      "layer3.1.conv2.weight - torch.Size([640, 640, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,233 - mmcv - INFO - \n",
      "layer3.1.bn2.weight - torch.Size([640]): \n",
      "ConstantInit: val=0, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,234 - mmcv - INFO - \n",
      "layer3.1.bn2.bias - torch.Size([640]): \n",
      "The value is the same before and after calling `init_weights` of ResNet  \n",
      " \n",
      "2023-01-04 15:00:40,265 - mmcv - INFO - initialize SECONDFPN with init_cfg [{'type': 'Kaiming', 'layer': 'ConvTranspose2d'}, {'type': 'Constant', 'layer': 'NaiveSyncBatchNorm2d', 'val': 1.0}]\n",
      "2023-01-04 15:00:40,286 - mmcv - INFO - \n",
      "deblocks.0.0.weight - torch.Size([160, 64, 1, 1]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,287 - mmcv - INFO - \n",
      "deblocks.0.1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:40,287 - mmcv - INFO - \n",
      "deblocks.0.1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:40,288 - mmcv - INFO - \n",
      "deblocks.1.0.weight - torch.Size([160, 64, 2, 2]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,288 - mmcv - INFO - \n",
      "deblocks.1.1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:40,289 - mmcv - INFO - \n",
      "deblocks.1.1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:40,289 - mmcv - INFO - \n",
      "deblocks.2.0.weight - torch.Size([320, 64, 4, 4]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,291 - mmcv - INFO - \n",
      "deblocks.2.1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:40,291 - mmcv - INFO - \n",
      "deblocks.2.1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:40,292 - mmcv - INFO - \n",
      "deblocks.3.0.weight - torch.Size([640, 64, 8, 8]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "2023-01-04 15:00:40,293 - mmcv - INFO - \n",
      "deblocks.3.1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n",
      "2023-01-04 15:00:40,293 - mmcv - INFO - \n",
      "deblocks.3.1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of SECONDFPN  \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BEVStereo(\n",
       "  (backbone): BEVStereoLSSFPN(\n",
       "    (img_backbone): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): ResLayer(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): ResLayer(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): ResLayer(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): ResLayer(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    init_cfg={'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'}\n",
       "    (img_neck): SECONDFPN(\n",
       "      (deblocks): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(512, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): ConvTranspose2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): ConvTranspose2d(2048, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    init_cfg=[{'type': 'Kaiming', 'layer': 'ConvTranspose2d'}, {'type': 'Constant', 'layer': 'NaiveSyncBatchNorm2d', 'val': 1.0}]\n",
       "    (depth_net): DepthNet(\n",
       "      (reduce_conv): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (context_conv): Conv2d(512, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): BatchNorm1d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (depth_mlp): Mlp(\n",
       "        (fc1): Linear(in_features=27, out_features=512, bias=True)\n",
       "        (act): ReLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (depth_se): SELayer(\n",
       "        (conv_reduce): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act1): ReLU()\n",
       "        (conv_expand): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (context_mlp): Mlp(\n",
       "        (fc1): Linear(in_features=27, out_features=512, bias=True)\n",
       "        (act): ReLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (context_se): SELayer(\n",
       "        (conv_reduce): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act1): ReLU()\n",
       "        (conv_expand): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (depth_feat_conv): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): ASPP(\n",
       "          (aspp1): _ASPPModule(\n",
       "            (atrous_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (aspp2): _ASPPModule(\n",
       "            (atrous_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (aspp3): _ASPPModule(\n",
       "            (atrous_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (aspp4): _ASPPModule(\n",
       "            (atrous_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (global_avg_pool): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "            (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (3): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(2560, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (3): DeformConv2dPack(in_channels=512,\n",
       "        out_channels=512,\n",
       "        kernel_size=(3, 3),\n",
       "        stride=(1, 1),\n",
       "        padding=(1, 1),\n",
       "        dilation=(1, 1),\n",
       "        groups=4,\n",
       "        deform_groups=1,\n",
       "        bias=False)\n",
       "      )\n",
       "      (mu_sigma_range_net): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (mono_depth_net): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (depth_aggregation_net): DepthAggregation(\n",
       "      (reduce_conv): Sequential(\n",
       "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (out_conv): Sequential(\n",
       "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (similarity_net): Sequential(\n",
       "      (0): ConvBnReLU3D(\n",
       "        (conv): Conv3d(8, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ConvBnReLU3D(\n",
       "        (conv): Conv3d(16, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Conv3d(8, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "    (depth_downsample_net): Sequential(\n",
       "      (0): Conv2d(112, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(256, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (context_downsample_net): Identity()\n",
       "    (mask_net): Sequential(\n",
       "      (0): Conv2d(224, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (6): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (head): BEVDepthHead(\n",
       "    (loss_cls): GaussianFocalLoss()\n",
       "    (loss_bbox): L1Loss()\n",
       "    (shared_conv): ConvModule(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activate): ReLU(inplace=True)\n",
       "    )\n",
       "    (task_heads): ModuleList(\n",
       "      (0): SeparateHead(\n",
       "        (reg): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (height): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (dim): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (rot): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (vel): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (heatmap): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}\n",
       "      (1): SeparateHead(\n",
       "        (reg): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (height): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (dim): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (rot): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (vel): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (heatmap): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}\n",
       "      (2): SeparateHead(\n",
       "        (reg): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (height): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (dim): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (rot): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (vel): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (heatmap): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}\n",
       "      (3): SeparateHead(\n",
       "        (reg): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (height): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (dim): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (rot): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (vel): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (heatmap): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}\n",
       "      (4): SeparateHead(\n",
       "        (reg): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (height): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (dim): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (rot): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (vel): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (heatmap): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}\n",
       "      (5): SeparateHead(\n",
       "        (reg): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (height): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (dim): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (rot): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (vel): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (heatmap): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}\n",
       "    )\n",
       "    (trunk): ResNet(\n",
       "      (conv1): Conv2d(160, 160, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (layer1): ResLayer(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "      )\n",
       "      (layer2): ResLayer(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "      )\n",
       "      (layer3): ResLayer(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "      )\n",
       "    )\n",
       "    init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]\n",
       "    (neck): SECONDFPN(\n",
       "      (deblocks): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): ConvTranspose2d(160, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): ConvTranspose2d(160, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): ConvTranspose2d(320, 64, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): ConvTranspose2d(640, 64, kernel_size=(8, 8), stride=(8, 8), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    init_cfg=[{'type': 'Kaiming', 'layer': 'ConvTranspose2d'}, {'type': 'Constant', 'layer': 'NaiveSyncBatchNorm2d', 'val': 1.0}]\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Init BEVStereo model and hyper-params \"\"\"\n",
    "\n",
    "exp_name = 'bevstereo_ema_da_key2'\n",
    "ckpt_path = './ckpt/bev_stereo_lss_r50_256x704_128x128_20e_cbgs_2key_da.pth'\n",
    "use_ema = False\n",
    "\n",
    "# data_root_dir = '../BEVFormer/data/nuscenes/'\n",
    "data_root_dir = '../data/nuscenes/'\n",
    "default_root_dir = os.path.join('./outputs/', exp_name)\n",
    "\n",
    "device = 'cuda:0'\n",
    "gpus = 1\n",
    "eval_interval = 1\n",
    "batch_size_per_device = 1\n",
    "basic_lr_per_img = 2e-4 / 64\n",
    "\n",
    "mmcv.mkdir_or_exist(default_root_dir)\n",
    "evaluator = DetNuscEvaluator(class_names=CLASSES, output_dir=default_root_dir)\n",
    "\n",
    "mode = 'valid'\n",
    "data_use_cbgs = False\n",
    "num_sweeps = 1\n",
    "sweep_idxes = list()\n",
    "key_idxes = list()\n",
    "data_return_depth = True\n",
    "downsample_factor = backbone_conf['downsample_factor']\n",
    "dbound = backbone_conf['d_bound']\n",
    "depth_channels = int((dbound[1] - dbound[0]) / dbound[2])\n",
    "use_fusion = False\n",
    "\n",
    "train_info_paths = os.path.join(data_root_dir, 'nuscenes_infos_train.pkl')\n",
    "val_info_paths = os.path.join(data_root_dir, 'nuscenes_infos_val.pkl')\n",
    "# predict_info_paths = os.path.join(data_root_dir, 'nuscenes_infos_test.pkl')\n",
    "predict_info_paths = os.path.join(data_root_dir, 'nuscenes_infos_val.pkl')\n",
    "\n",
    "\"\"\"BEVStereo configurations\"\"\"\n",
    "# base ckpt of bevstereo (2 key)\n",
    "key_idxes = [-1]\n",
    "head_conf['bev_backbone_conf']['in_channels'] = 80 * (len(key_idxes) + 1)\n",
    "head_conf['bev_neck_conf']['in_channels'] = [80 * (len(key_idxes) + 1), 160, 320, 640]\n",
    "head_conf['train_cfg']['code_weight'] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "head_conf['test_cfg']['thresh_scale'] = [0.6, 0.4, 0.4, 0.7, 0.8, 0.9]\n",
    "head_conf['test_cfg']['nms_type'] = 'size_aware_circle'\n",
    "\n",
    "# ckpt with data augmentation\n",
    "backbone_conf['use_da'] = True\n",
    "data_use_cbgs = True\n",
    "basic_lr_per_img = 2e-4 / 32\n",
    "\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "model = BEVStereo(backbone_conf,\n",
    "                  head_conf,\n",
    "                  is_train_depth=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87717e68-a63f-4897-b6c4-462c2a68b695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c9df9-45aa-4a17-b421-b48cd80c9304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153a081-fa63-484c-b69d-b1c3eba92694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda05ab-66af-4137-99e5-aaab7c27ff73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d1bfd8e-8f2d-4033-8dc5-152b0fbdc58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Init dataset iterator \"\"\"\n",
    "\n",
    "dataset = NuscDetDataset(ida_aug_conf=ida_aug_conf,\n",
    "                         bda_aug_conf=bda_aug_conf,\n",
    "                         classes=CLASSES,\n",
    "                         data_root=data_root_dir,\n",
    "                         info_paths=predict_info_paths,\n",
    "                         is_train=False,\n",
    "                         img_conf=img_conf,\n",
    "                         num_sweeps=num_sweeps,\n",
    "                         sweep_idxes=sweep_idxes,\n",
    "                         key_idxes=key_idxes,\n",
    "                         return_depth=use_fusion,\n",
    "                         use_fusion=use_fusion)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size_per_device,\n",
    "    shuffle=False,\n",
    "    collate_fn=partial(collate_fn, is_return_depth=use_fusion),\n",
    "    num_workers=4,\n",
    "    sampler=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3ea30dee-539a-4ed0-ba3b-ecf954a3f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = iter(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9ff34373-2ff3-492f-8ee2-c6609b5c8f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 6, 3, 256, 704])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(data_iterator)\n",
    "(sweep_imgs, mats, _, img_metas, _, _) = data\n",
    "sweep_imgs.shape # (B, num_sweeps, num_cameras, 3, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b10ff705-a0b8-415e-a2e4-092badc2253e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time (ms):  264.6473083496094\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Model inference \"\"\"\n",
    "with torch.no_grad():\n",
    "    (sweep_imgs, mats, _, img_metas, _, _) = data\n",
    "\n",
    "    if torch.cuda.is_available(): # to GPU\n",
    "        for key, value in mats.items():\n",
    "            mats[key] = value.to(device)\n",
    "        sweep_imgs = sweep_imgs.to(device)\n",
    "\n",
    "    starter.record()\n",
    "\n",
    "    preds = model(sweep_imgs, mats)\n",
    "\n",
    "    ender.record()\n",
    "    # wait for gpu sync\n",
    "    torch.cuda.synchronize()\n",
    "    inference_time = starter.elapsed_time(ender)\n",
    "    print(\"inference time (ms): \", inference_time)\n",
    "    \n",
    "    results = model.get_bboxes(preds[0], img_metas)\n",
    "    for i in range(len(results)):\n",
    "        results[i][0] = results[i][0].detach().cpu().numpy()\n",
    "        results[i][1] = results[i][1].detach().cpu().numpy()\n",
    "        results[i][2] = results[i][2].detach().cpu().numpy()\n",
    "        results[i].append(img_metas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec890b-ba23-4618-8662-937cdaa6a252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3689c-79e9-4f58-abda-57a5b03acc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_results.append(predict_step_output[i][:3])\n",
    "all_img_metas.append(predict_step_output[i][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c93b8-6849-4fa4-b4c4-88473a5f8b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494648a-d215-409d-ba7b-c8f6d8bf0255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77e8f065-1254-4da3-a101-795936177788",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Runner using PyTorch Lightning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d4e97c-61e1-4458-9a0a-66fb420fe765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(profiler='simple',\n",
    "                     deterministic=False,\n",
    "                     max_epochs=24,\n",
    "                     # strategy='ddp',\n",
    "                     accelerator='gpu',\n",
    "                     devices=1,\n",
    "                     num_sanity_val_steps=0,\n",
    "                     gradient_clip_val=5,\n",
    "                     limit_val_batches=0,\n",
    "                     enable_checkpointing=True,\n",
    "                     precision=16,\n",
    "                     default_root_dir=os.path.join('./outputs/', exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d04e2-cdb0-47ea-86b7-323d7dcc83e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_step_outputs = trainer.predict(model, ckpt_path=ckpt_path)\n",
    "\n",
    "all_pred_results = list()\n",
    "all_img_metas = list()\n",
    "\n",
    "for predict_step_output in predict_step_outputs:\n",
    "    for i in range(len(predict_step_output)):\n",
    "        all_pred_results.append(predict_step_output[i][:3])\n",
    "        all_img_metas.append(predict_step_output[i][3])\n",
    "\n",
    "synchronize()\n",
    "len_dataset = len(model.test_dataloader().dataset)\n",
    "all_pred_results = sum(\n",
    "    map(list, zip(*all_gather_object(all_pred_results))),\n",
    "    [])[:len_dataset]\n",
    "all_img_metas = sum(map(list, zip(*all_gather_object(all_img_metas))), [])[:len_dataset]\n",
    "\n",
    "model.evaluator._format_bbox(all_pred_results, all_img_metas, os.path.dirname('./outputs/' + exp_name + '/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e43f3-e2e0-4a03-a4a9-e951b2fd3f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a29b8b-779a-4dea-b641-c99eb8285637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd186e7a-fe10-4e7f-80fa-1fed12d9e718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df479c-9edb-4752-9fb7-6e6f1c2d46b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937cebf9-5462-4645-989b-8799d091f8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883520f-0ca0-44c4-852f-2eec92a71070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaefa5c-d00e-4333-b92a-85c948b6e9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ccee1-da00-46db-b6c9-48fc757c24c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab0c3c-c8e4-44d6-aecc-1a2e0a9893ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicv",
   "language": "python",
   "name": "omnicv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
